{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d292972",
   "metadata": {},
   "source": [
    "# Deep Learning Preprocessing Pipeline for Flight Delay Data\n",
    "\n",
    "This notebook builds upon the base preprocessing pipeline to create features specifically optimized for deep learning models such as neural networks. Deep learning models typically require specialized preprocessing including proper normalization, embedding-friendly encodings for categorical variables, and structured data formats compatible with deep learning frameworks.\n",
    "\n",
    "## Key Processing Steps:\n",
    "1. Loading the base preprocessed data\n",
    "2. Feature engineering specific to deep learning models\n",
    "3. Advanced encoding for categorical variables (embeddings)\n",
    "4. Data normalization (standardization)\n",
    "5. Sequence generation for RNNs and LSTMs\n",
    "6. Data batching and formatting for deep learning frameworks\n",
    "7. Time-based validation strategy\n",
    "8. Exporting the processed data for DL model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ba8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and paths configured.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure paths dynamically using relative paths\n",
    "import os.path as path\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "notebook_dir = path.dirname(path.abspath('__file__'))\n",
    "# Get project root (parent of notebooks directory)\n",
    "project_root = path.abspath(path.join(notebook_dir, '..', '..'))\n",
    "\n",
    "# Define paths relative to project root\n",
    "BASE_PROCESSED_PATH = path.join(project_root, 'data', 'processed', 'base_preprocessed_flights.csv')\n",
    "DL_PROCESSED_PATH = path.join(project_root, 'data', 'processed', 'dl_ready_flights')\n",
    "DL_MODEL_PATH = path.join(project_root, 'models', 'dl')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(os.path.dirname(DL_PROCESSED_PATH), exist_ok=True)\n",
    "os.makedirs(DL_MODEL_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Base processed data path: {BASE_PROCESSED_PATH}\")\n",
    "print(f\"DL processed data path: {DL_PROCESSED_PATH}\")\n",
    "print(f\"DL model path: {DL_MODEL_PATH}\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Libraries and paths configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da50820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data in chunks\n",
    "def load_processed_data(file_path, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    Generator function to load preprocessed data in chunks\n",
    "    \"\"\"\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        # Convert date columns to datetime\n",
    "        date_columns = [col for col in chunk.columns if 'DATE' in col.upper()]\n",
    "        for col in date_columns:\n",
    "            chunk[col] = pd.to_datetime(chunk[col], errors='coerce')\n",
    "        \n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2d81a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape of first chunk: (500000, 28)\n",
      "\n",
      "Columns and data types:\n",
      "- FL_DATE: datetime64[ns]\n",
      "- ORIGIN: object\n",
      "- DEST: object\n",
      "- CRS_DEP_TIME: int64\n",
      "- DEP_TIME: float64\n",
      "- DEP_DELAY: float64\n",
      "- TAXI_OUT: float64\n",
      "- WHEELS_OFF: float64\n",
      "- WHEELS_ON: float64\n",
      "- TAXI_IN: float64\n",
      "- CRS_ARR_TIME: int64\n",
      "- ARR_TIME: float64\n",
      "- ARR_DELAY: float64\n",
      "- CANCELLED: int64\n",
      "- CANCELLATION_CODE: object\n",
      "- DIVERTED: int64\n",
      "- CRS_ELAPSED_TIME: float64\n",
      "- AIR_TIME: float64\n",
      "- DISTANCE: float64\n",
      "- YEAR: int64\n",
      "- MONTH: int64\n",
      "- DAY_OF_MONTH: int64\n",
      "- DAY_OF_WEEK: int64\n",
      "- QUARTER: int64\n",
      "- SEASON: int64\n",
      "- IS_HOLIDAY_SEASON: int64\n",
      "- DEP_HOUR: int64\n",
      "- TIME_OF_DAY: object\n",
      "\n",
      "Sample data (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <th>TAXI_IN</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>CANCELLATION_CODE</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>QUARTER</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>IS_HOLIDAY_SEASON</th>\n",
       "      <th>DEP_HOUR</th>\n",
       "      <th>TIME_OF_DAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>FLL</td>\n",
       "      <td>EWR</td>\n",
       "      <td>715</td>\n",
       "      <td>1151.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>1443.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>901</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>MSP</td>\n",
       "      <td>SEA</td>\n",
       "      <td>1280</td>\n",
       "      <td>2114.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2123.0</td>\n",
       "      <td>2232.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1395</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>Evening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>DEN</td>\n",
       "      <td>MSP</td>\n",
       "      <td>594</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>1247.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>772</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>MSP</td>\n",
       "      <td>SFO</td>\n",
       "      <td>969</td>\n",
       "      <td>1608.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1109</td>\n",
       "      <td>1853.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>DAL</td>\n",
       "      <td>OKC</td>\n",
       "      <td>610</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>670</td>\n",
       "      <td>1331.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FL_DATE ORIGIN DEST  CRS_DEP_TIME  DEP_TIME  DEP_DELAY  TAXI_OUT  \\\n",
       "0 2019-01-09    FLL  EWR           715    1151.0       -4.0      19.0   \n",
       "1 2022-11-19    MSP  SEA          1280    2114.0       -6.0       9.0   \n",
       "2 2022-07-22    DEN  MSP           594    1000.0        6.0      20.0   \n",
       "3 2023-03-06    MSP  SFO           969    1608.0       -1.0      27.0   \n",
       "4 2019-07-31    DAL  OKC           610    1237.0      147.0      15.0   \n",
       "\n",
       "   WHEELS_OFF  WHEELS_ON  TAXI_IN  CRS_ARR_TIME  ARR_TIME  ARR_DELAY  \\\n",
       "0      1210.0     1443.0      4.0           901    1447.0      -14.0   \n",
       "1      2123.0     2232.0     38.0          1395    2310.0       -5.0   \n",
       "2      1020.0     1247.0      5.0           772    1252.0        0.0   \n",
       "3      1635.0     1844.0      9.0          1109    1853.0       24.0   \n",
       "4      1252.0     1328.0      3.0           670    1331.0      141.0   \n",
       "\n",
       "   CANCELLED CANCELLATION_CODE  DIVERTED  CRS_ELAPSED_TIME  AIR_TIME  \\\n",
       "0          0               NaN         0             186.0     153.0   \n",
       "1          0               NaN         0             235.0     189.0   \n",
       "2          0               NaN         0             118.0      87.0   \n",
       "3          0               NaN         0             260.0     249.0   \n",
       "4          0               NaN         0              60.0      36.0   \n",
       "\n",
       "   DISTANCE  YEAR  MONTH  DAY_OF_MONTH  DAY_OF_WEEK  QUARTER  SEASON  \\\n",
       "0    1065.0  2019      1             9            3        1       1   \n",
       "1    1399.0  2022     11            19            6        4       4   \n",
       "2     680.0  2022      7            22            5        3       3   \n",
       "3    1589.0  2023      3             6            1        1       2   \n",
       "4     181.0  2019      7            31            3        3       3   \n",
       "\n",
       "   IS_HOLIDAY_SEASON  DEP_HOUR TIME_OF_DAY  \n",
       "0                  0        11     Morning  \n",
       "1                  1        21     Evening  \n",
       "2                  0         9     Morning  \n",
       "3                  0        16   Afternoon  \n",
       "4                  0        10     Morning  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the data\n",
    "first_chunk = next(load_processed_data(BASE_PROCESSED_PATH))\n",
    "\n",
    "print(f\"Data shape of first chunk: {first_chunk.shape}\")\n",
    "print(\"\\nColumns and data types:\")\n",
    "for col in first_chunk.columns:\n",
    "    print(f\"- {col}: {first_chunk[col].dtype}\")\n",
    "\n",
    "print(\"\\nSample data (first 5 rows):\")\n",
    "display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed3e2b",
   "metadata": {},
   "source": [
    "## Feature Engineering Specific to Deep Learning Models\n",
    "\n",
    "Deep learning models require specific preprocessing approaches. Let's define functions to prepare data for neural networks:\n",
    "\n",
    "1. Proper categorical encodings for embeddings\n",
    "2. Feature normalization\n",
    "3. Sequence data preparation for recurrent networks\n",
    "4. Structured representations for tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "529ac768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DL-specific features\n",
    "def create_dl_features(df):\n",
    "    \"\"\"\n",
    "    Create features specifically useful for deep learning models\n",
    "    \"\"\"\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # ======== TEMPORAL FEATURES FOR SEQUENTIAL MODELS ========\n",
    "    \n",
    "    # Sort by date and time if available\n",
    "    if 'FL_DATE' in df_featured.columns and 'DEP_TIME' in df_featured.columns:\n",
    "        df_featured = df_featured.sort_values(['FL_DATE', 'DEP_TIME'])\n",
    "    \n",
    "    # Create normalized time features (better for neural networks)\n",
    "    if 'MONTH' in df_featured.columns:\n",
    "        # Normalize month to [0,1]\n",
    "        df_featured['MONTH_NORM'] = (df_featured['MONTH'] - 1) / 11\n",
    "    \n",
    "    if 'DAY_OF_MONTH' in df_featured.columns:\n",
    "        # Normalize day to [0,1]\n",
    "        df_featured['DAY_OF_MONTH_NORM'] = (df_featured['DAY_OF_MONTH'] - 1) / 30\n",
    "    \n",
    "    if 'DAY_OF_WEEK' in df_featured.columns:\n",
    "        # Normalize day of week to [0,1]\n",
    "        df_featured['DAY_OF_WEEK_NORM'] = (df_featured['DAY_OF_WEEK'] - 1) / 6\n",
    "    \n",
    "    if 'DEP_HOUR' in df_featured.columns:\n",
    "        # Normalize hour to [0,1]\n",
    "        df_featured['DEP_HOUR_NORM'] = df_featured['DEP_HOUR'] / 23\n",
    "    \n",
    "    # Create sine and cosine features for cyclical time variables\n",
    "    # These are particularly useful for neural networks to understand cyclical patterns\n",
    "    if 'MONTH' in df_featured.columns:\n",
    "        df_featured['MONTH_SIN'] = np.sin(2 * np.pi * df_featured['MONTH'] / 12)\n",
    "        df_featured['MONTH_COS'] = np.cos(2 * np.pi * df_featured['MONTH'] / 12)\n",
    "    \n",
    "    if 'DAY_OF_WEEK' in df_featured.columns:\n",
    "        df_featured['DAY_OF_WEEK_SIN'] = np.sin(2 * np.pi * df_featured['DAY_OF_WEEK'] / 7)\n",
    "        df_featured['DAY_OF_WEEK_COS'] = np.cos(2 * np.pi * df_featured['DAY_OF_WEEK'] / 7)\n",
    "    \n",
    "    if 'DEP_HOUR' in df_featured.columns:\n",
    "        df_featured['DEP_HOUR_SIN'] = np.sin(2 * np.pi * df_featured['DEP_HOUR'] / 24)\n",
    "        df_featured['DEP_HOUR_COS'] = np.cos(2 * np.pi * df_featured['DEP_HOUR'] / 24)\n",
    "    \n",
    "    # ======== AGGREGATED FEATURES FOR CONTEXT ========\n",
    "    # Note: In a full pipeline, these would be pre-computed from historical data\n",
    "    # Here we're calculating them within the chunk as an approximation\n",
    "    \n",
    "    if 'ORIGIN' in df_featured.columns:\n",
    "        # Airport busy level - compute average daily flights per origin\n",
    "        airport_flights = df_featured.groupby('ORIGIN').size().reset_index(name='ORIGIN_FLIGHTS')\n",
    "        df_featured = df_featured.merge(airport_flights, on='ORIGIN', how='left')\n",
    "        \n",
    "        # Normalize the airport flight count\n",
    "        max_flights = df_featured['ORIGIN_FLIGHTS'].max()\n",
    "        if max_flights > 0:  # Avoid division by zero\n",
    "            df_featured['ORIGIN_FLIGHTS_NORM'] = df_featured['ORIGIN_FLIGHTS'] / max_flights\n",
    "    \n",
    "    if 'OP_CARRIER' in df_featured.columns:\n",
    "        # Carrier size - compute total flights per carrier\n",
    "        carrier_flights = df_featured.groupby('OP_CARRIER').size().reset_index(name='CARRIER_FLIGHTS')\n",
    "        df_featured = df_featured.merge(carrier_flights, on='OP_CARRIER', how='left')\n",
    "        \n",
    "        # Normalize the carrier flight count\n",
    "        max_flights = df_featured['CARRIER_FLIGHTS'].max()\n",
    "        if max_flights > 0:  # Avoid division by zero\n",
    "            df_featured['CARRIER_FLIGHTS_NORM'] = df_featured['CARRIER_FLIGHTS'] / max_flights\n",
    "    \n",
    "    # ======== SEQUENTIAL FEATURES ========\n",
    "    \n",
    "    # Code for creating sequential features will be in a separate function\n",
    "    \n",
    "    return df_featured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd185dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label encoders for categorical variables (for embeddings)\n",
    "def create_label_encoders(df, categorical_cols=None):\n",
    "    \"\"\"\n",
    "    Create label encoders for categorical variables to be used in embeddings\n",
    "    \"\"\"\n",
    "    if categorical_cols is None:\n",
    "        # Identify categorical columns automatically\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Create a dictionary to store encoders\n",
    "    encoders = {}\n",
    "    vocab_sizes = {}\n",
    "    \n",
    "    # Create label encoder for each categorical column\n",
    "    for col in categorical_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Create and fit the label encoder\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(df[col].astype(str).fillna('UNKNOWN'))\n",
    "        encoders[col] = encoder\n",
    "        \n",
    "        # Store vocabulary size (number of unique categories + 1 for unknown)\n",
    "        vocab_sizes[col] = len(encoder.classes_) + 1\n",
    "    \n",
    "    return encoders, vocab_sizes, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d50707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoders and create embedding-ready data\n",
    "def apply_label_encodings(df, encoders, categorical_cols):\n",
    "    \"\"\"\n",
    "    Apply label encoders and prepare data for embeddings\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Apply encoding to each categorical column\n",
    "    for col, encoder in encoders.items():\n",
    "        if col in df_encoded.columns:\n",
    "            # Convert to string and fill NAs\n",
    "            col_data = df_encoded[col].astype(str).fillna('UNKNOWN')\n",
    "            \n",
    "            # Handle values not seen during training\n",
    "            unique_vals = set(col_data.unique())\n",
    "            known_vals = set(encoder.classes_)\n",
    "            unknown_vals = unique_vals - known_vals\n",
    "            \n",
    "            if unknown_vals:\n",
    "                # Replace unknown values with a known value (e.g., 'UNKNOWN')\n",
    "                for val in unknown_vals:\n",
    "                    col_data = col_data.replace(val, 'UNKNOWN')\n",
    "            \n",
    "            # Transform using the encoder\n",
    "            try:\n",
    "                df_encoded[f'{col}_ENCODED'] = encoder.transform(col_data)\n",
    "            except:\n",
    "                # If transformation fails, use a default value\n",
    "                print(f\"Error encoding {col}, using default values\")\n",
    "                df_encoded[f'{col}_ENCODED'] = 0\n",
    "            \n",
    "            # Drop the original column (we keep the encoded version for embeddings)\n",
    "            df_encoded = df_encoded.drop(col, axis=1)\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0da836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numeric features for deep learning\n",
    "def normalize_features_dl(df, exclude_cols=None, scaler=None):\n",
    "    \"\"\"\n",
    "    Normalize numeric features for neural networks\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    \n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # Get numeric columns excluding those in exclude_cols\n",
    "    numeric_cols = df_norm.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    # Fit or transform with scaler\n",
    "    if scaler is None:\n",
    "        # First call - fit a new scaler\n",
    "        scaler = StandardScaler()\n",
    "        df_norm[numeric_cols] = scaler.fit_transform(df_norm[numeric_cols])\n",
    "    else:\n",
    "        # Subsequent calls - use the fitted scaler\n",
    "        df_norm[numeric_cols] = scaler.transform(df_norm[numeric_cols])\n",
    "    \n",
    "    return df_norm, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14cf3dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence data for RNN/LSTM models\n",
    "def create_sequence_data(df, seq_length=10, time_col='FL_DATE', group_cols=None, target_col='DEP_DELAY'):\n",
    "    \"\"\"\n",
    "    Create sequences for RNN/LSTM models\n",
    "    \n",
    "    This function creates sequences of data by grouping by specified columns\n",
    "    and ordering by time. Each sequence will have seq_length steps.\n",
    "    \"\"\"\n",
    "    if group_cols is None:\n",
    "        # Default grouping by origin airport and carrier\n",
    "        group_cols = ['ORIGIN', 'OP_CARRIER']\n",
    "    \n",
    "    # Filter group_cols to include only columns that exist\n",
    "    group_cols = [col for col in group_cols if col in df.columns]\n",
    "    \n",
    "    # Ensure we have at least one grouping column\n",
    "    if not group_cols:\n",
    "        raise ValueError(\"No valid grouping columns found\")\n",
    "    \n",
    "    # Sort by group and time\n",
    "    if time_col in df.columns:\n",
    "        df_sorted = df.sort_values(group_cols + [time_col])\n",
    "    else:\n",
    "        # If no time column, just use the existing order\n",
    "        df_sorted = df\n",
    "    \n",
    "    # Lists to store sequences and targets\n",
    "    X_sequences = []\n",
    "    y_values = []\n",
    "    \n",
    "    # Group by the specified columns\n",
    "    for _, group in df_sorted.groupby(group_cols):\n",
    "        # Skip if group is too small\n",
    "        if len(group) < seq_length + 1:\n",
    "            continue\n",
    "        \n",
    "        # Extract features (exclude target and time column)\n",
    "        features = group.drop([target_col], axis=1)\n",
    "        if time_col in features.columns:\n",
    "            features = features.drop([time_col], axis=1)\n",
    "        \n",
    "        # Extract target\n",
    "        targets = group[target_col].values\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(len(group) - seq_length):\n",
    "            X_sequences.append(features.iloc[i:i+seq_length].values)\n",
    "            y_values.append(targets[i+seq_length])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_sequences = np.array(X_sequences)\n",
    "    y_values = np.array(y_values)\n",
    "    \n",
    "    return X_sequences, y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9daf8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based train-test-validation split for deep learning\n",
    "def time_based_split_dl(df, date_col='FL_DATE', test_size=0.2, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Create a time-based split for deep learning models\n",
    "    \"\"\"\n",
    "    if date_col not in df.columns:\n",
    "        raise ValueError(f\"{date_col} not found in dataframe\")\n",
    "    \n",
    "    # Sort by date\n",
    "    df_sorted = df.sort_values(date_col)\n",
    "    \n",
    "    # Determine split points\n",
    "    n_samples = len(df_sorted)\n",
    "    test_start_idx = int(n_samples * (1 - test_size))\n",
    "    val_start_idx = int(n_samples * (1 - test_size - val_size))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = df_sorted.iloc[:val_start_idx]\n",
    "    val_data = df_sorted.iloc[val_start_idx:test_start_idx]\n",
    "    test_data = df_sorted.iloc[test_start_idx:]\n",
    "    \n",
    "    print(f\"Train set: {len(train_data):,} rows from {train_data[date_col].min()} to {train_data[date_col].max()}\")\n",
    "    print(f\"Validation set: {len(val_data):,} rows from {val_data[date_col].min()} to {val_data[date_col].max()}\")\n",
    "    print(f\"Test set: {len(test_data):,} rows from {test_data[date_col].min()} to {test_data[date_col].max()}\")\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187689a5",
   "metadata": {},
   "source": [
    "## Execute the DL Preprocessing Pipeline\n",
    "\n",
    "Now we'll run the DL preprocessing pipeline on our base preprocessed data. For deep learning, we'll create both:\n",
    "1. A standard tabular dataset for feedforward neural networks\n",
    "2. Sequence data for RNN/LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process chunks with the DL pipeline\n",
    "def process_chunk_dl(chunk, encoders=None, categorical_cols=None, scaler=None, exclude_from_scaling=None):\n",
    "    \"\"\"\n",
    "    Apply DL preprocessing to a chunk of base preprocessed data\n",
    "    \"\"\"\n",
    "    # Handle missing values first: for columns with >50% missing, drop 90% of missing rows\n",
    "    chunk = drop_90pct_missing_per_column(chunk, threshold=0.5)\n",
    "    # Add DL-specific features\n",
    "    chunk_dl = create_dl_features(chunk)\n",
    "    # Fit or apply label encoders for categorical variables\n",
    "    if encoders is None:\n",
    "        # First chunk - create encoders\n",
    "        encoders, vocab_sizes, categorical_cols = create_label_encoders(chunk_dl)\n",
    "    # Apply encodings\n",
    "    chunk_dl = apply_label_encodings(chunk_dl, encoders, categorical_cols)\n",
    "    # Normalize numeric features\n",
    "    chunk_dl, scaler = normalize_features_dl(chunk_dl, exclude_from_scaling, scaler)\n",
    "    return chunk_dl, encoders, categorical_cols, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c858a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all chunks and save the DL-ready dataset\n",
    "def prepare_dl_dataset(input_file, output_dir, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    Process all chunks and prepare DL-ready datasets\n",
    "    \"\"\"\n",
    "    encoders = None\n",
    "    categorical_cols = None\n",
    "    scaler = None\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define output paths\n",
    "    tabular_output = os.path.join(output_dir, 'dl_tabular_data.csv')\n",
    "    encoders_output = os.path.join(output_dir, 'dl_encoders.pkl')\n",
    "    scaler_output = os.path.join(output_dir, 'dl_scaler.pkl')\n",
    "    \n",
    "    print(f\"Starting DL preprocessing of {input_file}...\")\n",
    "    \n",
    "    # Process in chunks\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(load_processed_data(input_file, chunk_size=chunk_size)):\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Process the chunk\n",
    "        processed_chunk, encoders, categorical_cols, scaler = process_chunk_dl(\n",
    "            chunk, encoders, categorical_cols, scaler\n",
    "        )\n",
    "        \n",
    "        chunks.append(processed_chunk)\n",
    "        \n",
    "        # Print progress\n",
    "        end_time = datetime.now()\n",
    "        elapsed = (end_time - start_time).total_seconds()\n",
    "        print(f\"Processed chunk {i+1}: {len(processed_chunk):,} rows in {elapsed:.2f} seconds\")\n",
    "        \n",
    "        # To save memory, periodically combine and save chunks\n",
    "        if len(chunks) >= 5:\n",
    "            combined = pd.concat(chunks)\n",
    "            \n",
    "            # Save with mode='a' (append) after first chunk\n",
    "            if i <= 5:\n",
    "                combined.to_csv(tabular_output, index=False)\n",
    "            else:\n",
    "                combined.to_csv(tabular_output, mode='a', header=False, index=False)\n",
    "                \n",
    "            # Clear chunks list to free memory\n",
    "            chunks = []\n",
    "    \n",
    "    # Save any remaining chunks\n",
    "    if chunks:\n",
    "        combined = pd.concat(chunks)\n",
    "        # Check if file exists\n",
    "        if os.path.exists(tabular_output):\n",
    "            combined.to_csv(tabular_output, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            combined.to_csv(tabular_output, index=False)\n",
    "    \n",
    "    # Save encoders and scaler\n",
    "    with open(encoders_output, 'wb') as f:\n",
    "        pickle.dump((encoders, categorical_cols), f)\n",
    "    \n",
    "    with open(scaler_output, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(f\"DL preprocessing complete!\")\n",
    "    print(f\"Tabular data saved to: {tabular_output}\")\n",
    "    print(f\"Encoders saved to: {encoders_output}\")\n",
    "    print(f\"Scaler saved to: {scaler_output}\")\n",
    "    \n",
    "    return encoders, categorical_cols, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65dd3203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DL preprocessing of /Users/osx/flightDelayPIPELINE.2/data/processed/base_preprocessed_flights.csv...\n",
      "Processed chunk 1: 500,000 rows in 1.27 seconds\n",
      "Processed chunk 1: 500,000 rows in 1.27 seconds\n",
      "Processed chunk 2: 500,000 rows in 0.84 seconds\n",
      "Processed chunk 2: 500,000 rows in 0.84 seconds\n",
      "Error encoding DEST, using default values\n",
      "Error encoding DEST, using default values\n",
      "Processed chunk 3: 500,000 rows in 1.02 seconds\n",
      "Processed chunk 3: 500,000 rows in 1.02 seconds\n",
      "Error encoding DEST, using default values\n",
      "Error encoding DEST, using default values\n",
      "Processed chunk 4: 500,000 rows in 1.37 seconds\n",
      "Processed chunk 4: 500,000 rows in 1.37 seconds\n",
      "Error encoding DEST, using default values\n",
      "Error encoding DEST, using default values\n",
      "Processed chunk 5: 500,000 rows in 1.07 seconds\n",
      "Processed chunk 5: 500,000 rows in 1.07 seconds\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Execute the DL preprocessing pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m encoders, categorical_cols, scaler = \u001b[43mprepare_dl_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_PROCESSED_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDL_PROCESSED_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mprepare_dl_dataset\u001b[39m\u001b[34m(input_file, output_dir, chunk_size)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Save with mode='a' (append) after first chunk\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i <= \u001b[32m5\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mcombined\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtabular_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     45\u001b[39m     combined.to_csv(tabular_output, mode=\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m, header=\u001b[38;5;28;01mFalse\u001b[39;00m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/generic.py:3967\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3956\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3958\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3959\u001b[39m     frame=df,\n\u001b[32m   3960\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3964\u001b[39m     decimal=decimal,\n\u001b[32m   3965\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3972\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3984\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/io/formats/csvs.py:270\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.filepath_or_buffer,\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/io/formats/csvs.py:275\u001b[39m, in \u001b[36mCSVFormatter._save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._need_to_save_header:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_header()\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/io/formats/csvs.py:313\u001b[39m, in \u001b[36mCSVFormatter._save_body\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_i >= end_i:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/pandas/io/formats/csvs.py:324\u001b[39m, in \u001b[36mCSVFormatter._save_chunk\u001b[39m\u001b[34m(self, start_i, end_i)\u001b[39m\n\u001b[32m    321\u001b[39m data = \u001b[38;5;28mlist\u001b[39m(res._iter_column_arrays())\n\u001b[32m    323\u001b[39m ix = \u001b[38;5;28mself\u001b[39m.data_index[slicer]._get_values_for_csv(**\u001b[38;5;28mself\u001b[39m._number_format)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[43mlibwriters\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mwriters.pyx:73\u001b[39m, in \u001b[36mpandas._libs.writers.write_csv_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# Execute the DL preprocessing pipeline\n",
    "encoders, categorical_cols, scaler = prepare_dl_dataset(BASE_PROCESSED_PATH, DL_PROCESSED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the output file and create sample sequences\n",
    "try:\n",
    "    # Define paths\n",
    "    tabular_output = os.path.join(DL_PROCESSED_PATH, 'dl_tabular_data.csv')\n",
    "    \n",
    "    # Read a sample of the processed data\n",
    "    dl_sample = pd.read_csv(tabular_output, nrows=10000)\n",
    "    print(f\"DL processed data shape: {dl_sample.shape}\")\n",
    "    \n",
    "    print(\"\\nColumns in DL processed data:\")\n",
    "    for col in dl_sample.columns:\n",
    "        print(f\"- {col}: {dl_sample[col].dtype}\")\n",
    "    \n",
    "    print(\"\\nSample of DL processed data:\")\n",
    "    display(dl_sample.head())\n",
    "    \n",
    "    # Convert date column back to datetime if needed\n",
    "    if 'FL_DATE' in dl_sample.columns:\n",
    "        dl_sample['FL_DATE'] = pd.to_datetime(dl_sample['FL_DATE'])\n",
    "    \n",
    "    # Create a small sequence dataset as an example\n",
    "    print(\"\\nCreating sample sequences for RNN/LSTM models...\")\n",
    "    X_seq, y_seq = create_sequence_data(dl_sample, seq_length=5)\n",
    "    \n",
    "    print(f\"Sequence data shape: {X_seq.shape}\")\n",
    "    print(f\"Target data shape: {y_seq.shape}\")\n",
    "    print(\"Sample sequence:\")\n",
    "    display(X_seq[0])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error verifying output: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b0833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train-test-val split for deep learning (small sample for demonstration)\n",
    "try:\n",
    "    # Read a sample of the data\n",
    "    dl_sample = pd.read_csv(os.path.join(DL_PROCESSED_PATH, 'dl_tabular_data.csv'), nrows=50000)\n",
    "    \n",
    "    # Convert date column back to datetime if needed\n",
    "    if 'FL_DATE' in dl_sample.columns:\n",
    "        dl_sample['FL_DATE'] = pd.to_datetime(dl_sample['FL_DATE'])\n",
    "        \n",
    "        # Create time-based splits\n",
    "        train_data, val_data, test_data = time_based_split_dl(dl_sample)\n",
    "        \n",
    "        # Visualize the target distribution in each split\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(131)\n",
    "        plt.hist(train_data['DEP_DELAY'], bins=50, alpha=0.7)\n",
    "        plt.title('Train Set - Departure Delay Distribution')\n",
    "        plt.xlabel('Delay (minutes)')\n",
    "        \n",
    "        plt.subplot(132)\n",
    "        plt.hist(val_data['DEP_DELAY'], bins=50, alpha=0.7)\n",
    "        plt.title('Validation Set - Departure Delay Distribution')\n",
    "        plt.xlabel('Delay (minutes)')\n",
    "        \n",
    "        plt.subplot(133)\n",
    "        plt.hist(test_data['DEP_DELAY'], bins=50, alpha=0.7)\n",
    "        plt.title('Test Set - Departure Delay Distribution')\n",
    "        plt.xlabel('Delay (minutes)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating time-based splits: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76fd322",
   "metadata": {},
   "source": [
    "## Creating Model-Ready Batches for Deep Learning Frameworks\n",
    "\n",
    "In this section, we provide example code for formatting the preprocessed data into batches suitable for deep learning frameworks. This is a demonstration using a small sample, but the same approach can be applied to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505da545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create batches for a feed-forward neural network\n",
    "def create_tabular_batches(df, target_col='DEP_DELAY', batch_size=32):\n",
    "    \"\"\"\n",
    "    Create batches for training a tabular deep learning model\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Create batches\n",
    "    n_samples = len(X)\n",
    "    n_batches = n_samples // batch_size\n",
    "    \n",
    "    # Lists to store batches\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    # Create batches\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        X_batches.append(X.iloc[start_idx:end_idx].values)\n",
    "        y_batches.append(y[start_idx:end_idx])\n",
    "    \n",
    "    return X_batches, y_batches, X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aaf14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Create tabular batches from a small sample\n",
    "try:\n",
    "    # Create batches from the train set\n",
    "    if 'train_data' in locals():\n",
    "        # Remove date column if present\n",
    "        if 'FL_DATE' in train_data.columns:\n",
    "            train_features = train_data.drop(columns=['FL_DATE'])\n",
    "        else:\n",
    "            train_features = train_data\n",
    "            \n",
    "        # Create batches\n",
    "        X_batches, y_batches, feature_names = create_tabular_batches(train_features, batch_size=32)\n",
    "        \n",
    "        print(f\"Created {len(X_batches)} batches for training\")\n",
    "        print(f\"Batch shape: {X_batches[0].shape}\")\n",
    "        print(f\"Target shape: {y_batches[0].shape}\")\n",
    "        \n",
    "        # Show the first few feature names\n",
    "        print(\"\\nFirst 10 features:\")\n",
    "        for i, name in enumerate(feature_names[:10]):\n",
    "            print(f\"{i+1}. {name}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating batches: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617535ad",
   "metadata": {},
   "source": [
    "## Summary of Deep Learning Preprocessing\n",
    "\n",
    "The DL preprocessing pipeline has:\n",
    "\n",
    "1. Added DL-specific engineered features optimized for neural networks\n",
    "2. Created normalized time features (cyclic encoding using sin/cos)\n",
    "3. Applied label encoding to categorical variables for embedding layers\n",
    "4. Normalized numeric features for better convergence\n",
    "5. Created sequence data for RNN/LSTM models\n",
    "6. Implemented time-based train-test-validation splitting\n",
    "7. Demonstrated batch creation for deep learning frameworks\n",
    "\n",
    "This DL-ready dataset is optimized for both feedforward neural networks (tabular data) and recurrent neural networks (sequence data). The preprocessing enhances model convergence and provides appropriate formats for embedding layers commonly used in deep learning models for categorical variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
