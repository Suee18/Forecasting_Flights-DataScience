{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cbc41a",
   "metadata": {},
   "source": [
    "# Full Time Series Preprocessing Pipeline for Flight Delay Data\n",
    "\n",
    "This notebook performs the complete time series preprocessing pipeline, including:\n",
    "- Loading the full base dataset in chunks\n",
    "- Handling missing values and imputation\n",
    "- Scaling features\n",
    "- Train/validation/test split (chronological)\n",
    "- Outputting all necessary files for time series models, including a per-flight delay file (FL_DATE, DEP_DELAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddc22b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE_PROCESSED_PATH = os.path.join('..', '..', 'data', 'processed', 'base_preprocessed_flights.csv')\n",
    "TS_PROCESSED_PATH = os.path.join('..', '..', 'data', 'processed', 'ts_ready_flights')\n",
    "MODEL_READY_PATH = os.path.join(TS_PROCESSED_PATH, 'model_ready')\n",
    "os.makedirs(TS_PROCESSED_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_READY_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a2840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_with_high_missing(df, threshold=0.5):\n",
    "    missing_ratio = df.isnull().mean(axis=0)\n",
    "    cols_to_drop = missing_ratio[missing_ratio >= threshold].index.tolist()\n",
    "    return df.drop(columns=cols_to_drop)\n",
    "\n",
    "def impute_missing_values(df):\n",
    "    df_imputed = df.copy()\n",
    "    for col in df_imputed.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df_imputed[col]):\n",
    "            median_val = df_imputed[col].median()\n",
    "            df_imputed[col] = df_imputed[col].fillna(median_val)\n",
    "        else:\n",
    "            mode_val = df_imputed[col].mode(dropna=True)\n",
    "            if not mode_val.empty:\n",
    "                df_imputed[col] = df_imputed[col].fillna(mode_val[0])\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "617792e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, clean, and concatenate all data in chunks\n",
    "def load_and_clean_all_data(base_path):\n",
    "    chunk_iter = pd.read_csv(base_path, chunksize=500_000)\n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunk_iter:\n",
    "        if 'FL_DATE' in chunk.columns:\n",
    "            chunk['FL_DATE'] = pd.to_datetime(chunk['FL_DATE'], errors='coerce')\n",
    "        chunk = drop_columns_with_high_missing(chunk, threshold=0.5)\n",
    "        chunk = impute_missing_values(chunk)\n",
    "        cleaned_chunks.append(chunk)\n",
    "    df_full = pd.concat(cleaned_chunks, ignore_index=True)\n",
    "    df_full = df_full.drop_duplicates()\n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dc32068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full cleaned data shape: (2470191, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <th>TAXI_IN</th>\n",
       "      <th>...</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>QUARTER</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>IS_HOLIDAY_SEASON</th>\n",
       "      <th>DEP_HOUR</th>\n",
       "      <th>TIME_OF_DAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>FLL</td>\n",
       "      <td>EWR</td>\n",
       "      <td>715</td>\n",
       "      <td>711</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>1443.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>MSP</td>\n",
       "      <td>SEA</td>\n",
       "      <td>1280</td>\n",
       "      <td>1274</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2123.0</td>\n",
       "      <td>2232.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>Evening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>DEN</td>\n",
       "      <td>MSP</td>\n",
       "      <td>594</td>\n",
       "      <td>600</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>1247.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>680.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>MSP</td>\n",
       "      <td>SFO</td>\n",
       "      <td>969</td>\n",
       "      <td>968</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>DAL</td>\n",
       "      <td>OKC</td>\n",
       "      <td>610</td>\n",
       "      <td>757</td>\n",
       "      <td>147.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>181.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FL_DATE ORIGIN DEST  CRS_DEP_TIME  DEP_TIME  DEP_DELAY  TAXI_OUT  \\\n",
       "0 2019-01-09    FLL  EWR           715       711       -4.0      19.0   \n",
       "1 2022-11-19    MSP  SEA          1280      1274       -6.0       9.0   \n",
       "2 2022-07-22    DEN  MSP           594       600        6.0      20.0   \n",
       "3 2023-03-06    MSP  SFO           969       968       -1.0      27.0   \n",
       "4 2019-07-31    DAL  OKC           610       757      147.0      15.0   \n",
       "\n",
       "   WHEELS_OFF  WHEELS_ON  TAXI_IN  ...  DISTANCE  YEAR  MONTH  DAY_OF_MONTH  \\\n",
       "0      1210.0     1443.0      4.0  ...    1065.0  2019      1             9   \n",
       "1      2123.0     2232.0     38.0  ...    1399.0  2022     11            19   \n",
       "2      1020.0     1247.0      5.0  ...     680.0  2022      7            22   \n",
       "3      1635.0     1844.0      9.0  ...    1589.0  2023      3             6   \n",
       "4      1252.0     1328.0      3.0  ...     181.0  2019      7            31   \n",
       "\n",
       "   DAY_OF_WEEK  QUARTER  SEASON  IS_HOLIDAY_SEASON  DEP_HOUR  TIME_OF_DAY  \n",
       "0            3        1       1                  0        11      Morning  \n",
       "1            6        4       4                  1        21      Evening  \n",
       "2            5        3       3                  0         9      Morning  \n",
       "3            1        1       2                  0        16    Afternoon  \n",
       "4            3        3       3                  0        10      Morning  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and clean the full dataset\n",
    "df_full = load_and_clean_all_data(BASE_PROCESSED_PATH)\n",
    "print(f\"Full cleaned data shape: {df_full.shape}\")\n",
    "display(df_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a515c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1345137, 27), Test: (669391, 27)\n"
     ]
    }
   ],
   "source": [
    "# Chronological train/val/test split by year\n",
    "from datetime import timedelta\n",
    "\n",
    "def time_series_split_by_year(df, date_col='FL_DATE', train_years=3, test_years=1):\n",
    "    df = df.sort_values(date_col)\n",
    "    min_date = df[date_col].min()\n",
    "    max_date = df[date_col].max()\n",
    "    # Calculate split dates\n",
    "    train_end = min_date + pd.DateOffset(years=train_years)\n",
    "    test_start = train_end\n",
    "    test_end = test_start + pd.DateOffset(years=test_years)\n",
    "    # Assign splits\n",
    "    train = df[df[date_col] < train_end].copy()\n",
    "    test = df[(df[date_col] >= test_start) & (df[date_col] < test_end)].copy()\n",
    "    val = pd.DataFrame()  # No explicit validation set in this split\n",
    "    print(f\"Train: {train.shape}, Test: {test.shape}\")\n",
    "    return train, val, test\n",
    "\n",
    "train_df, val_df, test_df = time_series_split_by_year(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fe7251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (fit on train, transform all)\n",
    "scaler = StandardScaler()\n",
    "feature_cols = [col for col in train_df.columns if col not in ['FL_DATE', 'DEP_DELAY'] and pd.api.types.is_numeric_dtype(train_df[col])]\n",
    "\n",
    "# Impute numeric columns: use median if outliers, mean otherwise\n",
    "def has_outliers(series, z_thresh=3):\n",
    "    if series.isnull().all():\n",
    "        return False\n",
    "    z = (series - series.mean()) / series.std(ddof=0)\n",
    "    return (z.abs() > z_thresh).any()\n",
    "\n",
    "imputed_train = train_df.copy()\n",
    "imputed_val = val_df.copy()\n",
    "imputed_test = test_df.copy()\n",
    "\n",
    "for col in feature_cols:\n",
    "    if has_outliers(train_df[col]):\n",
    "        imp = SimpleImputer(strategy='median')\n",
    "    else:\n",
    "        imp = SimpleImputer(strategy='mean')\n",
    "    imputed_train[[col]] = imp.fit_transform(train_df[[col]])\n",
    "    if len(val_df) > 0 and col in val_df.columns:\n",
    "        imputed_val[[col]] = imp.transform(val_df[[col]])\n",
    "    if col in test_df.columns:\n",
    "        imputed_test[[col]] = imp.transform(test_df[[col]])\n",
    "\n",
    "# Only use columns that exist in each split to avoid KeyError\n",
    "val_feature_cols = [col for col in feature_cols if col in imputed_val.columns]\n",
    "test_feature_cols = [col for col in feature_cols if col in imputed_test.columns]\n",
    "\n",
    "train_scaled = imputed_train.copy()\n",
    "val_scaled = imputed_val.copy()\n",
    "test_scaled = imputed_test.copy()\n",
    "\n",
    "train_scaled[feature_cols] = scaler.fit_transform(imputed_train[feature_cols])\n",
    "if len(val_df) > 0 and val_feature_cols:\n",
    "    val_scaled[val_feature_cols] = scaler.transform(imputed_val[val_feature_cols])\n",
    "if test_feature_cols:\n",
    "    test_scaled[test_feature_cols] = scaler.transform(imputed_test[test_feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fda3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train/val/test splits and scaler.\n"
     ]
    }
   ],
   "source": [
    "# Save splits and scaler\n",
    "train_scaled.to_pickle(os.path.join(MODEL_READY_PATH, 'train_ts.pkl'))\n",
    "val_scaled.to_pickle(os.path.join(MODEL_READY_PATH, 'val_ts.pkl'))\n",
    "test_scaled.to_pickle(os.path.join(MODEL_READY_PATH, 'test_ts.pkl'))\n",
    "import pickle\n",
    "with open(os.path.join(MODEL_READY_PATH, 'scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Saved train/val/test splits and scaler.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b341da5",
   "metadata": {},
   "source": [
    "## Output: Per-Flight Delay File for TS Models\n",
    "This file contains FL_DATE and DEP_DELAY for every flight, deduplicated and cleaned, for use in time series models (e.g., N-BEATS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26ab1976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-flight delay file: (2470191, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FL_DATE  DEP_DELAY\n",
       "0 2019-01-09       -4.0\n",
       "1 2022-11-19       -6.0\n",
       "2 2022-07-22        6.0\n",
       "3 2023-03-06       -1.0\n",
       "4 2019-07-31      147.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "per_flight_delay = df_full.drop_duplicates()[['FL_DATE', 'DEP_DELAY']]\n",
    "per_flight_delay.to_csv(os.path.join(TS_PROCESSED_PATH, 'per_flight_delay_ts.csv'), index=False)\n",
    "per_flight_delay.to_pickle(os.path.join(TS_PROCESSED_PATH, 'per_flight_delay_ts.pkl'))\n",
    "print(f\"Saved per-flight delay file: {per_flight_delay.shape}\")\n",
    "display(per_flight_delay.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
