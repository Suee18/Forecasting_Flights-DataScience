{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d8f7ce2",
   "metadata": {},
   "source": [
    "# Time Series Preprocessing Pipeline for Flight Delay Data\n",
    "\n",
    "This notebook builds upon the base preprocessing pipeline to create features specifically optimized for time series models such as ARIMA, Prophet, and other forecasting algorithms. Time series analysis requires specific preprocessing steps including proper temporal aggregation, handling of seasonality, and preparation of lagged features.\n",
    "\n",
    "## Key Processing Steps:\n",
    "1. Loading the base preprocessed data\n",
    "2. Temporal aggregation (daily, hourly)\n",
    "3. Time series feature engineering (lags, rolling windows)\n",
    "4. Handling seasonality\n",
    "5. Stationarity testing and transformations\n",
    "6. Creation of exogenous variables\n",
    "7. Time-based train-test splits with forecast horizon\n",
    "8. Exporting the processed data for time series model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure paths dynamically using relative paths\n",
    "import os.path as path\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "notebook_dir = path.dirname(path.abspath('__file__'))\n",
    "# Get project root (parent of notebooks directory)\n",
    "project_root = path.abspath(path.join(notebook_dir, '..', '..'))\n",
    "\n",
    "# Define paths relative to project root\n",
    "BASE_PROCESSED_PATH = path.join(project_root, 'data', 'processed', 'base_preprocessed_flights.csv')\n",
    "TS_PROCESSED_PATH = path.join(project_root, 'data', 'processed', 'ts_ready_flights')\n",
    "TS_MODEL_PATH = path.join(project_root, 'models', 'ts')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(os.path.dirname(TS_PROCESSED_PATH), exist_ok=True)\n",
    "os.makedirs(TS_MODEL_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Base processed data path: {BASE_PROCESSED_PATH}\")\n",
    "print(f\"Time Series processed data path: {TS_PROCESSED_PATH}\")\n",
    "print(f\"Time Series model path: {TS_MODEL_PATH}\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Libraries and paths configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5360cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data in chunks\n",
    "def load_processed_data(file_path, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    Generator function to load preprocessed data in chunks\n",
    "    \"\"\"\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        # Convert date columns to datetime\n",
    "        date_columns = [col for col in chunk.columns if 'DATE' in col.upper()]\n",
    "        for col in date_columns:\n",
    "            chunk[col] = pd.to_datetime(chunk[col], errors='coerce')\n",
    "        \n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e33e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "first_chunk = next(load_processed_data(BASE_PROCESSED_PATH))\n",
    "\n",
    "print(f\"Data shape of first chunk: {first_chunk.shape}\")\n",
    "print(\"\\nColumns and data types:\")\n",
    "for col in first_chunk.columns:\n",
    "    print(f\"- {col}: {first_chunk[col].dtype}\")\n",
    "\n",
    "print(\"\\nSample data (first 5 rows):\")\n",
    "display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d7751",
   "metadata": {},
   "source": [
    "## Time Series Pre-analysis\n",
    "\n",
    "Before we dive into preprocessing for time series models, let's first analyze the time series characteristics of our flight delay data to inform our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ef529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data by date for time series analysis\n",
    "def aggregate_to_time_series(chunks, agg_level='daily', target_col='DEP_DELAY'):\n",
    "    \"\"\"\n",
    "    Aggregate flight data into time series at different temporal resolutions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    chunks : generator\n",
    "        Generator of data chunks\n",
    "    agg_level : str\n",
    "        Level of aggregation ('daily', 'hourly', 'airport_daily', 'carrier_daily')\n",
    "    target_col : str\n",
    "        Column to aggregate (typically delay)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Aggregated time series data\n",
    "    \"\"\"\n",
    "    # Dictionary to store aggregated data\n",
    "    agg_data = {}\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Ensure we have date information\n",
    "        if 'FL_DATE' not in chunk.columns:\n",
    "            print(\"Error: FL_DATE column not found\")\n",
    "            return None\n",
    "        \n",
    "        # Define groupby columns based on aggregation level\n",
    "        if agg_level == 'daily':\n",
    "            groupby_cols = ['FL_DATE']\n",
    "        elif agg_level == 'hourly':\n",
    "            if 'DEP_HOUR' in chunk.columns:\n",
    "                chunk['DATETIME'] = chunk['FL_DATE'] + pd.to_timedelta(chunk['DEP_HOUR'], unit='h')\n",
    "                groupby_cols = ['DATETIME']\n",
    "            else:\n",
    "                print(\"Error: DEP_HOUR column not found for hourly aggregation\")\n",
    "                return None\n",
    "        elif agg_level == 'airport_daily':\n",
    "            if 'ORIGIN' in chunk.columns:\n",
    "                groupby_cols = ['FL_DATE', 'ORIGIN']\n",
    "            else:\n",
    "                print(\"Error: ORIGIN column not found for airport_daily aggregation\")\n",
    "                return None\n",
    "        elif agg_level == 'carrier_daily':\n",
    "            if 'OP_CARRIER' in chunk.columns:\n",
    "                groupby_cols = ['FL_DATE', 'OP_CARRIER']\n",
    "            else:\n",
    "                print(\"Error: OP_CARRIER column not found for carrier_daily aggregation\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Error: Unknown aggregation level: {agg_level}\")\n",
    "            return None\n",
    "        \n",
    "        # Skip chunk if target column is not present\n",
    "        if target_col not in chunk.columns:\n",
    "            continue\n",
    "        \n",
    "        # Aggregate data\n",
    "        agg = chunk.groupby(groupby_cols).agg(\n",
    "            avg_delay=(target_col, 'mean'),\n",
    "            median_delay=(target_col, 'median'),\n",
    "            max_delay=(target_col, 'max'),\n",
    "            min_delay=(target_col, 'min'),\n",
    "            std_delay=(target_col, 'std'),\n",
    "            num_flights=(target_col, 'count')\n",
    "        )\n",
    "        \n",
    "        # Process aggregated chunk\n",
    "        for idx, row in agg.iterrows():\n",
    "            if isinstance(idx, tuple):\n",
    "                # Multi-index case (airport or carrier level)\n",
    "                date = idx[0]\n",
    "                entity = idx[1]\n",
    "                \n",
    "                # Create a composite key\n",
    "                key = (date, entity)\n",
    "                \n",
    "                if key in agg_data:\n",
    "                    # Update existing entry with weighted average\n",
    "                    total_flights = agg_data[key]['num_flights'] + row['num_flights']\n",
    "                    agg_data[key]['avg_delay'] = (\n",
    "                        (agg_data[key]['avg_delay'] * agg_data[key]['num_flights'] +\n",
    "                         row['avg_delay'] * row['num_flights']) / total_flights\n",
    "                    )\n",
    "                    agg_data[key]['median_delay'] = (\n",
    "                        (agg_data[key]['median_delay'] * agg_data[key]['num_flights'] +\n",
    "                         row['median_delay'] * row['num_flights']) / total_flights\n",
    "                    )\n",
    "                    agg_data[key]['max_delay'] = max(agg_data[key]['max_delay'], row['max_delay'])\n",
    "                    agg_data[key]['min_delay'] = min(agg_data[key]['min_delay'], row['min_delay'])\n",
    "                    agg_data[key]['num_flights'] = total_flights\n",
    "                else:\n",
    "                    # Create new entry\n",
    "                    agg_data[key] = {\n",
    "                        'date': date,\n",
    "                        'entity': entity,\n",
    "                        'avg_delay': row['avg_delay'],\n",
    "                        'median_delay': row['median_delay'],\n",
    "                        'max_delay': row['max_delay'],\n",
    "                        'min_delay': row['min_delay'],\n",
    "                        'std_delay': row['std_delay'],\n",
    "                        'num_flights': row['num_flights']\n",
    "                    }\n",
    "            else:\n",
    "                # Single index case (daily or hourly)\n",
    "                date = idx\n",
    "                \n",
    "                if date in agg_data:\n",
    "                    # Update existing entry with weighted average\n",
    "                    total_flights = agg_data[date]['num_flights'] + row['num_flights']\n",
    "                    agg_data[date]['avg_delay'] = (\n",
    "                        (agg_data[date]['avg_delay'] * agg_data[date]['num_flights'] +\n",
    "                         row['avg_delay'] * row['num_flights']) / total_flights\n",
    "                    )\n",
    "                    agg_data[date]['median_delay'] = (\n",
    "                        (agg_data[date]['median_delay'] * agg_data[date]['num_flights'] +\n",
    "                         row['median_delay'] * row['num_flights']) / total_flights\n",
    "                    )\n",
    "                    agg_data[date]['max_delay'] = max(agg_data[date]['max_delay'], row['max_delay'])\n",
    "                    agg_data[date]['min_delay'] = min(agg_data[date]['min_delay'], row['min_delay'])\n",
    "                    agg_data[date]['num_flights'] = total_flights\n",
    "                else:\n",
    "                    # Create new entry\n",
    "                    agg_data[date] = {\n",
    "                        'date': date,\n",
    "                        'avg_delay': row['avg_delay'],\n",
    "                        'median_delay': row['median_delay'],\n",
    "                        'max_delay': row['max_delay'],\n",
    "                        'min_delay': row['min_delay'],\n",
    "                        'std_delay': row['std_delay'],\n",
    "                        'num_flights': row['num_flights']\n",
    "                    }\n",
    "    \n",
    "    # Convert aggregated data to DataFrame\n",
    "    if 'entity' in next(iter(agg_data.values())):\n",
    "        # Multi-index case\n",
    "        df_agg = pd.DataFrame([\n",
    "            {'date': v['date'], 'entity': v['entity'], 'avg_delay': v['avg_delay'],\n",
    "             'median_delay': v['median_delay'], 'max_delay': v['max_delay'],\n",
    "             'min_delay': v['min_delay'], 'std_delay': v['std_delay'],\n",
    "             'num_flights': v['num_flights']}\n",
    "            for v in agg_data.values()\n",
    "        ])\n",
    "        \n",
    "        # Set index\n",
    "        df_agg = df_agg.set_index(['date', 'entity'])\n",
    "        \n",
    "    else:\n",
    "        # Single index case\n",
    "        df_agg = pd.DataFrame([\n",
    "            {'date': v['date'], 'avg_delay': v['avg_delay'],\n",
    "             'median_delay': v['median_delay'], 'max_delay': v['max_delay'],\n",
    "             'min_delay': v['min_delay'], 'std_delay': v['std_delay'],\n",
    "             'num_flights': v['num_flights']}\n",
    "            for v in agg_data.values()\n",
    "        ])\n",
    "        \n",
    "        # Set index\n",
    "        df_agg = df_agg.set_index('date')\n",
    "    \n",
    "    # Sort by date\n",
    "    df_agg = df_agg.sort_index()\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06227264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and analyze daily aggregated time series\n",
    "ts_daily = aggregate_to_time_series(load_processed_data(BASE_PROCESSED_PATH), agg_level='daily')\n",
    "\n",
    "print(f\"Daily time series shape: {ts_daily.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(ts_daily.head())\n",
    "\n",
    "# Check the date range\n",
    "print(f\"Date range: {ts_daily.index.min()} to {ts_daily.index.max()}\")\n",
    "print(f\"Total days: {len(ts_daily)}\")\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(ts_daily.index, ts_daily['avg_delay'], linewidth=1.5)\n",
    "plt.title('Daily Average Departure Delay', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Average Delay (minutes)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2507b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for stationarity\n",
    "def check_stationarity(ts, column='avg_delay'):\n",
    "    \"\"\"\n",
    "    Check if a time series is stationary using the Augmented Dickey-Fuller test\n",
    "    \"\"\"\n",
    "    # Calculate rolling statistics\n",
    "    rolling_mean = ts[column].rolling(window=7).mean()\n",
    "    rolling_std = ts[column].rolling(window=7).std()\n",
    "    \n",
    "    # Plot rolling statistics\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(211)\n",
    "    plt.plot(ts.index, ts[column], label='Original')\n",
    "    plt.plot(ts.index, rolling_mean, label='Rolling Mean (7-day)')\n",
    "    plt.plot(ts.index, rolling_std, label='Rolling Std (7-day)')\n",
    "    plt.legend()\n",
    "    plt.title('Rolling Mean & Standard Deviation', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Perform ADF test\n",
    "    result = adfuller(ts[column].dropna())\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.bar(['ADF Statistic', 'p-value', '1%', '5%', '10%'], \n",
    "            [result[0], result[1], result[4]['1%'], result[4]['5%'], result[4]['10%']], \n",
    "            color=['blue', 'red', 'green', 'green', 'green'])\n",
    "    plt.axhline(y=result[4]['5%'], color='green', linestyle='-', alpha=0.5)\n",
    "    plt.title('Augmented Dickey-Fuller Test Results', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print test results\n",
    "    print('Augmented Dickey-Fuller Test Results:')\n",
    "    print(f'ADF Statistic: {result[0]:.4f}')\n",
    "    print(f'p-value: {result[1]:.4f}')\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'   {key}: {value:.4f}')\n",
    "    \n",
    "    # Interpret results\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"\\nConclusion: Time series is stationary (reject null hypothesis)\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\nConclusion: Time series is non-stationary (fail to reject null hypothesis)\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c52b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check stationarity of the daily average delay time series\n",
    "is_stationary = check_stationarity(ts_daily)\n",
    "\n",
    "# If not stationary, demonstrate differencing\n",
    "if not is_stationary:\n",
    "    # First-order differencing\n",
    "    ts_daily['diff1'] = ts_daily['avg_delay'].diff().dropna()\n",
    "    \n",
    "    # Check stationarity of differenced series\n",
    "    print(\"\\nChecking stationarity after first-order differencing:\")\n",
    "    is_stationary_diff1 = check_stationarity(ts_daily.dropna(), column='diff1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5233fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal decomposition\n",
    "def decompose_time_series(ts, column='avg_delay'):\n",
    "    \"\"\"\n",
    "    Decompose time series into trend, seasonal, and residual components\n",
    "    \"\"\"\n",
    "    # Fill any missing values with forward fill\n",
    "    ts_filled = ts[column].fillna(method='ffill')\n",
    "    \n",
    "    # Check if we have enough data for decomposition\n",
    "    if len(ts_filled) < 14:  # Need at least twice the period for weekly seasonality\n",
    "        print(\"Not enough data for decomposition\")\n",
    "        return None\n",
    "    \n",
    "    # Perform seasonal decomposition with additive model\n",
    "    try:\n",
    "        decomposition = seasonal_decompose(ts_filled, model='additive', period=7)  # Weekly seasonality\n",
    "        \n",
    "        # Plot decomposition\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        plt.subplot(411)\n",
    "        plt.plot(decomposition.observed)\n",
    "        plt.title('Observed', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(412)\n",
    "        plt.plot(decomposition.trend)\n",
    "        plt.title('Trend', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(413)\n",
    "        plt.plot(decomposition.seasonal)\n",
    "        plt.title('Seasonality (Weekly)', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(414)\n",
    "        plt.plot(decomposition.resid)\n",
    "        plt.title('Residuals', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return decomposition\n",
    "    except Exception as e:\n",
    "        print(f\"Error during decomposition: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e46b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the time series to understand seasonality patterns\n",
    "decomposition = decompose_time_series(ts_daily)\n",
    "\n",
    "# Analyze the weekly seasonality pattern if we have valid decomposition\n",
    "if decomposition is not None:\n",
    "    # Extract seasonal pattern (should be of length 7 for weekly)\n",
    "    seasonal_pattern = decomposition.seasonal[:7]\n",
    "    \n",
    "    # Create day labels\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    \n",
    "    # Plot the weekly pattern\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(days, seasonal_pattern)\n",
    "    plt.title('Weekly Seasonality Pattern of Flight Delays', fontsize=16)\n",
    "    plt.ylabel('Effect on Delay (minutes)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d02d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ACF and PACF plots for time series modeling\n",
    "def analyze_acf_pacf(ts, column='avg_delay', lags=40):\n",
    "    \"\"\"\n",
    "    Analyze auto-correlation and partial auto-correlation functions\n",
    "    \"\"\"\n",
    "    # Fill missing values\n",
    "    ts_filled = ts[column].fillna(method='ffill')\n",
    "    \n",
    "    # Create the plots\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    plot_acf(ts_filled, lags=lags, alpha=0.05, title='Autocorrelation Function (ACF)', ax=plt.gca())\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plot_pacf(ts_filled, lags=lags, alpha=0.05, method='ols', title='Partial Autocorrelation Function (PACF)', ax=plt.gca())\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29de7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ACF and PACF to guide model selection\n",
    "analyze_acf_pacf(ts_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e2b989",
   "metadata": {},
   "source": [
    "## Feature Engineering for Time Series Models\n",
    "\n",
    "Based on our analysis, we'll create features specifically designed for time series forecasting. These include:\n",
    "1. Lag features\n",
    "2. Rolling statistics\n",
    "3. Seasonal indicators\n",
    "4. Holiday indicators\n",
    "5. External regressors (e.g., weather data, if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series features for forecasting\n",
    "def create_ts_features(ts_data):\n",
    "    \"\"\"\n",
    "    Create features for time series forecasting\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original data\n",
    "    ts = ts_data.copy()\n",
    "    \n",
    "    # ======== LAG FEATURES ========\n",
    "    # Create lags of the target variable (useful for autoregressive models)\n",
    "    for lag in [1, 2, 3, 7, 14, 21, 28]:  # Daily, weekly, and monthly lags\n",
    "        ts[f'avg_delay_lag{lag}'] = ts['avg_delay'].shift(lag)\n",
    "    \n",
    "    # ======== ROLLING WINDOW FEATURES ========\n",
    "    # Create rolling statistics\n",
    "    for window in [3, 7, 14, 28]:\n",
    "        ts[f'avg_delay_roll_mean{window}'] = ts['avg_delay'].rolling(window=window).mean()\n",
    "        ts[f'avg_delay_roll_std{window}'] = ts['avg_delay'].rolling(window=window).std()\n",
    "        ts[f'avg_delay_roll_max{window}'] = ts['avg_delay'].rolling(window=window).max()\n",
    "        ts[f'avg_delay_roll_min{window}'] = ts['avg_delay'].rolling(window=window).min()\n",
    "    \n",
    "    # ======== CALENDAR FEATURES ========\n",
    "    # Extract date components\n",
    "    if isinstance(ts.index, pd.DatetimeIndex):\n",
    "        # Add calendar features\n",
    "        ts['month'] = ts.index.month\n",
    "        ts['day_of_week'] = ts.index.dayofweek  # 0=Monday, 6=Sunday\n",
    "        ts['day_of_year'] = ts.index.dayofyear\n",
    "        ts['quarter'] = ts.index.quarter\n",
    "        ts['week_of_year'] = ts.index.isocalendar().week\n",
    "        \n",
    "        # Add cyclical encoding of calendar features\n",
    "        ts['month_sin'] = np.sin(2 * np.pi * ts['month'] / 12)\n",
    "        ts['month_cos'] = np.cos(2 * np.pi * ts['month'] / 12)\n",
    "        ts['day_of_week_sin'] = np.sin(2 * np.pi * ts['day_of_week'] / 7)\n",
    "        ts['day_of_week_cos'] = np.cos(2 * np.pi * ts['day_of_week'] / 7)\n",
    "        \n",
    "        # Create weekend indicator\n",
    "        ts['is_weekend'] = (ts['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # ======== TREND FEATURES ========\n",
    "    # Create a simple trend feature (days since start)\n",
    "    if isinstance(ts.index, pd.DatetimeIndex):\n",
    "        ts['trend'] = (ts.index - ts.index.min()).days\n",
    "    \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d97b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create holiday indicators\n",
    "def add_holiday_indicators(ts_data):\n",
    "    \"\"\"\n",
    "    Add holiday indicators to the time series data\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original data\n",
    "    ts = ts_data.copy()\n",
    "    \n",
    "    # Check that index is a datetime index\n",
    "    if not isinstance(ts.index, pd.DatetimeIndex):\n",
    "        print(\"Error: Index is not a DatetimeIndex\")\n",
    "        return ts\n",
    "    \n",
    "    # Initialize holiday columns\n",
    "    ts['is_holiday'] = 0\n",
    "    ts['is_holiday_season'] = 0\n",
    "    \n",
    "    # Get years in the data\n",
    "    years = ts.index.year.unique()\n",
    "    \n",
    "    # Define major US holidays (simplified version)\n",
    "    holidays = []\n",
    "    for year in years:\n",
    "        # New Year's Day\n",
    "        holidays.append(pd.Timestamp(year=year, month=1, day=1))\n",
    "        \n",
    "        # Memorial Day (last Monday in May)\n",
    "        memorial_day = pd.Timestamp(year=year, month=5, day=31)\n",
    "        memorial_day = memorial_day - pd.Timedelta(days=(memorial_day.dayofweek - 0) % 7)\n",
    "        holidays.append(memorial_day)\n",
    "        \n",
    "        # Independence Day\n",
    "        holidays.append(pd.Timestamp(year=year, month=7, day=4))\n",
    "        \n",
    "        # Labor Day (first Monday in September)\n",
    "        labor_day = pd.Timestamp(year=year, month=9, day=1)\n",
    "        labor_day = labor_day + pd.Timedelta(days=(0 - labor_day.dayofweek) % 7)\n",
    "        holidays.append(labor_day)\n",
    "        \n",
    "        # Thanksgiving (fourth Thursday in November)\n",
    "        thanksgiving = pd.Timestamp(year=year, month=11, day=1)\n",
    "        thanksgiving = thanksgiving + pd.Timedelta(days=(3 - thanksgiving.dayofweek) % 7 + 21)\n",
    "        holidays.append(thanksgiving)\n",
    "        \n",
    "        # Christmas\n",
    "        holidays.append(pd.Timestamp(year=year, month=12, day=25))\n",
    "    \n",
    "    # Mark holidays\n",
    "    for holiday in holidays:\n",
    "        # Mark the actual holiday\n",
    "        if holiday in ts.index:\n",
    "            ts.loc[holiday, 'is_holiday'] = 1\n",
    "        \n",
    "        # Mark days before and after as holiday-adjacent (for travel effects)\n",
    "        for i in range(1, 3):  # 1-2 days before and after\n",
    "            before = holiday - pd.Timedelta(days=i)\n",
    "            after = holiday + pd.Timedelta(days=i)\n",
    "            \n",
    "            if before in ts.index:\n",
    "                ts.loc[before, 'is_holiday'] = 0.5  # Half weight for adjacent days\n",
    "            \n",
    "            if after in ts.index:\n",
    "                ts.loc[after, 'is_holiday'] = 0.5  # Half weight for adjacent days\n",
    "    \n",
    "    # Mark holiday seasons (Nov 15 - Jan 15)\n",
    "    for year in years:\n",
    "        # Thanksgiving to New Year's season\n",
    "        season_start = pd.Timestamp(year=year, month=11, day=15)\n",
    "        season_end = pd.Timestamp(year=year+1, month=1, day=15)\n",
    "        \n",
    "        # Mark the holiday season\n",
    "        mask = (ts.index >= season_start) & (ts.index <= season_end)\n",
    "        ts.loc[mask, 'is_holiday_season'] = 1\n",
    "    \n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c883718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process features for time series forecasting\n",
    "ts_daily_featured = create_ts_features(ts_daily)\n",
    "ts_daily_featured = add_holiday_indicators(ts_daily_featured)\n",
    "\n",
    "print(f\"Time series with features shape: {ts_daily_featured.shape}\")\n",
    "print(\"\\nFeatures created:\")\n",
    "for col in ts_daily_featured.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "# Show a sample of the featured data\n",
    "display(ts_daily_featured.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train-test split for time series\n",
    "def ts_train_test_split(ts_data, test_days=30, val_days=15):\n",
    "    \"\"\"\n",
    "    Create a time-based train-test split for time series data\n",
    "    \"\"\"\n",
    "    # Make sure the data is sorted by date\n",
    "    ts_sorted = ts_data.sort_index()\n",
    "    \n",
    "    # Identify split points\n",
    "    test_start = ts_sorted.index[-test_days]\n",
    "    val_start = test_start - pd.Timedelta(days=val_days)\n",
    "    \n",
    "    # Split the data\n",
    "    train = ts_sorted[ts_sorted.index < val_start]\n",
    "    val = ts_sorted[(ts_sorted.index >= val_start) & (ts_sorted.index < test_start)]\n",
    "    test = ts_sorted[ts_sorted.index >= test_start]\n",
    "    \n",
    "    print(f\"Train set: {len(train)} days from {train.index[0]} to {train.index[-1]}\")\n",
    "    print(f\"Validation set: {len(val)} days from {val.index[0]} to {val.index[-1]}\")\n",
    "    print(f\"Test set: {len(test)} days from {test.index[0]} to {test.index[-1]}\")\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c1b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train-test-validation split for time series\n",
    "train_ts, val_ts, test_ts = ts_train_test_split(ts_daily_featured)\n",
    "\n",
    "# Visualize the splits\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(train_ts.index, train_ts['avg_delay'], label='Train')\n",
    "plt.plot(val_ts.index, val_ts['avg_delay'], label='Validation', color='orange')\n",
    "plt.plot(test_ts.index, test_ts['avg_delay'], label='Test', color='red')\n",
    "\n",
    "plt.title('Time Series Train-Validation-Test Split', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Average Delay (minutes)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61ced8",
   "metadata": {},
   "source": [
    "## Airport-Level and Carrier-Level Time Series\n",
    "\n",
    "For more granular forecasting, we'll also create aggregations at the airport and carrier level. This enables us to forecast delays for specific airports or carriers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create airport-level time series\n",
    "ts_airport = aggregate_to_time_series(load_processed_data(BASE_PROCESSED_PATH), agg_level='airport_daily')\n",
    "\n",
    "print(f\"Airport-level time series shape: {ts_airport.shape}\")\n",
    "print(f\"Number of airports: {len(ts_airport.index.levels[1])}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(ts_airport.head())\n",
    "\n",
    "# Get top airports by number of flights\n",
    "top_airports = ts_airport.groupby(level=1)['num_flights'].sum().nlargest(5)\n",
    "print(\"\\nTop 5 airports by number of flights:\")\n",
    "print(top_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create carrier-level time series\n",
    "ts_carrier = aggregate_to_time_series(load_processed_data(BASE_PROCESSED_PATH), agg_level='carrier_daily')\n",
    "\n",
    "print(f\"Carrier-level time series shape: {ts_carrier.shape}\")\n",
    "print(f\"Number of carriers: {len(ts_carrier.index.levels[1])}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(ts_carrier.head())\n",
    "\n",
    "# Get top carriers by number of flights\n",
    "top_carriers = ts_carrier.groupby(level=1)['num_flights'].sum().nlargest(5)\n",
    "print(\"\\nTop 5 carriers by number of flights:\")\n",
    "print(top_carriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time series for top airports\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Get top 5 airports\n",
    "top_5_airports = top_airports.index\n",
    "\n",
    "# Create a 7-day moving average for each airport\n",
    "for airport in top_5_airports:\n",
    "    try:\n",
    "        airport_data = ts_airport.xs(airport, level=1)\n",
    "        plt.plot(airport_data.index, \n",
    "                 airport_data['avg_delay'].rolling(window=7).mean(), \n",
    "                 label=airport)\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting {airport}: {e}\")\n",
    "\n",
    "plt.title('7-Day Moving Average of Delays by Airport', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Average Delay (minutes)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f9668",
   "metadata": {},
   "source": [
    "## Save Processed Time Series Data\n",
    "\n",
    "We'll save the processed time series data in both CSV and pickle formats for use in time series modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e35ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed time series data\n",
    "def save_ts_data(ts_data, name, output_dir):\n",
    "    \"\"\"\n",
    "    Save time series data in multiple formats\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save as CSV\n",
    "    csv_path = os.path.join(output_dir, f\"{name}.csv\")\n",
    "    ts_data.to_csv(csv_path)\n",
    "    \n",
    "    # Save as pickle\n",
    "    pickle_path = os.path.join(output_dir, f\"{name}.pkl\")\n",
    "    ts_data.to_pickle(pickle_path)\n",
    "    \n",
    "    print(f\"Saved {name} to:\")\n",
    "    print(f\"- CSV: {csv_path}\")\n",
    "    print(f\"- Pickle: {pickle_path}\")\n",
    "    \n",
    "    return csv_path, pickle_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the daily time series with features\n",
    "save_ts_data(ts_daily_featured, \"daily_delay_ts\", TS_PROCESSED_PATH)\n",
    "\n",
    "# Save airport-level time series\n",
    "if 'ts_airport' in locals():\n",
    "    save_ts_data(ts_airport, \"airport_delay_ts\", TS_PROCESSED_PATH)\n",
    "\n",
    "# Save carrier-level time series\n",
    "if 'ts_carrier' in locals():\n",
    "    save_ts_data(ts_carrier, \"carrier_delay_ts\", TS_PROCESSED_PATH)\n",
    "\n",
    "# Save train-test-validation splits\n",
    "if 'train_ts' in locals() and 'val_ts' in locals() and 'test_ts' in locals():\n",
    "    save_ts_data(train_ts, \"train_ts\", TS_PROCESSED_PATH)\n",
    "    save_ts_data(val_ts, \"val_ts\", TS_PROCESSED_PATH)\n",
    "    save_ts_data(test_ts, \"test_ts\", TS_PROCESSED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb9814",
   "metadata": {},
   "source": [
    "## Time Series Cross-Validation\n",
    "\n",
    "For time series, standard cross-validation isn't appropriate. We'll implement time series-specific cross-validation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5496e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series cross-validation function\n",
    "def ts_cross_validation(ts_data, n_splits=3, test_days=7, val_days=7):\n",
    "    \"\"\"\n",
    "    Create time series cross-validation folds\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ts_data : pandas.DataFrame\n",
    "        Time series data with DatetimeIndex\n",
    "    n_splits : int\n",
    "        Number of validation folds to create\n",
    "    test_days : int\n",
    "        Number of days to include in each test fold\n",
    "    val_days : int\n",
    "        Number of days to include in each validation fold\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list of tuples\n",
    "        Each tuple contains (train, val, test) for a fold\n",
    "    \"\"\"\n",
    "    # Sort the data by date\n",
    "    ts_sorted = ts_data.sort_index()\n",
    "    \n",
    "    # Calculate the size of each fold\n",
    "    fold_size = test_days + val_days\n",
    "    \n",
    "    # List to store the folds\n",
    "    folds = []\n",
    "    \n",
    "    # Create folds\n",
    "    for i in range(n_splits):\n",
    "        # Calculate the end of this fold\n",
    "        fold_end = ts_sorted.index[-1] - pd.Timedelta(days=i*fold_size)\n",
    "        test_start = fold_end - pd.Timedelta(days=test_days)\n",
    "        val_start = test_start - pd.Timedelta(days=val_days)\n",
    "        \n",
    "        # Create the train, validation, and test sets\n",
    "        test = ts_sorted[(ts_sorted.index > test_start) & (ts_sorted.index <= fold_end)]\n",
    "        val = ts_sorted[(ts_sorted.index > val_start) & (ts_sorted.index <= test_start)]\n",
    "        train = ts_sorted[ts_sorted.index <= val_start]\n",
    "        \n",
    "        # Append to folds list\n",
    "        folds.append((train, val, test))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aaae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate time series cross-validation\n",
    "cv_folds = ts_cross_validation(ts_daily_featured, n_splits=3)\n",
    "\n",
    "# Visualize CV folds\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i, (train, val, test) in enumerate(cv_folds):\n",
    "    plt.subplot(len(cv_folds), 1, i+1)\n",
    "    \n",
    "    plt.plot(train.index, train['avg_delay'], label='Train', alpha=0.7)\n",
    "    plt.plot(val.index, val['avg_delay'], label='Validation', color='orange')\n",
    "    plt.plot(test.index, test['avg_delay'], label='Test', color='red')\n",
    "    \n",
    "    plt.title(f'Fold {i+1}: Train {train.index[0]} to {train.index[-1]}, '\n",
    "              f'Val {val.index[0]} to {val.index[-1]}, Test {test.index[0]} to {test.index[-1]}', \n",
    "              fontsize=14)\n",
    "    plt.ylabel('Avg Delay (min)', fontsize=12)\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c21b6",
   "metadata": {},
   "source": [
    "## Create Model-Ready Data for Different Time Series Algorithms\n",
    "\n",
    "Different time series models have different data format requirements. Let's prepare data specifically for:\n",
    "1. ARIMA/SARIMA models\n",
    "2. Prophet models\n",
    "3. Neural network-based forecasting (e.g., LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3417e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data formatted for different time series algorithms\n",
    "\n",
    "# 1. Format for ARIMA/SARIMA\n",
    "def format_for_arima(ts_data, target_col='avg_delay'):\n",
    "    \"\"\"Format data for ARIMA/SARIMA models\"\"\"\n",
    "    # For ARIMA, we typically need just the target variable as a series\n",
    "    arima_data = ts_data[target_col].copy()\n",
    "    \n",
    "    # For SARIMAX with exogenous variables, we can include other features\n",
    "    exog_cols = [col for col in ts_data.columns if col != target_col and\n",
    "                col not in ['avg_delay_lag1', 'avg_delay_lag2', 'avg_delay_lag3']]  # Exclude lags\n",
    "    \n",
    "    # Create exogenous variables dataframe\n",
    "    exog_data = ts_data[exog_cols].copy() if exog_cols else None\n",
    "    \n",
    "    return arima_data, exog_data\n",
    "\n",
    "# 2. Format for Prophet\n",
    "def format_for_prophet(ts_data, target_col='avg_delay'):\n",
    "    \"\"\"Format data for Facebook Prophet\"\"\"\n",
    "    # Prophet requires a specific DataFrame format with 'ds' and 'y' columns\n",
    "    prophet_data = pd.DataFrame({\n",
    "        'ds': ts_data.index,\n",
    "        'y': ts_data[target_col]\n",
    "    })\n",
    "    \n",
    "    # Add additional regressors (optional)\n",
    "    for col in ts_data.columns:\n",
    "        if col != target_col and 'lag' not in col:  # Exclude lags\n",
    "            prophet_data[col] = ts_data[col]\n",
    "    \n",
    "    return prophet_data\n",
    "\n",
    "# 3. Format for ML-based forecasting (LSTM)\n",
    "def format_for_lstm(ts_data, target_col='avg_delay', seq_length=7):\n",
    "    \"\"\"Format data for LSTM models\"\"\"\n",
    "    # For LSTM, we need sequences of data\n",
    "    # Prepare the data\n",
    "    data = ts_data.copy()\n",
    "    \n",
    "    # Create sequences\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        # Get sequence of features\n",
    "        features = data.iloc[i:i+seq_length].drop(columns=[target_col])\n",
    "        # Get target value (next day's value)\n",
    "        target = data.iloc[i+seq_length][target_col]\n",
    "        \n",
    "        X.append(features.values)\n",
    "        y.append(target)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Get feature names for reference\n",
    "    feature_names = data.drop(columns=[target_col]).columns\n",
    "    \n",
    "    return X, y, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model-ready data examples\n",
    "\n",
    "# Create ARIMA/SARIMA format data\n",
    "arima_data, arima_exog = format_for_arima(train_ts)\n",
    "print(f\"ARIMA data shape: {arima_data.shape}\")\n",
    "if arima_exog is not None:\n",
    "    print(f\"ARIMA exogenous data shape: {arima_exog.shape}\")\n",
    "\n",
    "# Create Prophet format data\n",
    "prophet_data = format_for_prophet(train_ts)\n",
    "print(f\"\\nProphet data shape: {prophet_data.shape}\")\n",
    "print(\"Prophet data columns:\", prophet_data.columns.tolist()[:10], \"...\")\n",
    "\n",
    "# Create LSTM format data\n",
    "lstm_X, lstm_y, lstm_features = format_for_lstm(train_ts)\n",
    "print(f\"\\nLSTM X shape: {lstm_X.shape}\")\n",
    "print(f\"LSTM y shape: {lstm_y.shape}\")\n",
    "print(f\"LSTM features: {len(lstm_features)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f058e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model-ready data\n",
    "def save_model_ready_data(data_dict, output_dir):\n",
    "    \"\"\"Save model-ready data for different algorithms\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save each item in the dictionary\n",
    "    for name, data in data_dict.items():\n",
    "        # Define the file path\n",
    "        file_path = os.path.join(output_dir, f\"{name}.pkl\")\n",
    "        \n",
    "        # Save the data\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        \n",
    "        print(f\"Saved {name} to {file_path}\")\n",
    "\n",
    "# Create a dictionary of data to save\n",
    "model_ready_data = {\n",
    "    'arima_data': arima_data,\n",
    "    'arima_exog': arima_exog,\n",
    "    'prophet_data': prophet_data,\n",
    "    'lstm_data': (lstm_X, lstm_y, lstm_features)\n",
    "}\n",
    "\n",
    "# Save the data\n",
    "save_model_ready_data(model_ready_data, os.path.join(TS_PROCESSED_PATH, 'model_ready'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285aff69",
   "metadata": {},
   "source": [
    "## Summary of Time Series Preprocessing\n",
    "\n",
    "The time series preprocessing pipeline has:\n",
    "\n",
    "1. Aggregated flight data at different temporal and entity levels (daily, airport, carrier)\n",
    "2. Analyzed time series characteristics including stationarity and seasonality\n",
    "3. Created lag features, rolling statistics, and seasonal indicators\n",
    "4. Prepared train-test splits with time-based validation\n",
    "5. Implemented time series cross-validation strategies\n",
    "6. Formatted data for different time series algorithms (ARIMA, Prophet, LSTM)\n",
    "7. Saved processed data in multiple formats for time series modeling\n",
    "\n",
    "This time series-ready dataset is optimized for forecasting models and includes features designed to capture temporal patterns in flight delays. The preprocessing ensures proper handling of time-based dependencies and provides multiple aggregation levels for different forecasting tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
