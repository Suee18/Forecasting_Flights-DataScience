{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa650db",
   "metadata": {},
   "source": [
    "# Machine Learning Preprocessing Pipeline for Flight Delay Data\n",
    "\n",
    "This notebook builds upon the base preprocessing pipeline to create features specifically optimized for traditional machine learning models like XGBoost, Random Forest, etc. These models typically require structured tabular data with well-engineered features, proper encoding, and handling of outliers.\n",
    "\n",
    "## Key Processing Steps:\n",
    "1. Loading the base preprocessed data\n",
    "2. Feature engineering specific to ML models\n",
    "3. Categorical encoding (one-hot, ordinal, target)\n",
    "4. Handling outliers\n",
    "5. Feature selection/importance analysis\n",
    "6. Feature scaling/normalization\n",
    "7. Train-test splitting with time-based validation\n",
    "8. Exporting the processed data for ML model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a6c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base processed data path: /Users/osx/DataSceince_FL_FR/Forecasting_Flights-DataScience/data/processed/base_preprocessed_flights.csv\n",
      "ML processed data path: /Users/osx/DataSceince_FL_FR/Forecasting_Flights-DataScience/data/processed/ml_ready_flights/ml_ready_flights.csv\n",
      "ML model path: /Users/osx/DataSceince_FL_FR/Forecasting_Flights-DataScience/models\n",
      "Libraries and paths configured.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure paths dynamically using relative paths\n",
    "import os.path as path\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "notebook_dir = path.dirname(path.abspath('__file__'))\n",
    "# Get project root (parent of notebooks directory)\n",
    "project_root = path.abspath(path.join(notebook_dir, '..', '..'))\n",
    "\n",
    "# Define paths relative to project root\n",
    "BASE_PROCESSED_PATH = path.join(project_root, 'data', 'processed', 'base_preprocessed_flights.csv')\n",
    "ML_PROCESSED_PATH = path.join(project_root, 'data', 'processed','ml_ready_flights', 'ml_ready_flights.csv')\n",
    "ML_MODEL_PATH = path.join(project_root, 'models')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(os.path.dirname(ML_PROCESSED_PATH), exist_ok=True)\n",
    "os.makedirs(ML_MODEL_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Base processed data path: {BASE_PROCESSED_PATH}\")\n",
    "print(f\"ML processed data path: {ML_PROCESSED_PATH}\")\n",
    "print(f\"ML model path: {ML_MODEL_PATH}\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Libraries and paths configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b97862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data in chunks\n",
    "def load_processed_data(file_path, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    Generator function to load preprocessed data in chunks\n",
    "    \"\"\"\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        # Convert date columns to datetime\n",
    "        date_columns = [col for col in chunk.columns if 'DATE' in col.upper()]\n",
    "        for col in date_columns:\n",
    "            chunk[col] = pd.to_datetime(chunk[col], errors='coerce')\n",
    "        \n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6591e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape of first chunk: (500000, 41)\n",
      "\n",
      "Columns and data types:\n",
      "- FL_DATE: datetime64[ns]\n",
      "- AIRLINE: object\n",
      "- AIRLINE_DOT: object\n",
      "- AIRLINE_CODE: object\n",
      "- DOT_CODE: int64\n",
      "- FL_NUMBER: int64\n",
      "- ORIGIN: object\n",
      "- ORIGIN_CITY: object\n",
      "- DEST: object\n",
      "- DEST_CITY: object\n",
      "- CRS_DEP_TIME: int64\n",
      "- DEP_TIME: int64\n",
      "- DEP_DELAY: float64\n",
      "- TAXI_OUT: float64\n",
      "- WHEELS_OFF: float64\n",
      "- WHEELS_ON: float64\n",
      "- TAXI_IN: float64\n",
      "- CRS_ARR_TIME: int64\n",
      "- ARR_TIME: float64\n",
      "- ARR_DELAY: float64\n",
      "- CANCELLED: int64\n",
      "- CANCELLATION_CODE: float64\n",
      "- DIVERTED: int64\n",
      "- CRS_ELAPSED_TIME: float64\n",
      "- ELAPSED_TIME: float64\n",
      "- AIR_TIME: float64\n",
      "- DISTANCE: float64\n",
      "- DELAY_DUE_CARRIER: float64\n",
      "- DELAY_DUE_WEATHER: float64\n",
      "- DELAY_DUE_NAS: float64\n",
      "- DELAY_DUE_SECURITY: float64\n",
      "- DELAY_DUE_LATE_AIRCRAFT: float64\n",
      "- YEAR: int64\n",
      "- QUARTER: int64\n",
      "- MONTH: int64\n",
      "- DAY_OF_MONTH: int64\n",
      "- DAY_OF_WEEK: int64\n",
      "- SEASON: int64\n",
      "- IS_HOLIDAY_SEASON: int64\n",
      "- DEP_HOUR: int64\n",
      "- TIME_OF_DAY: object\n",
      "\n",
      "Sample data (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>AIRLINE_DOT</th>\n",
       "      <th>AIRLINE_CODE</th>\n",
       "      <th>DOT_CODE</th>\n",
       "      <th>FL_NUMBER</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>ORIGIN_CITY</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEST_CITY</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <th>TAXI_IN</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>CANCELLATION_CODE</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>DELAY_DUE_CARRIER</th>\n",
       "      <th>DELAY_DUE_WEATHER</th>\n",
       "      <th>DELAY_DUE_NAS</th>\n",
       "      <th>DELAY_DUE_SECURITY</th>\n",
       "      <th>DELAY_DUE_LATE_AIRCRAFT</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>QUARTER</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>IS_HOLIDAY_SEASON</th>\n",
       "      <th>DEP_HOUR</th>\n",
       "      <th>TIME_OF_DAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "      <td>United Air Lines Inc.: UA</td>\n",
       "      <td>UA</td>\n",
       "      <td>19977</td>\n",
       "      <td>1562</td>\n",
       "      <td>FLL</td>\n",
       "      <td>Fort Lauderdale, FL</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark, NJ</td>\n",
       "      <td>715</td>\n",
       "      <td>711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>1443.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>901</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>Delta Air Lines Inc.</td>\n",
       "      <td>Delta Air Lines Inc.: DL</td>\n",
       "      <td>DL</td>\n",
       "      <td>19790</td>\n",
       "      <td>1149</td>\n",
       "      <td>MSP</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>SEA</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>1280</td>\n",
       "      <td>1274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2123.0</td>\n",
       "      <td>2232.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1395</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>Evening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "      <td>United Air Lines Inc.: UA</td>\n",
       "      <td>UA</td>\n",
       "      <td>19977</td>\n",
       "      <td>459</td>\n",
       "      <td>DEN</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>MSP</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>594</td>\n",
       "      <td>600</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>1247.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>772</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>Delta Air Lines Inc.</td>\n",
       "      <td>Delta Air Lines Inc.: DL</td>\n",
       "      <td>DL</td>\n",
       "      <td>19790</td>\n",
       "      <td>2295</td>\n",
       "      <td>MSP</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>969</td>\n",
       "      <td>968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1109</td>\n",
       "      <td>1853.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>Southwest Airlines Co.</td>\n",
       "      <td>Southwest Airlines Co.: WN</td>\n",
       "      <td>WN</td>\n",
       "      <td>19393</td>\n",
       "      <td>665</td>\n",
       "      <td>DAL</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>OKC</td>\n",
       "      <td>Oklahoma City, OK</td>\n",
       "      <td>610</td>\n",
       "      <td>757</td>\n",
       "      <td>147.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>670</td>\n",
       "      <td>1331.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FL_DATE                 AIRLINE                 AIRLINE_DOT AIRLINE_CODE  \\\n",
       "0 2019-01-09   United Air Lines Inc.   United Air Lines Inc.: UA           UA   \n",
       "1 2022-11-19    Delta Air Lines Inc.    Delta Air Lines Inc.: DL           DL   \n",
       "2 2022-07-22   United Air Lines Inc.   United Air Lines Inc.: UA           UA   \n",
       "3 2023-03-06    Delta Air Lines Inc.    Delta Air Lines Inc.: DL           DL   \n",
       "4 2019-07-31  Southwest Airlines Co.  Southwest Airlines Co.: WN           WN   \n",
       "\n",
       "   DOT_CODE  FL_NUMBER ORIGIN          ORIGIN_CITY DEST          DEST_CITY  \\\n",
       "0     19977       1562    FLL  Fort Lauderdale, FL  EWR         Newark, NJ   \n",
       "1     19790       1149    MSP      Minneapolis, MN  SEA        Seattle, WA   \n",
       "2     19977        459    DEN           Denver, CO  MSP    Minneapolis, MN   \n",
       "3     19790       2295    MSP      Minneapolis, MN  SFO  San Francisco, CA   \n",
       "4     19393        665    DAL           Dallas, TX  OKC  Oklahoma City, OK   \n",
       "\n",
       "   CRS_DEP_TIME  DEP_TIME  DEP_DELAY  TAXI_OUT  WHEELS_OFF  WHEELS_ON  \\\n",
       "0           715       711        0.0      19.0      1210.0     1443.0   \n",
       "1          1280      1274        0.0       9.0      2123.0     2232.0   \n",
       "2           594       600        6.0      20.0      1020.0     1247.0   \n",
       "3           969       968        0.0      27.0      1635.0     1844.0   \n",
       "4           610       757      147.0      15.0      1252.0     1328.0   \n",
       "\n",
       "   TAXI_IN  CRS_ARR_TIME  ARR_TIME  ARR_DELAY  CANCELLED  CANCELLATION_CODE  \\\n",
       "0      4.0           901    1447.0        0.0          0                NaN   \n",
       "1     38.0          1395    2310.0        0.0          0                NaN   \n",
       "2      5.0           772    1252.0        0.0          0                NaN   \n",
       "3      9.0          1109    1853.0       24.0          0                NaN   \n",
       "4      3.0           670    1331.0      141.0          0                NaN   \n",
       "\n",
       "   DIVERTED  CRS_ELAPSED_TIME  ELAPSED_TIME  AIR_TIME  DISTANCE  \\\n",
       "0         0             186.0         176.0     153.0    1065.0   \n",
       "1         0             235.0         236.0     189.0    1399.0   \n",
       "2         0             118.0         112.0      87.0     680.0   \n",
       "3         0             260.0         285.0     249.0    1589.0   \n",
       "4         0              60.0          54.0      36.0     181.0   \n",
       "\n",
       "   DELAY_DUE_CARRIER  DELAY_DUE_WEATHER  DELAY_DUE_NAS  DELAY_DUE_SECURITY  \\\n",
       "0                NaN                NaN            NaN                 NaN   \n",
       "1                NaN                NaN            NaN                 NaN   \n",
       "2                NaN                NaN            NaN                 NaN   \n",
       "3                0.0                0.0           24.0                 0.0   \n",
       "4              141.0                0.0            0.0                 0.0   \n",
       "\n",
       "   DELAY_DUE_LATE_AIRCRAFT  YEAR  QUARTER  MONTH  DAY_OF_MONTH  DAY_OF_WEEK  \\\n",
       "0                      NaN  2019        1      1             9            2   \n",
       "1                      NaN  2022        4     11            19            5   \n",
       "2                      NaN  2022        3      7            22            4   \n",
       "3                      0.0  2023        1      3             6            0   \n",
       "4                      0.0  2019        3      7            31            2   \n",
       "\n",
       "   SEASON  IS_HOLIDAY_SEASON  DEP_HOUR TIME_OF_DAY  \n",
       "0       1                  0        11     Morning  \n",
       "1       4                  1        21     Evening  \n",
       "2       3                  0         9     Morning  \n",
       "3       2                  0        16   Afternoon  \n",
       "4       3                  0        10     Morning  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the data\n",
    "first_chunk = next(load_processed_data(BASE_PROCESSED_PATH))\n",
    "\n",
    "print(f\"Data shape of first chunk: {first_chunk.shape}\")\n",
    "print(\"\\nColumns and data types:\")\n",
    "for col in first_chunk.columns:\n",
    "    print(f\"- {col}: {first_chunk[col].dtype}\")\n",
    "\n",
    "print(\"\\nSample data (first 5 rows):\")\n",
    "display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa2a77",
   "metadata": {},
   "source": [
    "## Feature Engineering Specific to Machine Learning Models\n",
    "\n",
    "We'll define functions to create additional features that are particularly useful for machine learning models. These include:\n",
    "\n",
    "1. Categorical encoding\n",
    "2. Route-based features\n",
    "3. Carrier-based features\n",
    "4. Temporal patterns\n",
    "5. Airport congestion metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff53b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ML-specific features\n",
    "def create_ml_features(df):\n",
    "    \"\"\"\n",
    "    Create features specifically useful for ML models\n",
    "    \"\"\"\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # ======== ROUTE AND AIRPORT FEATURES ========\n",
    "    \n",
    "    # Create route features if origin and destination are present\n",
    "    if 'ORIGIN' in df_featured.columns and 'DEST' in df_featured.columns:\n",
    "        # Create a combined route feature\n",
    "        df_featured['ROUTE'] = df_featured['ORIGIN'] + '_' + df_featured['DEST']\n",
    "        \n",
    "        # Create distance buckets (if distance is available)\n",
    "        if 'DISTANCE' in df_featured.columns:\n",
    "            df_featured['DISTANCE_GROUP'] = pd.cut(\n",
    "                df_featured['DISTANCE'],\n",
    "                bins=[0, 500, 1000, 1500, 2000, 2500, 5000],\n",
    "                labels=['Very Short', 'Short', 'Medium Short', 'Medium', 'Medium Long', 'Long']\n",
    "            )\n",
    "    \n",
    "    # ======== TEMPORAL FEATURES ========\n",
    "    \n",
    "    # Create cyclical features for time variables\n",
    "    if 'MONTH' in df_featured.columns:\n",
    "        # Cyclical encoding of month (1-12)\n",
    "        df_featured['MONTH_SIN'] = np.sin(2 * np.pi * df_featured['MONTH'] / 12)\n",
    "        df_featured['MONTH_COS'] = np.cos(2 * np.pi * df_featured['MONTH'] / 12)\n",
    "    \n",
    "    if 'DAY_OF_WEEK' in df_featured.columns:\n",
    "        # Cyclical encoding of day of week (1-7)\n",
    "        df_featured['DAY_OF_WEEK_SIN'] = np.sin(2 * np.pi * df_featured['DAY_OF_WEEK'] / 7)\n",
    "        df_featured['DAY_OF_WEEK_COS'] = np.cos(2 * np.pi * df_featured['DAY_OF_WEEK'] / 7)\n",
    "    \n",
    "    if 'DEP_HOUR' in df_featured.columns:\n",
    "        # Cyclical encoding of hour (0-23)\n",
    "        df_featured['DEP_HOUR_SIN'] = np.sin(2 * np.pi * df_featured['DEP_HOUR'] / 24)\n",
    "        df_featured['DEP_HOUR_COS'] = np.cos(2 * np.pi * df_featured['DEP_HOUR'] / 24)\n",
    "    \n",
    "    # ======== FLIGHT-SPECIFIC FEATURES ========\n",
    "    \n",
    "    # Create a departure vs. scheduled departure difference\n",
    "    if 'DEP_TIME' in df_featured.columns and 'CRS_DEP_TIME' in df_featured.columns:\n",
    "        try:\n",
    "            df_featured['DEP_DIFF'] = df_featured['DEP_TIME'] - df_featured['CRS_DEP_TIME']\n",
    "        except:\n",
    "            print(\"Could not calculate DEP_DIFF\")\n",
    "    \n",
    "    # Combine delay types (if available)\n",
    "    delay_cols = ['CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY']\n",
    "    if all(col in df_featured.columns for col in delay_cols):\n",
    "        # Fill NaNs with 0 to avoid issues in summation\n",
    "        for col in delay_cols:\n",
    "            df_featured[col] = df_featured[col].fillna(0)\n",
    "            \n",
    "        # Create delay type features\n",
    "        df_featured['TOTAL_DELAY'] = df_featured[delay_cols].sum(axis=1)\n",
    "        \n",
    "        # Calculate percentage of each delay type\n",
    "        for col in delay_cols:\n",
    "            df_featured[f'{col}_RATIO'] = (df_featured[col] / df_featured['TOTAL_DELAY']).fillna(0)\n",
    "    \n",
    "    # ======== INTERACTION FEATURES ========\n",
    "    \n",
    "    # Create interaction features between important variables\n",
    "    if 'DAY_OF_WEEK' in df_featured.columns and 'DEP_HOUR' in df_featured.columns:\n",
    "        df_featured['DAY_HOUR'] = df_featured['DAY_OF_WEEK'].astype(str) + '_' + df_featured['DEP_HOUR'].astype(str)\n",
    "    \n",
    "    if 'MONTH' in df_featured.columns and 'DAY_OF_WEEK' in df_featured.columns:\n",
    "        df_featured['MONTH_DAY'] = df_featured['MONTH'].astype(str) + '_' + df_featured['DAY_OF_WEEK'].astype(str)\n",
    "    \n",
    "    return df_featured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b982f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers for ML models\n",
    "def handle_outliers_ml(df, target_col='DEP_DELAY'):\n",
    "    \"\"\"\n",
    "    Handle outliers in a way suitable for ML models\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Define numeric columns that might have outliers\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove target column from the list if it's there\n",
    "    if target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "    \n",
    "    # Define columns to check for outliers\n",
    "    outlier_columns = ['DEP_DELAY', 'ARR_DELAY', 'TAXI_OUT', 'TAXI_IN', \n",
    "                      'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE']\n",
    "    \n",
    "    # Filter to keep only columns that exist in the dataframe\n",
    "    outlier_columns = [col for col in outlier_columns if col in df_clean.columns]\n",
    "    \n",
    "    # Handle outliers for each column\n",
    "    for col in outlier_columns:\n",
    "        # Skip non-numeric columns\n",
    "        if df_clean[col].dtype not in [np.number]:\n",
    "            continue\n",
    "            \n",
    "        # Skip columns already processed in base pipeline\n",
    "        if col not in df_clean.columns:\n",
    "            continue\n",
    "        \n",
    "        # Calculate IQR\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define bounds\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Cap outliers (winsorize)\n",
    "        if col != target_col:  # Don't cap the target variable\n",
    "            df_clean[col] = df_clean[col].clip(lower_bound, upper_bound)\n",
    "        else:\n",
    "            # For the target column, we might want to keep outliers but flag them\n",
    "            df_clean[f'{col}_IS_OUTLIER'] = ((df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)).astype(int)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ec5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ML-ready feature encoders\n",
    "def create_categorical_encoders(df, categorical_cols=None, target_col='DEP_DELAY'):\n",
    "    \"\"\"\n",
    "    Create encoders for categorical variables\n",
    "    \"\"\"\n",
    "    if categorical_cols is None:\n",
    "        # Identify categorical columns automatically\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Create a dictionary to store encoders\n",
    "    encoders = {}\n",
    "    \n",
    "    # One-hot encoding for low-cardinality categoricals\n",
    "    encoders['onehot'] = {}\n",
    "    \n",
    "    # Ordinal encoding for high-cardinality categoricals\n",
    "    encoders['ordinal'] = {}\n",
    "    \n",
    "    # Determine cardinality of each categorical\n",
    "    for col in categorical_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        n_unique = df[col].nunique()\n",
    "        \n",
    "        # Create appropriate encoder based on cardinality\n",
    "        if n_unique <= 15:  # Low cardinality\n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "            encoder.fit(df[[col]])\n",
    "            encoders['onehot'][col] = encoder\n",
    "        else:  # High cardinality\n",
    "            encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "            encoder.fit(df[[col]])\n",
    "            encoders['ordinal'][col] = encoder\n",
    "    \n",
    "    return encoders, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94fa7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply categorical encoders\n",
    "def apply_categorical_encodings(df, encoders, categorical_cols):\n",
    "    \"\"\"\n",
    "    Apply the fitted encoders to transform categorical data\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Apply one-hot encoding\n",
    "    for col, encoder in encoders['onehot'].items():\n",
    "        if col in df_encoded.columns:\n",
    "            # Transform the data\n",
    "            encoded_array = encoder.transform(df_encoded[[col]])\n",
    "            \n",
    "            # Create new column names\n",
    "            feature_names = [f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "            \n",
    "            # Create a DataFrame with the encoded values\n",
    "            encoded_df = pd.DataFrame(encoded_array, columns=feature_names, index=df_encoded.index)\n",
    "            \n",
    "            # Concatenate with the original DataFrame\n",
    "            df_encoded = pd.concat([df_encoded, encoded_df], axis=1)\n",
    "            \n",
    "            # Drop the original categorical column\n",
    "            df_encoded = df_encoded.drop(col, axis=1)\n",
    "    \n",
    "    # Apply ordinal encoding\n",
    "    for col, encoder in encoders['ordinal'].items():\n",
    "        if col in df_encoded.columns:\n",
    "            # Transform the data\n",
    "            encoded_array = encoder.transform(df_encoded[[col]])\n",
    "            \n",
    "            # Replace the original column with the encoded values\n",
    "            df_encoded[col] = encoded_array\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a466da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for ML\n",
    "def select_features_ml(df, target_col='DEP_DELAY', k=20):\n",
    "    \"\"\"\n",
    "    Select top k features based on correlation with target\n",
    "    \"\"\"\n",
    "    # Split data into X and y\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Keep only numeric columns\n",
    "    numeric_X = X.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Drop any constant columns\n",
    "    non_constant_cols = [col for col in numeric_X.columns if numeric_X[col].nunique() > 1]\n",
    "    numeric_X = numeric_X[non_constant_cols]\n",
    "    \n",
    "    # Calculate correlation with target\n",
    "    correlations = numeric_X.corrwith(y).abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Select top k features\n",
    "    top_features = correlations.nlargest(min(k, len(correlations))).index.tolist()\n",
    "    \n",
    "    # Add target column back\n",
    "    selected_features = top_features + [target_col]\n",
    "    \n",
    "    return selected_features, correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dbabb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling for ML models\n",
    "def scale_features_ml(df, target_col='DEP_DELAY', scaler_type='standard'):\n",
    "    \"\"\"\n",
    "    Scale features for ML models\n",
    "    \"\"\"\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Get numeric columns excluding the target\n",
    "    numeric_cols = df_scaled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "    \n",
    "    # Choose scaler\n",
    "    if scaler_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(\"scaler_type must be 'standard' or 'minmax'\")\n",
    "    \n",
    "    # Scale numeric columns\n",
    "    if numeric_cols:\n",
    "        df_scaled[numeric_cols] = scaler.fit_transform(df_scaled[numeric_cols])\n",
    "    \n",
    "    return df_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69d5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based train-test split for ML models\n",
    "def time_based_train_test_split(df, date_col='FL_DATE', test_size=0.2, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Split data based on time to prevent data leakage\n",
    "    \"\"\"\n",
    "    if date_col not in df.columns:\n",
    "        raise ValueError(f\"{date_col} not found in dataframe\")\n",
    "    \n",
    "    # Sort by date\n",
    "    df_sorted = df.sort_values(date_col)\n",
    "    \n",
    "    # Determine split points\n",
    "    n_samples = len(df_sorted)\n",
    "    test_start_idx = int(n_samples * (1 - test_size))\n",
    "    val_start_idx = int(n_samples * (1 - test_size - val_size))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = df_sorted.iloc[:val_start_idx]\n",
    "    val_data = df_sorted.iloc[val_start_idx:test_start_idx]\n",
    "    test_data = df_sorted.iloc[test_start_idx:]\n",
    "    \n",
    "    print(f\"Train set: {len(train_data):,} rows from {train_data[date_col].min()} to {train_data[date_col].max()}\")\n",
    "    print(f\"Validation set: {len(val_data):,} rows from {val_data[date_col].min()} to {val_data[date_col].max()}\")\n",
    "    print(f\"Test set: {len(test_data):,} rows from {test_data[date_col].min()} to {test_data[date_col].max()}\")\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60c052",
   "metadata": {},
   "source": [
    "## Execute the ML Preprocessing Pipeline\n",
    "\n",
    "Now we'll run the complete ML preprocessing pipeline on our base preprocessed data to prepare it for machine learning model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a3eae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to handle missing values\n",
    "def drop_90pct_missing_per_column(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    For columns with missingness > threshold (e.g., 0.5 or 50%),\n",
    "    drop 90% of rows with missing values in those columns.\n",
    "    This helps reduce bias while keeping some data for training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input dataframe\n",
    "    threshold : float, default=0.5\n",
    "        The threshold of missingness percentage to consider a column\n",
    "        for partial row dropping\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with some rows dropped for high-missingness columns\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Calculate percentage of missing values per column\n",
    "    missingness = df_result.isnull().mean()\n",
    "    \n",
    "    # Get columns with high missingness\n",
    "    high_missing_cols = missingness[missingness > threshold].index.tolist()\n",
    "    \n",
    "    for col in high_missing_cols:\n",
    "        # Get indices of rows with missing values in this column\n",
    "        missing_indices = df_result[df_result[col].isnull()].index\n",
    "        \n",
    "        # Calculate how many rows to drop (90% of missing)\n",
    "        num_to_drop = int(len(missing_indices) * 0.9)\n",
    "        \n",
    "        # Randomly select indices to drop\n",
    "        if num_to_drop > 0:\n",
    "            indices_to_drop = np.random.choice(missing_indices, size=num_to_drop, replace=False)\n",
    "            \n",
    "            # Drop the selected rows\n",
    "            df_result = df_result.drop(indices_to_drop)\n",
    "            \n",
    "            print(f\"Dropped {num_to_drop} rows with missing values in column '{col}'\")\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c000ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process chunks with the ML pipeline\n",
    "def process_chunk_ml(chunk, encoders=None, categorical_cols=None, feature_list=None):\n",
    "    \"\"\"\n",
    "    Apply ML preprocessing to a chunk of base preprocessed data\n",
    "    \"\"\"\n",
    "    # Handle missing values first: for columns with >50% missing, drop 90% of missing rows\n",
    "    chunk = drop_90pct_missing_per_column(chunk, threshold=0.5)\n",
    "    # Add ML-specific features\n",
    "    chunk_ml = create_ml_features(chunk)\n",
    "    # Handle outliers\n",
    "    chunk_ml = handle_outliers_ml(chunk_ml)\n",
    "    # Fit or apply categorical encoders\n",
    "    if encoders is None:\n",
    "        encoders, categorical_cols = create_categorical_encoders(chunk_ml)\n",
    "    chunk_ml = apply_categorical_encodings(chunk_ml, encoders, categorical_cols)\n",
    "    # Select features if feature_list provided\n",
    "    if feature_list is not None:\n",
    "        available_features = [col for col in feature_list if col in chunk_ml.columns]\n",
    "        chunk_ml = chunk_ml[available_features]\n",
    "    return chunk_ml, encoders, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "313c4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all chunks and prepare ML-ready dataset\n",
    "def prepare_ml_dataset(input_file, output_file, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    Process all chunks and prepare an ML-ready dataset\n",
    "    \"\"\"\n",
    "    encoders = None\n",
    "    categorical_cols = None\n",
    "    feature_list = None\n",
    "    sample_for_feature_selection = None\n",
    "    first_chunk = True\n",
    "    \n",
    "    print(f\"Starting ML preprocessing of {input_file}...\")\n",
    "    \n",
    "    # Process in chunks\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(load_processed_data(input_file, chunk_size=chunk_size)):\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Process the chunk\n",
    "        processed_chunk, encoders, categorical_cols = process_chunk_ml(chunk, encoders, categorical_cols)\n",
    "        \n",
    "        # If this is the first chunk, use it for feature selection\n",
    "        if i == 0:\n",
    "            # Save a sample for feature selection\n",
    "            if len(processed_chunk) > 10000:\n",
    "                sample_for_feature_selection = processed_chunk.sample(10000, random_state=42)\n",
    "            else:\n",
    "                sample_for_feature_selection = processed_chunk\n",
    "        \n",
    "        chunks.append(processed_chunk)\n",
    "        \n",
    "        # Print progress\n",
    "        end_time = datetime.now()\n",
    "        elapsed = (end_time - start_time).total_seconds()\n",
    "        print(f\"Processed chunk {i+1}: {len(processed_chunk):,} rows in {elapsed:.2f} seconds\")\n",
    "        \n",
    "        # To save memory, periodically combine and save chunks\n",
    "        if len(chunks) >= 5 or (i == 0 and first_chunk):\n",
    "            combined = pd.concat(chunks)\n",
    "            \n",
    "            # If this is the first batch, do feature selection\n",
    "            if first_chunk:\n",
    "                print(\"Performing feature selection...\")\n",
    "                selected_features, correlations = select_features_ml(sample_for_feature_selection)\n",
    "                feature_list = selected_features\n",
    "                print(f\"Selected {len(feature_list)} features\")\n",
    "                \n",
    "                # Keep only selected features\n",
    "                available_features = [col for col in feature_list if col in combined.columns]\n",
    "                combined = combined[available_features]\n",
    "                \n",
    "                # Set a flag to indicate we've done feature selection\n",
    "                first_chunk = False\n",
    "                \n",
    "                # Save with mode='w' (write) for first chunk\n",
    "                combined.to_csv(output_file, index=False)\n",
    "            else:\n",
    "                # Keep only selected features\n",
    "                available_features = [col for col in feature_list if col in combined.columns]\n",
    "                combined = combined[available_features]\n",
    "                \n",
    "                # Save with mode='a' (append) for subsequent chunks\n",
    "                combined.to_csv(output_file, mode='a', header=False, index=False)\n",
    "                \n",
    "            # Clear chunks list to free memory\n",
    "            chunks = []\n",
    "    \n",
    "    # Save any remaining chunks\n",
    "    if chunks:\n",
    "        combined = pd.concat(chunks)\n",
    "        \n",
    "        # Keep only selected features\n",
    "        if feature_list is not None:\n",
    "            available_features = [col for col in feature_list if col in combined.columns]\n",
    "            combined = combined[available_features]\n",
    "            \n",
    "        # Append to the output file\n",
    "        combined.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    print(f\"ML preprocessing complete! Output saved to {output_file}\")\n",
    "    \n",
    "    # Return for subsequent operations\n",
    "    return encoders, categorical_cols, feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b665d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML preprocessing of /Users/osx/DataSceince_FL_FR/Forecasting_Flights-DataScience/data/processed/base_preprocessed_flights.csv...\n",
      "Dropped 450000 rows with missing values in column 'CANCELLATION_CODE'\n",
      "Dropped 36053 rows with missing values in column 'DELAY_DUE_CARRIER'\n",
      "Dropped 3605 rows with missing values in column 'DELAY_DUE_WEATHER'\n",
      "Dropped 360 rows with missing values in column 'DELAY_DUE_NAS'\n",
      "Dropped 36 rows with missing values in column 'DELAY_DUE_SECURITY'\n",
      "Dropped 4 rows with missing values in column 'DELAY_DUE_LATE_AIRCRAFT'\n",
      "Processed chunk 1: 9,942 rows in 0.38 seconds\n",
      "Performing feature selection...\n",
      "Selected 21 features\n",
      "Dropped 450000 rows with missing values in column 'CANCELLATION_CODE'\n",
      "Dropped 35982 rows with missing values in column 'DELAY_DUE_CARRIER'\n",
      "Dropped 3599 rows with missing values in column 'DELAY_DUE_WEATHER'\n",
      "Dropped 360 rows with missing values in column 'DELAY_DUE_NAS'\n",
      "Dropped 36 rows with missing values in column 'DELAY_DUE_SECURITY'\n",
      "Dropped 3 rows with missing values in column 'DELAY_DUE_LATE_AIRCRAFT'\n",
      "Processed chunk 2: 10,020 rows in 0.34 seconds\n",
      "Dropped 450000 rows with missing values in column 'CANCELLATION_CODE'\n",
      "Dropped 36223 rows with missing values in column 'DELAY_DUE_CARRIER'\n",
      "Dropped 3622 rows with missing values in column 'DELAY_DUE_WEATHER'\n",
      "Dropped 362 rows with missing values in column 'DELAY_DUE_NAS'\n",
      "Dropped 36 rows with missing values in column 'DELAY_DUE_SECURITY'\n",
      "Dropped 4 rows with missing values in column 'DELAY_DUE_LATE_AIRCRAFT'\n",
      "Processed chunk 3: 9,753 rows in 0.33 seconds\n",
      "Dropped 450000 rows with missing values in column 'CANCELLATION_CODE'\n",
      "Dropped 36126 rows with missing values in column 'DELAY_DUE_CARRIER'\n",
      "Dropped 3612 rows with missing values in column 'DELAY_DUE_WEATHER'\n",
      "Dropped 361 rows with missing values in column 'DELAY_DUE_NAS'\n",
      "Dropped 36 rows with missing values in column 'DELAY_DUE_SECURITY'\n",
      "Dropped 4 rows with missing values in column 'DELAY_DUE_LATE_AIRCRAFT'\n",
      "Processed chunk 4: 9,861 rows in 0.32 seconds\n",
      "Dropped 423240 rows with missing values in column 'CANCELLATION_CODE'\n",
      "Dropped 33936 rows with missing values in column 'DELAY_DUE_CARRIER'\n",
      "Dropped 3393 rows with missing values in column 'DELAY_DUE_WEATHER'\n",
      "Dropped 340 rows with missing values in column 'DELAY_DUE_NAS'\n",
      "Dropped 34 rows with missing values in column 'DELAY_DUE_SECURITY'\n",
      "Dropped 3 rows with missing values in column 'DELAY_DUE_LATE_AIRCRAFT'\n",
      "Processed chunk 5: 9,321 rows in 0.30 seconds\n",
      "ML preprocessing complete! Output saved to /Users/osx/DataSceince_FL_FR/Forecasting_Flights-DataScience/data/processed/ml_ready_flights/ml_ready_flights.csv\n"
     ]
    }
   ],
   "source": [
    "# Execute the ML preprocessing pipeline\n",
    "encoders, categorical_cols, feature_list = prepare_ml_dataset(BASE_PROCESSED_PATH, ML_PROCESSED_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e84afe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML processed data shape: (1000, 21)\n",
      "\n",
      "Columns in ML processed data:\n",
      "- ARR_DELAY: float64\n",
      "- DEP_DELAY_IS_OUTLIER: int64\n",
      "- DELAY_DUE_CARRIER: float64\n",
      "- DELAY_DUE_LATE_AIRCRAFT: float64\n",
      "- DELAY_DUE_WEATHER: float64\n",
      "- TAXI_OUT: float64\n",
      "- DELAY_DUE_NAS: float64\n",
      "- ARR_TIME: float64\n",
      "- WHEELS_ON: float64\n",
      "- ELAPSED_TIME: float64\n",
      "- TAXI_IN: float64\n",
      "- DOT_CODE: int64\n",
      "- DEP_HOUR_COS: float64\n",
      "- AIR_TIME: float64\n",
      "- DEP_TIME: int64\n",
      "- DELAY_DUE_SECURITY: float64\n",
      "- FL_NUMBER: int64\n",
      "- TIME_OF_DAY_Evening: float64\n",
      "- DEP_DIFF: int64\n",
      "- TIME_OF_DAY_Night: float64\n",
      "- DEP_DELAY: float64\n",
      "\n",
      "Sample of ML processed data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>DEP_DELAY_IS_OUTLIER</th>\n",
       "      <th>DELAY_DUE_CARRIER</th>\n",
       "      <th>DELAY_DUE_LATE_AIRCRAFT</th>\n",
       "      <th>DELAY_DUE_WEATHER</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>DELAY_DUE_NAS</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <th>ELAPSED_TIME</th>\n",
       "      <th>TAXI_IN</th>\n",
       "      <th>DOT_CODE</th>\n",
       "      <th>DEP_HOUR_COS</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DELAY_DUE_SECURITY</th>\n",
       "      <th>FL_NUMBER</th>\n",
       "      <th>TIME_OF_DAY_Evening</th>\n",
       "      <th>DEP_DIFF</th>\n",
       "      <th>TIME_OF_DAY_Night</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19393</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19393</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>202.0</td>\n",
       "      <td>867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>1727.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19930</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>205.0</td>\n",
       "      <td>631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2131.0</td>\n",
       "      <td>2126.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19805</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>281.0</td>\n",
       "      <td>1143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1612.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20397</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>43.0</td>\n",
       "      <td>915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ARR_DELAY  DEP_DELAY_IS_OUTLIER  DELAY_DUE_CARRIER  \\\n",
       "0       35.0                     0               10.0   \n",
       "1       35.0                     0               35.0   \n",
       "2       16.0                     0               16.0   \n",
       "3       67.0                     0                0.0   \n",
       "4       21.0                     0                1.0   \n",
       "\n",
       "   DELAY_DUE_LATE_AIRCRAFT  DELAY_DUE_WEATHER  TAXI_OUT  DELAY_DUE_NAS  \\\n",
       "0                     22.0                0.0      30.0            3.0   \n",
       "1                      0.0                0.0      14.0            0.0   \n",
       "2                      0.0                0.0      31.0            0.0   \n",
       "3                     67.0                0.0      14.0            0.0   \n",
       "4                     20.0                0.0      14.0            0.0   \n",
       "\n",
       "   ARR_TIME  WHEELS_ON  ELAPSED_TIME  TAXI_IN  DOT_CODE  DEP_HOUR_COS  \\\n",
       "0    1925.0     1923.0         168.0      2.0     19393     -0.258819   \n",
       "1    2005.0     2003.0         218.0      2.0     19393     -0.965926   \n",
       "2    1731.0     1727.0         240.0      4.0     19930     -0.866025   \n",
       "3    2131.0     2126.0         328.0      5.0     19805     -0.258819   \n",
       "4    1620.0     1612.0          65.0      8.0     20397     -0.866025   \n",
       "\n",
       "   AIR_TIME  DEP_TIME  DELAY_DUE_SECURITY  FL_NUMBER  TIME_OF_DAY_Evening  \\\n",
       "0     136.0      1057                 0.0        496                  0.0   \n",
       "1     202.0       867                 0.0       1099                  0.0   \n",
       "2     205.0       631                 0.0        500                  0.0   \n",
       "3     281.0      1143                 0.0        285                  0.0   \n",
       "4      43.0       915                 0.0       5314                  0.0   \n",
       "\n",
       "   DEP_DIFF  TIME_OF_DAY_Night  DEP_DELAY  \n",
       "0        32                0.0       32.0  \n",
       "1        42                0.0       42.0  \n",
       "2        31                0.0       31.0  \n",
       "3        88                0.0       88.0  \n",
       "4        38                0.0       38.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify the output file\n",
    "try:\n",
    "    # Read a sample of the processed data\n",
    "    ml_sample = pd.read_csv(ML_PROCESSED_PATH, nrows=1000)\n",
    "    print(f\"ML processed data shape: {ml_sample.shape}\")\n",
    "    print(\"\\nColumns in ML processed data:\")\n",
    "    for col in ml_sample.columns:\n",
    "        print(f\"- {col}: {ml_sample[col].dtype}\")\n",
    "    \n",
    "    print(\"\\nSample of ML processed data:\")\n",
    "    display(ml_sample.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading processed file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feb5dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in time-based splitting demonstration: FL_DATE not found in dataframe\n"
     ]
    }
   ],
   "source": [
    "# Load a sample of the full ML-ready dataset for time-based splitting demonstration\n",
    "try:\n",
    "    # Read a larger sample for demonstration\n",
    "    ml_data_sample = pd.read_csv(ML_PROCESSED_PATH, nrows=100000)\n",
    "    \n",
    "    # Convert date column back to datetime if needed\n",
    "    if 'FL_DATE' in ml_data_sample.columns:\n",
    "        ml_data_sample['FL_DATE'] = pd.to_datetime(ml_data_sample['FL_DATE'])\n",
    "    \n",
    "    # Perform time-based train-test-validation split\n",
    "    train_data, val_data, test_data = time_based_train_test_split(ml_data_sample)\n",
    "    \n",
    "    # Visualize the distribution of the target variable in each split\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    train_data['DEP_DELAY'].hist(bins=50, alpha=0.7)\n",
    "    plt.title('Train Set DEP_DELAY Distribution')\n",
    "    plt.xlabel('Departure Delay (minutes)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    val_data['DEP_DELAY'].hist(bins=50, alpha=0.7)\n",
    "    plt.title('Validation Set DEP_DELAY Distribution')\n",
    "    plt.xlabel('Departure Delay (minutes)')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    test_data['DEP_DELAY'].hist(bins=50, alpha=0.7)\n",
    "    plt.title('Test Set DEP_DELAY Distribution')\n",
    "    plt.xlabel('Departure Delay (minutes)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in time-based splitting demonstration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a00a1c",
   "metadata": {},
   "source": [
    "## Summary of Machine Learning Preprocessing\n",
    "\n",
    "The ML preprocessing pipeline has:\n",
    "\n",
    "1. Added ML-specific engineered features\n",
    "2. Handled outliers using winsorization\n",
    "3. Encoded categorical variables using appropriate techniques\n",
    "4. Selected the most relevant features\n",
    "5. Prepared the data for time-based validation\n",
    "6. Created a dataset ready for ML model training\n",
    "\n",
    "This ML-ready dataset is optimized for traditional machine learning algorithms like XGBoost, Random Forest, etc. It includes properly encoded categorical variables, handles outliers and missing values, and provides a robust selection of features with strong predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e45529",
   "metadata": {},
   "source": [
    "# Preprocessing Optimizations for Gradient Boosting Models\n",
    "\n",
    "The preprocessing pipeline we've built is already well-suited for traditional ML models, but we can optimize it specifically for gradient boosting models like XGBoost, LightGBM, and GBDT. These models have different preprocessing requirements compared to other algorithms.\n",
    "\n",
    "## Key Optimizations for Gradient Boosting Models:\n",
    "\n",
    "1. **Feature scaling is unnecessary** - Tree-based models are invariant to monotonic transformations\n",
    "2. **Categorical encoding can be simplified** - Some models can handle categorical features directly\n",
    "3. **Special handling for missing values** - GB models can learn patterns from missing data\n",
    "4. **Feature interaction importance** - These models benefit from explicit interaction features\n",
    "5. **Optimal sampling strategy** - Balanced sampling for imbalanced delay distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ac11f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized handling of missing values for gradient boosting models\n",
    "def handle_missing_for_gb_models(df):\n",
    "    \"\"\"\n",
    "    Optimized missing value handling for gradient boosting models.\n",
    "    Instead of dropping rows, preserve missing values where possible\n",
    "    since XGBoost/LightGBM can handle them natively.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with optimized missing value handling\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Calculate percentage of missing values per column\n",
    "    missingness = df_result.isnull().mean()\n",
    "    \n",
    "    # For columns with extreme missingness (>80%), drop them\n",
    "    extreme_missing_cols = missingness[missingness > 0.8].index.tolist()\n",
    "    \n",
    "    if extreme_missing_cols:\n",
    "        print(f\"Dropping columns with >80% missing values: {extreme_missing_cols}\")\n",
    "        df_result = df_result.drop(columns=extreme_missing_cols)\n",
    "    \n",
    "    # For columns with high missingness (>50% but 80%), keep as is\n",
    "    # Gradient boosting models can handle missing values directly\n",
    "    high_missing_cols = missingness[(missingness > 0.5) & (missingness <= 0.8)].index.tolist()\n",
    "    \n",
    "    if high_missing_cols:\n",
    "        print(f\"Keeping columns with missing values for GB models to handle: {high_missing_cols}\")\n",
    "    \n",
    "    # For numeric columns with moderate missingness (10-50%), \n",
    "    # add a binary indicator of missingness before imputation\n",
    "    moderate_missing_cols = [col for col in df_result.select_dtypes(include=[np.number]).columns \n",
    "                           if 0.1 < missingness.get(col, 0) <= 0.5]\n",
    "    \n",
    "    for col in moderate_missing_cols:\n",
    "        # Create a missing indicator feature\n",
    "        df_result[f\"{col}_IS_MISSING\"] = df_result[col].isnull().astype(int)\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1263940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized categorical encoding for gradient boosting models\n",
    "def encode_categoricals_for_gb_models(df, cat_cols=None, target_col='DEP_DELAY'):\n",
    "    \"\"\"\n",
    "    Optimized categorical encoding for gradient boosting models.\n",
    "    XGBoost and LightGBM have different optimal encoding strategies:\n",
    "    \n",
    "    1. For LightGBM: Can use categorical features directly with categorical_feature parameter\n",
    "    2. For XGBoost: Requires encoding, but target encoding often works better than one-hot\n",
    "    \n",
    "    This function creates both options so they're available during model training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input dataframe\n",
    "    cat_cols : list, default=None\n",
    "        List of categorical columns. If None, auto-detected.\n",
    "    target_col : str, default='DEP_DELAY'\n",
    "        The target column for target encoding\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with optimized categorical encodings\n",
    "    dict\n",
    "        Dictionary of categorical columns for LightGBM\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Auto-detect categorical columns if not provided\n",
    "    if cat_cols is None:\n",
    "        cat_cols = df_result.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Create a dictionary for LightGBM categorical features\n",
    "    lightgbm_cat_cols = {}\n",
    "    \n",
    "    # Dictionary to store category mappings (for XGBoost)\n",
    "    category_maps = {}\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        if col not in df_result.columns:\n",
    "            continue\n",
    "            \n",
    "        # 1. Convert to category dtype (for LightGBM)\n",
    "        df_result[col] = df_result[col].astype('category')\n",
    "        lightgbm_cat_cols[col] = df_result[col].cat.categories.tolist()\n",
    "        \n",
    "        # Track original order for categories (useful for model interpretation)\n",
    "        category_maps[col] = {val: idx for idx, val in enumerate(df_result[col].cat.categories)}\n",
    "        \n",
    "        # 2. Add label encoding (for XGBoost) - uses the underlying category codes\n",
    "        df_result[f\"{col}_encoded\"] = df_result[col].cat.codes\n",
    "        \n",
    "        # 3. For high-cardinality columns (>15 categories), add target encoding\n",
    "        if df_result[col].nunique() > 15 and target_col in df_result.columns:\n",
    "            # Calculate target mean per category with smoothing\n",
    "            global_mean = df_result[target_col].mean()\n",
    "            category_stats = df_result.groupby(col)[target_col].agg(['mean', 'count'])\n",
    "            \n",
    "            # Apply smoothing to reduce impact of rare categories\n",
    "            smoothing_factor = 100  # Adjust based on data size\n",
    "            category_stats['smooth_mean'] = (\n",
    "                (category_stats['count'] * category_stats['mean'] + \n",
    "                 smoothing_factor * global_mean) / \n",
    "                (category_stats['count'] + smoothing_factor)\n",
    "            )\n",
    "            \n",
    "            # Map the smoothed means back to the dataframe\n",
    "            df_result[f\"{col}_target_mean\"] = df_result[col].map(category_stats['smooth_mean'])\n",
    "    \n",
    "    return df_result, lightgbm_cat_cols, category_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42b4ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature engineering for gradient boosting models\n",
    "def create_advanced_features_for_gb(df):\n",
    "    \"\"\"\n",
    "    Create advanced features specifically beneficial for gradient boosting models.\n",
    "    GB models can automatically find some interactions, but explicit engineering\n",
    "    of certain features can still improve performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with advanced features\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # === 1. TEMPORAL LAG & AGGREGATION FEATURES ===\n",
    "    # These help the model capture temporal patterns that simple features might miss\n",
    "    \n",
    "    # Check if we have the necessary time features\n",
    "    if all(col in df_result.columns for col in ['FL_DATE', 'ORIGIN', 'DEST', 'DEP_DELAY']):\n",
    "        # Convert date to datetime if needed\n",
    "        if df_result['FL_DATE'].dtype != 'datetime64[ns]':\n",
    "            df_result['FL_DATE'] = pd.to_datetime(df_result['FL_DATE'], errors='coerce')\n",
    "        \n",
    "        # Sort by date for proper lag calculations\n",
    "        df_result = df_result.sort_values('FL_DATE')\n",
    "        \n",
    "        # Create origin airport delay aggregations (rolling windows)\n",
    "        # Group by origin airport and date, then calculate mean delays\n",
    "        if len(df_result) > 10000:  # Only if we have enough data\n",
    "            try:\n",
    "                origin_daily_delays = df_result.groupby(['ORIGIN', pd.Grouper(key='FL_DATE', freq='D')])['DEP_DELAY'].mean()\n",
    "                origin_daily_delays = origin_daily_delays.reset_index()\n",
    "                \n",
    "                # Create a lookup dictionary for faster processing\n",
    "                lookup_dict = {}\n",
    "                for _, row in origin_daily_delays.iterrows():\n",
    "                    origin = row['ORIGIN']\n",
    "                    date = row['FL_DATE']\n",
    "                    delay = row['DEP_DELAY']\n",
    "                    if origin not in lookup_dict:\n",
    "                        lookup_dict[origin] = {}\n",
    "                    lookup_dict[origin][date.date()] = delay\n",
    "                \n",
    "                # Function to look up previous day's average delay\n",
    "                def get_prev_day_delay(row):\n",
    "                    origin = row['ORIGIN']\n",
    "                    date = row['FL_DATE'].date()\n",
    "                    prev_date = (row['FL_DATE'] - pd.Timedelta(days=1)).date()\n",
    "                    \n",
    "                    if origin in lookup_dict and prev_date in lookup_dict[origin]:\n",
    "                        return lookup_dict[origin][prev_date]\n",
    "                    return np.nan\n",
    "                \n",
    "                # Apply the function to create the feature\n",
    "                df_result['ORIGIN_PREV_DAY_DELAY'] = df_result.apply(get_prev_day_delay, axis=1)\n",
    "                print(\"Added temporal lag features based on airport delay patterns\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not create temporal features: {e}\")\n",
    "    \n",
    "    # === 2. ADVANCED WEATHER/SEASONAL INTERACTION FEATURES ===\n",
    "    # Weather conditions often interact with airports and routes in complex ways\n",
    "    \n",
    "    # Check if we have weather or seasonal features\n",
    "    seasonal_cols = [col for col in df_result.columns if any(term in col.upper() for term in \n",
    "                                                          ['MONTH', 'DAY', 'SEASON', 'WEATHER'])]\n",
    "    \n",
    "    if 'ORIGIN' in df_result.columns and seasonal_cols:\n",
    "        # Example: Create season-airport interaction for major airports\n",
    "        if 'MONTH_SIN' in df_result.columns and 'MONTH_COS' in df_result.columns:\n",
    "            # Identify major airports (top 10 by frequency)\n",
    "            if len(df_result) > 5000:  # Only if we have enough data\n",
    "                top_airports = df_result['ORIGIN'].value_counts().head(10).index.tolist()\n",
    "                \n",
    "                # Create interaction features for major airports and seasons\n",
    "                for airport in top_airports:\n",
    "                    df_result[f'{airport}_MONTH_SIN'] = np.where(\n",
    "                        df_result['ORIGIN'] == airport, \n",
    "                        df_result['MONTH_SIN'], \n",
    "                        0\n",
    "                    )\n",
    "                    df_result[f'{airport}_MONTH_COS'] = np.where(\n",
    "                        df_result['ORIGIN'] == airport, \n",
    "                        df_result['MONTH_COS'], \n",
    "                        0\n",
    "                    )\n",
    "                print(f\"Created airport-seasonal interaction features for {len(top_airports)} major airports\")\n",
    "    \n",
    "    # === 3. RATIO AND RATE FEATURES ===\n",
    "    # Gradient boosting models can benefit from ratio features\n",
    "    \n",
    "    # Create ratios between related features\n",
    "    numeric_cols = df_result.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Example: Ratio of actual to planned flight time\n",
    "    if all(col in numeric_cols for col in ['ACTUAL_ELAPSED_TIME', 'CRS_ELAPSED_TIME']):\n",
    "        df_result['ELAPSED_TIME_RATIO'] = (df_result['ACTUAL_ELAPSED_TIME'] / \n",
    "                                          df_result['CRS_ELAPSED_TIME']).replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Example: Delay per distance unit\n",
    "    if all(col in numeric_cols for col in ['DEP_DELAY', 'DISTANCE']) and 'DISTANCE' in numeric_cols:\n",
    "        df_result['DELAY_PER_DISTANCE'] = (df_result['DEP_DELAY'] / \n",
    "                                          df_result['DISTANCE'].replace(0, np.nan)).replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eab305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete preprocessing pipeline for gradient boosting models\n",
    "def process_for_gradient_boosting(chunk, lightgbm_cat_mapping=None, target_col='DEP_DELAY'):\n",
    "    \"\"\"\n",
    "    Complete preprocessing function optimized for gradient boosting models.\n",
    "    \n",
    "    Key optimizations:\n",
    "    1. Preserves missing values where possible (XGBoost/LightGBM handle them well)\n",
    "    2. Uses appropriate categorical encoding (label + target encoding for XGBoost)\n",
    "    3. Creates advanced features beneficial for tree-based models\n",
    "    4. NO scaling needed - tree-based models are invariant to monotonic transformations\n",
    "    5. Handles outliers with less aggressive approach (tree models are robust to outliers)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    chunk : pandas.DataFrame\n",
    "        Input data chunk to process\n",
    "    lightgbm_cat_mapping : dict, default=None\n",
    "        Existing mappings for categorical features for LightGBM\n",
    "    target_col : str, default='DEP_DELAY'\n",
    "        Target column name\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Processed dataframe optimized for gradient boosting\n",
    "    dict\n",
    "        Categorical feature mappings for LightGBM\n",
    "    dict\n",
    "        Category label mappings\n",
    "    \"\"\"\n",
    "    # 1. Handle missing values optimally for GB models\n",
    "    chunk_gb = handle_missing_for_gb_models(chunk)\n",
    "    \n",
    "    # 2. Create basic ML features (route, cyclical encodings, etc.)\n",
    "    chunk_gb = create_ml_features(chunk_gb)\n",
    "    \n",
    "    # 3. Handle outliers with less aggressive approach\n",
    "    # For tree-based models, we typically allow more outliers since\n",
    "    # they don't influence the model as much as in linear models\n",
    "    outlier_columns = ['DEP_DELAY', 'ARR_DELAY', 'TAXI_OUT', 'TAXI_IN', \n",
    "                      'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE']\n",
    "    \n",
    "    # Filter to keep only columns that exist in the dataframe\n",
    "    outlier_columns = [col for col in outlier_columns if col in chunk_gb.columns]\n",
    "    \n",
    "    for col in outlier_columns:\n",
    "        # Only cap extreme outliers beyond 3*IQR (more permissive than standard 1.5*IQR)\n",
    "        if col in chunk_gb.columns and chunk_gb[col].dtype in [np.number]:\n",
    "            if col != target_col:  # Don't cap the target variable\n",
    "                Q1 = chunk_gb[col].quantile(0.25)\n",
    "                Q3 = chunk_gb[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                lower_bound = Q1 - 3 * IQR\n",
    "                upper_bound = Q3 + 3 * IQR\n",
    "                \n",
    "                # Cap extreme outliers only\n",
    "                chunk_gb[col] = chunk_gb[col].clip(lower_bound, upper_bound)\n",
    "            else:\n",
    "                # For the target column, just add an outlier flag\n",
    "                Q1 = chunk_gb[col].quantile(0.25)\n",
    "                Q3 = chunk_gb[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                lower_bound = Q1 - 3 * IQR\n",
    "                upper_bound = Q3 + 3 * IQR\n",
    "                \n",
    "                chunk_gb[f'{col}_IS_OUTLIER'] = ((chunk_gb[col] < lower_bound) | \n",
    "                                               (chunk_gb[col] > upper_bound)).astype(int)\n",
    "    \n",
    "    # 4. Apply categorical encoding optimized for gradient boosting\n",
    "    chunk_gb, lightgbm_cats, cat_maps = encode_categoricals_for_gb_models(\n",
    "        chunk_gb, cat_cols=None, target_col=target_col)\n",
    "    \n",
    "    # Update the LightGBM categorical mapping if it exists\n",
    "    if lightgbm_cat_mapping is None:\n",
    "        lightgbm_cat_mapping = lightgbm_cats\n",
    "    else:\n",
    "        # Update with any new categories found\n",
    "        for col, cats in lightgbm_cats.items():\n",
    "            if col not in lightgbm_cat_mapping:\n",
    "                lightgbm_cat_mapping[col] = cats\n",
    "    \n",
    "    # 5. Create advanced features specifically for gradient boosting\n",
    "    chunk_gb = create_advanced_features_for_gb(chunk_gb)\n",
    "    \n",
    "    # NO SCALING needed - tree-based models are invariant to monotonic transformations\n",
    "    \n",
    "    return chunk_gb, lightgbm_cat_mapping, cat_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "152784b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset optimized for gradient boosting models\n",
    "def prepare_gradient_boosting_dataset(input_file, output_file, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    Process all chunks and prepare a dataset optimized for gradient boosting models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to input data file\n",
    "    output_file : str\n",
    "        Path to output processed file\n",
    "    chunk_size : int, default=500000\n",
    "        Size of chunks to process at once\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (lightgbm_category_mapping, category_maps, important_features)\n",
    "    \"\"\"\n",
    "    lightgbm_cat_mapping = None\n",
    "    category_maps = {}\n",
    "    important_features = None\n",
    "    first_chunk = True\n",
    "    feature_importances = None\n",
    "    \n",
    "    print(f\"Starting Gradient Boosting preprocessing of {input_file}...\")\n",
    "    \n",
    "    # Process in chunks\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(load_processed_data(input_file, chunk_size=chunk_size)):\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Process the chunk with GB-optimized pipeline\n",
    "        processed_chunk, lightgbm_cat_mapping, cat_maps = process_for_gradient_boosting(\n",
    "            chunk, lightgbm_cat_mapping)\n",
    "        \n",
    "        # Update category maps\n",
    "        for col, mapping in cat_maps.items():\n",
    "            if col not in category_maps:\n",
    "                category_maps[col] = mapping\n",
    "        \n",
    "        # For the first chunk, try to get feature importances using a quick XGBoost model\n",
    "        if i == 0 and len(processed_chunk) > 5000:\n",
    "            try:\n",
    "                # Only need to do feature importance on a sample\n",
    "                sample_size = min(10000, len(processed_chunk))\n",
    "                sample = processed_chunk.sample(sample_size, random_state=42)\n",
    "                \n",
    "                # Check if XGBoost is available\n",
    "                try:\n",
    "                    import xgboost as xgb\n",
    "                    \n",
    "                    # Prepare data for a quick XGBoost run\n",
    "                    X = sample.select_dtypes(include=[np.number])\n",
    "                    X = X.drop(target_col, axis=1, errors='ignore')\n",
    "                    y = sample[target_col]\n",
    "                    \n",
    "                    # Train a quick model to get feature importances\n",
    "                    dtrain = xgb.DMatrix(X, label=y)\n",
    "                    params = {\n",
    "                        'max_depth': 3,\n",
    "                        'eta': 0.1,\n",
    "                        'objective': 'reg:squarederror',\n",
    "                        'nthread': 4,\n",
    "                        'verbosity': 0\n",
    "                    }\n",
    "                    gb_model = xgb.train(params, dtrain, num_boost_round=20)\n",
    "                    \n",
    "                    # Get feature importances\n",
    "                    feature_importances = pd.Series(gb_model.get_score(importance_type='gain'))\n",
    "                    feature_importances = feature_importances.sort_values(ascending=False)\n",
    "                    \n",
    "                    # Select important features\n",
    "                    important_features = feature_importances.index.tolist()\n",
    "                    print(f\"Identified {len(important_features)} important features from XGBoost\")\n",
    "                    \n",
    "                    # Add the target column\n",
    "                    if target_col not in important_features:\n",
    "                        important_features.append(target_col)\n",
    "                        \n",
    "                except ImportError:\n",
    "                    print(\"XGBoost not available. Skipping feature importance calculation.\")\n",
    "                    # If XGBoost is not available, keep all features\n",
    "                    important_features = processed_chunk.columns.tolist()\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing feature importances: {e}\")\n",
    "                important_features = processed_chunk.columns.tolist()\n",
    "        \n",
    "        chunks.append(processed_chunk)\n",
    "        \n",
    "        # Print progress\n",
    "        end_time = datetime.now()\n",
    "        elapsed = (end_time - start_time).total_seconds()\n",
    "        print(f\"Processed chunk {i+1}: {len(processed_chunk):,} rows in {elapsed:.2f} seconds\")\n",
    "        \n",
    "        # To save memory, periodically combine and save chunks\n",
    "        if len(chunks) >= 5 or (i == 0 and first_chunk):\n",
    "            combined = pd.concat(chunks)\n",
    "            \n",
    "            # Keep only important features if they've been identified\n",
    "            if important_features is not None:\n",
    "                # Filter to available features\n",
    "                available_features = [col for col in important_features if col in combined.columns]\n",
    "                combined = combined[available_features]\n",
    "            \n",
    "            # Save the data\n",
    "            if first_chunk:\n",
    "                # First chunk, use write mode\n",
    "                combined.to_csv(output_file, index=False)\n",
    "                first_chunk = False\n",
    "            else:\n",
    "                # Subsequent chunks, append\n",
    "                combined.to_csv(output_file, mode='a', header=False, index=False)\n",
    "            \n",
    "            # Clear chunks to free memory\n",
    "            chunks = []\n",
    "    \n",
    "    # Save any remaining chunks\n",
    "    if chunks:\n",
    "        combined = pd.concat(chunks)\n",
    "        \n",
    "        # Keep only important features if they've been identified\n",
    "        if important_features is not None:\n",
    "            available_features = [col for col in important_features if col in combined.columns]\n",
    "            combined = combined[available_features]\n",
    "        \n",
    "        # Append to output\n",
    "        combined.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    print(f\"Gradient Boosting preprocessing complete! Output saved to {output_file}\")\n",
    "    \n",
    "    # Also save the LightGBM categorical feature mapping\n",
    "    lightgbm_mapping_file = os.path.splitext(output_file)[0] + '_lightgbm_cats.json'\n",
    "    \n",
    "    import json\n",
    "    # Convert to a serializable format\n",
    "    serializable_mapping = {}\n",
    "    for col, cats in lightgbm_cat_mapping.items():\n",
    "        serializable_mapping[col] = [str(c) for c in cats]\n",
    "    \n",
    "    with open(lightgbm_mapping_file, 'w') as f:\n",
    "        json.dump(serializable_mapping, f)\n",
    "    \n",
    "    print(f\"LightGBM categorical mapping saved to {lightgbm_mapping_file}\")\n",
    "    \n",
    "    # Return for subsequent operations\n",
    "    return lightgbm_cat_mapping, category_maps, important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8889c",
   "metadata": {},
   "source": [
    "## Executing the Gradient Boosting Preprocessing Pipeline\n",
    "\n",
    "Let's execute our optimized preprocessing pipeline for gradient boosting models and compare it with the standard pipeline. The key differences in our approach are:\n",
    "\n",
    "1. **Missing Values**: Preserve them where possible instead of dropping rows\n",
    "2. **Categorical Encoding**: Create multiple encoding types (label + target) with LightGBM compatibility\n",
    "3. **Feature Engineering**: Advanced temporal features and airport-specific interactions\n",
    "4. **Feature Selection**: Using XGBoost's feature importance for early dimension reduction\n",
    "5. **Outlier Handling**: More permissive approach since tree models are more robust\n",
    "6. **No Scaling**: Removed unnecessary scaling that doesn't benefit tree-based models\n",
    "\n",
    "This pipeline is specifically optimized for XGBoost, LightGBM, and other gradient boosting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74aa1979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gradient Boosting preprocessing of /Users/osx/DataSceince_FL_FR/Forecasting_Flights-DataScience/data/processed/base_preprocessed_flights.csv...\n",
      "Dropping columns with >80% missing values: ['CANCELLATION_CODE', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT']\n",
      "Added temporal lag features based on airport delay patterns\n",
      "Created airport-seasonal interaction features for 10 major airports\n",
      "Error computing feature importances: name 'target_col' is not defined\n",
      "Processed chunk 1: 500,000 rows in 23.36 seconds\n",
      "Dropping columns with >80% missing values: ['CANCELLATION_CODE', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT']\n",
      "Added temporal lag features based on airport delay patterns\n",
      "Created airport-seasonal interaction features for 10 major airports\n",
      "Processed chunk 2: 500,000 rows in 23.69 seconds\n",
      "Dropping columns with >80% missing values: ['CANCELLATION_CODE', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT']\n",
      "Added temporal lag features based on airport delay patterns\n",
      "Created airport-seasonal interaction features for 10 major airports\n",
      "Processed chunk 3: 500,000 rows in 24.20 seconds\n",
      "Dropping columns with >80% missing values: ['CANCELLATION_CODE', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT']\n",
      "Added temporal lag features based on airport delay patterns\n",
      "Created airport-seasonal interaction features for 10 major airports\n",
      "Processed chunk 4: 500,000 rows in 23.36 seconds\n",
      "Dropping columns with >80% missing values: ['CANCELLATION_CODE', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT']\n",
      "Added temporal lag features based on airport delay patterns\n",
      "Created airport-seasonal interaction features for 10 major airports\n",
      "Processed chunk 5: 470,267 rows in 22.51 seconds\n",
      "Gradient Boosting preprocessing complete! Output saved to /Users/osx/DataSceince_FL_FR/Forecasting_Flights-DataScience/data/processed/ml_ready_flights/gb_ready_flights.csv\n",
      "LightGBM categorical mapping saved to /Users/osx/DataSceince_FL_FR/Forecasting_Flights-DataScience/data/processed/ml_ready_flights/gb_ready_flights_lightgbm_cats.json\n",
      "Standard ML processed data path: /Users/osx/DataSceince_FL_FR/Forecasting_Flights-DataScience/data/processed/ml_ready_flights/ml_ready_flights.csv\n",
      "Gradient Boosting optimized data path: /Users/osx/DataSceince_FL_FR/Forecasting_Flights-DataScience/data/processed/ml_ready_flights/gb_ready_flights.csv\n",
      "LightGBM categorical mapping file: /Users/osx/DataSceince_FL_FR/Forecasting_Flights-DataScience/data/processed/ml_ready_flights/gb_ready_flights_lightgbm_cats.json\n",
      "Gradient Boosting processed data shape: (2470267, 91)\n"
     ]
    }
   ],
   "source": [
    "# Define the output path for the gradient boosting optimized dataset\n",
    "GB_PROCESSED_PATH = path.join(project_root, 'data', 'processed', 'ml_ready_flights', 'gb_ready_flights.csv')\n",
    "\n",
    "# Make sure the directory exists\n",
    "os.makedirs(os.path.dirname(GB_PROCESSED_PATH), exist_ok=True)\n",
    "\n",
    "# Execute the gradient boosting preprocessing pipeline\n",
    "lightgbm_cats, category_maps, important_features = prepare_gradient_boosting_dataset(\n",
    "    BASE_PROCESSED_PATH, GB_PROCESSED_PATH)\n",
    "\n",
    "# Print paths for reference\n",
    "print(f\"Standard ML processed data path: {ML_PROCESSED_PATH}\")\n",
    "print(f\"Gradient Boosting optimized data path: {GB_PROCESSED_PATH}\")\n",
    "print(f\"LightGBM categorical mapping file: {os.path.splitext(GB_PROCESSED_PATH)[0] + '_lightgbm_cats.json'}\")\n",
    "\n",
    "dataframesize = pd.read_csv(GB_PROCESSED_PATH)\n",
    "print(f\"Gradient Boosting processed data shape: {dataframesize.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a500c",
   "metadata": {},
   "source": [
    "## Comparison of Standard vs. Gradient Boosting Optimized Preprocessing\n",
    "\n",
    "Let's summarize the key differences between our two preprocessing approaches:\n",
    "\n",
    "| Preprocessing Step | Standard Pipeline | Gradient Boosting Optimized Pipeline |\n",
    "|-------------------|-------------------|--------------------------------------|\n",
    "| **Missing Values** | Drops 90% of rows with missing values in columns with >50% missingness | Preserves missing values where possible, adds missing indicators |\n",
    "| **Categorical Encoding** | One-hot for low cardinality, ordinal for high cardinality | Multiple encodings (label + target), LightGBM compatibility, smoothed target encoding |\n",
    "| **Feature Engineering** | Basic features (route, cyclical, etc.) | Advanced temporal lag features, airport-seasonal interactions, rate/ratio features |\n",
    "| **Outlier Handling** | IQR method with 1.5 factor | More permissive 3*IQR approach (trees handle outliers better) |\n",
    "| **Feature Selection** | Correlation-based | XGBoost feature importance-based |\n",
    "| **Scaling** | Included but not always applied | Removed (unnecessary for tree models) |\n",
    "| **Data Chunking** | Basic chunking | Same approach but with optimized processing |\n",
    "\n",
    "When to use each pipeline:\n",
    "\n",
    "- **Standard Pipeline**: For linear models, neural networks, or mixed model ensembles\n",
    "- **Gradient Boosting Pipeline**: For XGBoost, LightGBM, CatBoost, and other tree-based gradient boosting models\n",
    "\n",
    "The optimized pipeline leverages the specific strengths of gradient boosting models, particularly their ability to:\n",
    "- Handle missing values natively\n",
    "- Work well with categorical features\n",
    "- Capture complex non-linear patterns\n",
    "- Be robust to outliers\n",
    "- Automatically determine feature importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
