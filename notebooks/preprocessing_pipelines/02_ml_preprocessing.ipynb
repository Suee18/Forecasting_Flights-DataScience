{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa650db",
   "metadata": {},
   "source": [
    "# Machine Learning Preprocessing Pipeline for Flight Delay Data\n",
    "\n",
    "This notebook builds upon the base preprocessing pipeline to create features specifically optimized for traditional machine learning models like XGBoost, Random Forest, etc. These models typically require structured tabular data with well-engineered features, proper encoding, and handling of outliers.\n",
    "\n",
    "## Key Processing Steps:\n",
    "1. Loading the base preprocessed data\n",
    "2. Feature engineering specific to ML models\n",
    "3. Categorical encoding (one-hot, ordinal, target)\n",
    "4. Handling outliers\n",
    "5. Feature selection/importance analysis\n",
    "6. Feature scaling/normalization\n",
    "7. Train-test splitting with time-based validation\n",
    "8. Exporting the processed data for ML model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a6c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base processed data path: /Users/osx/flightDelayPIPELINE.2/data/processed/base_preprocessed_flights.csv\n",
      "ML processed data path: /Users/osx/flightDelayPIPELINE.2/data/processed/ml_ready_flights.csv\n",
      "ML model path: /Users/osx/flightDelayPIPELINE.2/models\n",
      "Libraries and paths configured.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure paths dynamically using relative paths\n",
    "import os.path as path\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "notebook_dir = path.dirname(path.abspath('__file__'))\n",
    "# Get project root (parent of notebooks directory)\n",
    "project_root = path.abspath(path.join(notebook_dir, '..', '..'))\n",
    "\n",
    "# Define paths relative to project root\n",
    "BASE_PROCESSED_PATH = path.join(project_root, 'data', 'processed', 'base_preprocessed_flights.csv')\n",
    "ML_PROCESSED_PATH = path.join(project_root, 'data', 'processed', 'ml_ready_flights.csv')\n",
    "ML_MODEL_PATH = path.join(project_root, 'models')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(os.path.dirname(ML_PROCESSED_PATH), exist_ok=True)\n",
    "os.makedirs(ML_MODEL_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Base processed data path: {BASE_PROCESSED_PATH}\")\n",
    "print(f\"ML processed data path: {ML_PROCESSED_PATH}\")\n",
    "print(f\"ML model path: {ML_MODEL_PATH}\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Libraries and paths configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b97862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data in chunks\n",
    "def load_processed_data(file_path, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    Generator function to load preprocessed data in chunks\n",
    "    \"\"\"\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        # Convert date columns to datetime\n",
    "        date_columns = [col for col in chunk.columns if 'DATE' in col.upper()]\n",
    "        for col in date_columns:\n",
    "            chunk[col] = pd.to_datetime(chunk[col], errors='coerce')\n",
    "        \n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6591e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape of first chunk: (500000, 28)\n",
      "\n",
      "Columns and data types:\n",
      "- FL_DATE: datetime64[ns]\n",
      "- ORIGIN: object\n",
      "- DEST: object\n",
      "- CRS_DEP_TIME: int64\n",
      "- DEP_TIME: float64\n",
      "- DEP_DELAY: float64\n",
      "- TAXI_OUT: float64\n",
      "- WHEELS_OFF: float64\n",
      "- WHEELS_ON: float64\n",
      "- TAXI_IN: float64\n",
      "- CRS_ARR_TIME: int64\n",
      "- ARR_TIME: float64\n",
      "- ARR_DELAY: float64\n",
      "- CANCELLED: int64\n",
      "- CANCELLATION_CODE: object\n",
      "- DIVERTED: int64\n",
      "- CRS_ELAPSED_TIME: float64\n",
      "- AIR_TIME: float64\n",
      "- DISTANCE: float64\n",
      "- YEAR: int64\n",
      "- MONTH: int64\n",
      "- DAY_OF_MONTH: int64\n",
      "- DAY_OF_WEEK: int64\n",
      "- QUARTER: int64\n",
      "- SEASON: int64\n",
      "- IS_HOLIDAY_SEASON: int64\n",
      "- DEP_HOUR: int64\n",
      "- TIME_OF_DAY: object\n",
      "\n",
      "Sample data (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <th>TAXI_IN</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>CANCELLATION_CODE</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>QUARTER</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>IS_HOLIDAY_SEASON</th>\n",
       "      <th>DEP_HOUR</th>\n",
       "      <th>TIME_OF_DAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>FLL</td>\n",
       "      <td>EWR</td>\n",
       "      <td>715</td>\n",
       "      <td>1151.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>1443.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>901</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>MSP</td>\n",
       "      <td>SEA</td>\n",
       "      <td>1280</td>\n",
       "      <td>2114.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2123.0</td>\n",
       "      <td>2232.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1395</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>Evening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>DEN</td>\n",
       "      <td>MSP</td>\n",
       "      <td>594</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>1247.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>772</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>MSP</td>\n",
       "      <td>SFO</td>\n",
       "      <td>969</td>\n",
       "      <td>1608.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1109</td>\n",
       "      <td>1853.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>DAL</td>\n",
       "      <td>OKC</td>\n",
       "      <td>610</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>670</td>\n",
       "      <td>1331.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FL_DATE ORIGIN DEST  CRS_DEP_TIME  DEP_TIME  DEP_DELAY  TAXI_OUT  \\\n",
       "0 2019-01-09    FLL  EWR           715    1151.0       -4.0      19.0   \n",
       "1 2022-11-19    MSP  SEA          1280    2114.0       -6.0       9.0   \n",
       "2 2022-07-22    DEN  MSP           594    1000.0        6.0      20.0   \n",
       "3 2023-03-06    MSP  SFO           969    1608.0       -1.0      27.0   \n",
       "4 2019-07-31    DAL  OKC           610    1237.0      147.0      15.0   \n",
       "\n",
       "   WHEELS_OFF  WHEELS_ON  TAXI_IN  CRS_ARR_TIME  ARR_TIME  ARR_DELAY  \\\n",
       "0      1210.0     1443.0      4.0           901    1447.0      -14.0   \n",
       "1      2123.0     2232.0     38.0          1395    2310.0       -5.0   \n",
       "2      1020.0     1247.0      5.0           772    1252.0        0.0   \n",
       "3      1635.0     1844.0      9.0          1109    1853.0       24.0   \n",
       "4      1252.0     1328.0      3.0           670    1331.0      141.0   \n",
       "\n",
       "   CANCELLED CANCELLATION_CODE  DIVERTED  CRS_ELAPSED_TIME  AIR_TIME  \\\n",
       "0          0               NaN         0             186.0     153.0   \n",
       "1          0               NaN         0             235.0     189.0   \n",
       "2          0               NaN         0             118.0      87.0   \n",
       "3          0               NaN         0             260.0     249.0   \n",
       "4          0               NaN         0              60.0      36.0   \n",
       "\n",
       "   DISTANCE  YEAR  MONTH  DAY_OF_MONTH  DAY_OF_WEEK  QUARTER  SEASON  \\\n",
       "0    1065.0  2019      1             9            3        1       1   \n",
       "1    1399.0  2022     11            19            6        4       4   \n",
       "2     680.0  2022      7            22            5        3       3   \n",
       "3    1589.0  2023      3             6            1        1       2   \n",
       "4     181.0  2019      7            31            3        3       3   \n",
       "\n",
       "   IS_HOLIDAY_SEASON  DEP_HOUR TIME_OF_DAY  \n",
       "0                  0        11     Morning  \n",
       "1                  1        21     Evening  \n",
       "2                  0         9     Morning  \n",
       "3                  0        16   Afternoon  \n",
       "4                  0        10     Morning  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the data\n",
    "first_chunk = next(load_processed_data(BASE_PROCESSED_PATH))\n",
    "\n",
    "print(f\"Data shape of first chunk: {first_chunk.shape}\")\n",
    "print(\"\\nColumns and data types:\")\n",
    "for col in first_chunk.columns:\n",
    "    print(f\"- {col}: {first_chunk[col].dtype}\")\n",
    "\n",
    "print(\"\\nSample data (first 5 rows):\")\n",
    "display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa2a77",
   "metadata": {},
   "source": [
    "## Feature Engineering Specific to Machine Learning Models\n",
    "\n",
    "We'll define functions to create additional features that are particularly useful for machine learning models. These include:\n",
    "\n",
    "1. Categorical encoding\n",
    "2. Route-based features\n",
    "3. Carrier-based features\n",
    "4. Temporal patterns\n",
    "5. Airport congestion metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff53b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ML-specific features\n",
    "def create_ml_features(df):\n",
    "    \"\"\"\n",
    "    Create features specifically useful for ML models\n",
    "    \"\"\"\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # ======== ROUTE AND AIRPORT FEATURES ========\n",
    "    \n",
    "    # Create route features if origin and destination are present\n",
    "    if 'ORIGIN' in df_featured.columns and 'DEST' in df_featured.columns:\n",
    "        # Create a combined route feature\n",
    "        df_featured['ROUTE'] = df_featured['ORIGIN'] + '_' + df_featured['DEST']\n",
    "        \n",
    "        # Create distance buckets (if distance is available)\n",
    "        if 'DISTANCE' in df_featured.columns:\n",
    "            df_featured['DISTANCE_GROUP'] = pd.cut(\n",
    "                df_featured['DISTANCE'],\n",
    "                bins=[0, 500, 1000, 1500, 2000, 2500, 5000],\n",
    "                labels=['Very Short', 'Short', 'Medium Short', 'Medium', 'Medium Long', 'Long']\n",
    "            )\n",
    "    \n",
    "    # ======== TEMPORAL FEATURES ========\n",
    "    \n",
    "    # Create cyclical features for time variables\n",
    "    if 'MONTH' in df_featured.columns:\n",
    "        # Cyclical encoding of month (1-12)\n",
    "        df_featured['MONTH_SIN'] = np.sin(2 * np.pi * df_featured['MONTH'] / 12)\n",
    "        df_featured['MONTH_COS'] = np.cos(2 * np.pi * df_featured['MONTH'] / 12)\n",
    "    \n",
    "    if 'DAY_OF_WEEK' in df_featured.columns:\n",
    "        # Cyclical encoding of day of week (1-7)\n",
    "        df_featured['DAY_OF_WEEK_SIN'] = np.sin(2 * np.pi * df_featured['DAY_OF_WEEK'] / 7)\n",
    "        df_featured['DAY_OF_WEEK_COS'] = np.cos(2 * np.pi * df_featured['DAY_OF_WEEK'] / 7)\n",
    "    \n",
    "    if 'DEP_HOUR' in df_featured.columns:\n",
    "        # Cyclical encoding of hour (0-23)\n",
    "        df_featured['DEP_HOUR_SIN'] = np.sin(2 * np.pi * df_featured['DEP_HOUR'] / 24)\n",
    "        df_featured['DEP_HOUR_COS'] = np.cos(2 * np.pi * df_featured['DEP_HOUR'] / 24)\n",
    "    \n",
    "    # ======== FLIGHT-SPECIFIC FEATURES ========\n",
    "    \n",
    "    # Create a departure vs. scheduled departure difference\n",
    "    if 'DEP_TIME' in df_featured.columns and 'CRS_DEP_TIME' in df_featured.columns:\n",
    "        try:\n",
    "            df_featured['DEP_DIFF'] = df_featured['DEP_TIME'] - df_featured['CRS_DEP_TIME']\n",
    "        except:\n",
    "            print(\"Could not calculate DEP_DIFF\")\n",
    "    \n",
    "    # Combine delay types (if available)\n",
    "    delay_cols = ['CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY']\n",
    "    if all(col in df_featured.columns for col in delay_cols):\n",
    "        # Fill NaNs with 0 to avoid issues in summation\n",
    "        for col in delay_cols:\n",
    "            df_featured[col] = df_featured[col].fillna(0)\n",
    "            \n",
    "        # Create delay type features\n",
    "        df_featured['TOTAL_DELAY'] = df_featured[delay_cols].sum(axis=1)\n",
    "        \n",
    "        # Calculate percentage of each delay type\n",
    "        for col in delay_cols:\n",
    "            df_featured[f'{col}_RATIO'] = (df_featured[col] / df_featured['TOTAL_DELAY']).fillna(0)\n",
    "    \n",
    "    # ======== INTERACTION FEATURES ========\n",
    "    \n",
    "    # Create interaction features between important variables\n",
    "    if 'DAY_OF_WEEK' in df_featured.columns and 'DEP_HOUR' in df_featured.columns:\n",
    "        df_featured['DAY_HOUR'] = df_featured['DAY_OF_WEEK'].astype(str) + '_' + df_featured['DEP_HOUR'].astype(str)\n",
    "    \n",
    "    if 'MONTH' in df_featured.columns and 'DAY_OF_WEEK' in df_featured.columns:\n",
    "        df_featured['MONTH_DAY'] = df_featured['MONTH'].astype(str) + '_' + df_featured['DAY_OF_WEEK'].astype(str)\n",
    "    \n",
    "    return df_featured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b982f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers for ML models\n",
    "def handle_outliers_ml(df, target_col='DEP_DELAY'):\n",
    "    \"\"\"\n",
    "    Handle outliers in a way suitable for ML models\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Define numeric columns that might have outliers\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove target column from the list if it's there\n",
    "    if target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "    \n",
    "    # Define columns to check for outliers\n",
    "    outlier_columns = ['DEP_DELAY', 'ARR_DELAY', 'TAXI_OUT', 'TAXI_IN', \n",
    "                      'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE']\n",
    "    \n",
    "    # Filter to keep only columns that exist in the dataframe\n",
    "    outlier_columns = [col for col in outlier_columns if col in df_clean.columns]\n",
    "    \n",
    "    # Handle outliers for each column\n",
    "    for col in outlier_columns:\n",
    "        # Skip non-numeric columns\n",
    "        if df_clean[col].dtype not in [np.number]:\n",
    "            continue\n",
    "            \n",
    "        # Skip columns already processed in base pipeline\n",
    "        if col not in df_clean.columns:\n",
    "            continue\n",
    "        \n",
    "        # Calculate IQR\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define bounds\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Cap outliers (winsorize)\n",
    "        if col != target_col:  # Don't cap the target variable\n",
    "            df_clean[col] = df_clean[col].clip(lower_bound, upper_bound)\n",
    "        else:\n",
    "            # For the target column, we might want to keep outliers but flag them\n",
    "            df_clean[f'{col}_IS_OUTLIER'] = ((df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)).astype(int)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ec5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ML-ready feature encoders\n",
    "def create_categorical_encoders(df, categorical_cols=None, target_col='DEP_DELAY'):\n",
    "    \"\"\"\n",
    "    Create encoders for categorical variables\n",
    "    \"\"\"\n",
    "    if categorical_cols is None:\n",
    "        # Identify categorical columns automatically\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Create a dictionary to store encoders\n",
    "    encoders = {}\n",
    "    \n",
    "    # One-hot encoding for low-cardinality categoricals\n",
    "    encoders['onehot'] = {}\n",
    "    \n",
    "    # Ordinal encoding for high-cardinality categoricals\n",
    "    encoders['ordinal'] = {}\n",
    "    \n",
    "    # Determine cardinality of each categorical\n",
    "    for col in categorical_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        n_unique = df[col].nunique()\n",
    "        \n",
    "        # Create appropriate encoder based on cardinality\n",
    "        if n_unique <= 15:  # Low cardinality\n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "            encoder.fit(df[[col]])\n",
    "            encoders['onehot'][col] = encoder\n",
    "        else:  # High cardinality\n",
    "            encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "            encoder.fit(df[[col]])\n",
    "            encoders['ordinal'][col] = encoder\n",
    "    \n",
    "    return encoders, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94fa7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply categorical encoders\n",
    "def apply_categorical_encodings(df, encoders, categorical_cols):\n",
    "    \"\"\"\n",
    "    Apply the fitted encoders to transform categorical data\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Apply one-hot encoding\n",
    "    for col, encoder in encoders['onehot'].items():\n",
    "        if col in df_encoded.columns:\n",
    "            # Transform the data\n",
    "            encoded_array = encoder.transform(df_encoded[[col]])\n",
    "            \n",
    "            # Create new column names\n",
    "            feature_names = [f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "            \n",
    "            # Create a DataFrame with the encoded values\n",
    "            encoded_df = pd.DataFrame(encoded_array, columns=feature_names, index=df_encoded.index)\n",
    "            \n",
    "            # Concatenate with the original DataFrame\n",
    "            df_encoded = pd.concat([df_encoded, encoded_df], axis=1)\n",
    "            \n",
    "            # Drop the original categorical column\n",
    "            df_encoded = df_encoded.drop(col, axis=1)\n",
    "    \n",
    "    # Apply ordinal encoding\n",
    "    for col, encoder in encoders['ordinal'].items():\n",
    "        if col in df_encoded.columns:\n",
    "            # Transform the data\n",
    "            encoded_array = encoder.transform(df_encoded[[col]])\n",
    "            \n",
    "            # Replace the original column with the encoded values\n",
    "            df_encoded[col] = encoded_array\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a466da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for ML\n",
    "def select_features_ml(df, target_col='DEP_DELAY', k=20):\n",
    "    \"\"\"\n",
    "    Select top k features based on correlation with target\n",
    "    \"\"\"\n",
    "    # Split data into X and y\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Keep only numeric columns\n",
    "    numeric_X = X.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Drop any constant columns\n",
    "    non_constant_cols = [col for col in numeric_X.columns if numeric_X[col].nunique() > 1]\n",
    "    numeric_X = numeric_X[non_constant_cols]\n",
    "    \n",
    "    # Calculate correlation with target\n",
    "    correlations = numeric_X.corrwith(y).abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Select top k features\n",
    "    top_features = correlations.nlargest(min(k, len(correlations))).index.tolist()\n",
    "    \n",
    "    # Add target column back\n",
    "    selected_features = top_features + [target_col]\n",
    "    \n",
    "    return selected_features, correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dbabb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling for ML models\n",
    "def scale_features_ml(df, target_col='DEP_DELAY', scaler_type='standard'):\n",
    "    \"\"\"\n",
    "    Scale features for ML models\n",
    "    \"\"\"\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Get numeric columns excluding the target\n",
    "    numeric_cols = df_scaled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "    \n",
    "    # Choose scaler\n",
    "    if scaler_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(\"scaler_type must be 'standard' or 'minmax'\")\n",
    "    \n",
    "    # Scale numeric columns\n",
    "    if numeric_cols:\n",
    "        df_scaled[numeric_cols] = scaler.fit_transform(df_scaled[numeric_cols])\n",
    "    \n",
    "    return df_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69d5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based train-test split for ML models\n",
    "def time_based_train_test_split(df, date_col='FL_DATE', test_size=0.2, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Split data based on time to prevent data leakage\n",
    "    \"\"\"\n",
    "    if date_col not in df.columns:\n",
    "        raise ValueError(f\"{date_col} not found in dataframe\")\n",
    "    \n",
    "    # Sort by date\n",
    "    df_sorted = df.sort_values(date_col)\n",
    "    \n",
    "    # Determine split points\n",
    "    n_samples = len(df_sorted)\n",
    "    test_start_idx = int(n_samples * (1 - test_size))\n",
    "    val_start_idx = int(n_samples * (1 - test_size - val_size))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = df_sorted.iloc[:val_start_idx]\n",
    "    val_data = df_sorted.iloc[val_start_idx:test_start_idx]\n",
    "    test_data = df_sorted.iloc[test_start_idx:]\n",
    "    \n",
    "    print(f\"Train set: {len(train_data):,} rows from {train_data[date_col].min()} to {train_data[date_col].max()}\")\n",
    "    print(f\"Validation set: {len(val_data):,} rows from {val_data[date_col].min()} to {val_data[date_col].max()}\")\n",
    "    print(f\"Test set: {len(test_data):,} rows from {test_data[date_col].min()} to {test_data[date_col].max()}\")\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60c052",
   "metadata": {},
   "source": [
    "## Execute the ML Preprocessing Pipeline\n",
    "\n",
    "Now we'll run the complete ML preprocessing pipeline on our base preprocessed data to prepare it for machine learning model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c000ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process chunks with the ML pipeline\n",
    "def process_chunk_ml(chunk, encoders=None, categorical_cols=None, feature_list=None):\n",
    "    \"\"\"\n",
    "    Apply ML preprocessing to a chunk of base preprocessed data\n",
    "    \"\"\"\n",
    "    # Add ML-specific features\n",
    "    chunk_ml = create_ml_features(chunk)\n",
    "    \n",
    "    # Handle outliers\n",
    "    chunk_ml = handle_outliers_ml(chunk_ml)\n",
    "    \n",
    "    # Fit or apply categorical encoders\n",
    "    if encoders is None:\n",
    "        # First chunk - create encoders\n",
    "        encoders, categorical_cols = create_categorical_encoders(chunk_ml)\n",
    "        # Apply encodings\n",
    "        chunk_ml = apply_categorical_encodings(chunk_ml, encoders, categorical_cols)\n",
    "    else:\n",
    "        # Subsequent chunks - apply existing encoders\n",
    "        chunk_ml = apply_categorical_encodings(chunk_ml, encoders, categorical_cols)\n",
    "    \n",
    "    # Select features if feature_list provided\n",
    "    if feature_list is not None:\n",
    "        # Keep only the selected features that exist in the dataframe\n",
    "        available_features = [col for col in feature_list if col in chunk_ml.columns]\n",
    "        chunk_ml = chunk_ml[available_features]\n",
    "    \n",
    "    return chunk_ml, encoders, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "313c4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all chunks and prepare ML-ready dataset\n",
    "def prepare_ml_dataset(input_file, output_file, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    Process all chunks and prepare an ML-ready dataset\n",
    "    \"\"\"\n",
    "    encoders = None\n",
    "    categorical_cols = None\n",
    "    feature_list = None\n",
    "    sample_for_feature_selection = None\n",
    "    first_chunk = True\n",
    "    \n",
    "    print(f\"Starting ML preprocessing of {input_file}...\")\n",
    "    \n",
    "    # Process in chunks\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(load_processed_data(input_file, chunk_size=chunk_size)):\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Process the chunk\n",
    "        processed_chunk, encoders, categorical_cols = process_chunk_ml(chunk, encoders, categorical_cols)\n",
    "        \n",
    "        # If this is the first chunk, use it for feature selection\n",
    "        if i == 0:\n",
    "            # Save a sample for feature selection\n",
    "            if len(processed_chunk) > 10000:\n",
    "                sample_for_feature_selection = processed_chunk.sample(10000, random_state=42)\n",
    "            else:\n",
    "                sample_for_feature_selection = processed_chunk\n",
    "        \n",
    "        chunks.append(processed_chunk)\n",
    "        \n",
    "        # Print progress\n",
    "        end_time = datetime.now()\n",
    "        elapsed = (end_time - start_time).total_seconds()\n",
    "        print(f\"Processed chunk {i+1}: {len(processed_chunk):,} rows in {elapsed:.2f} seconds\")\n",
    "        \n",
    "        # To save memory, periodically combine and save chunks\n",
    "        if len(chunks) >= 5 or (i == 0 and first_chunk):\n",
    "            combined = pd.concat(chunks)\n",
    "            \n",
    "            # If this is the first batch, do feature selection\n",
    "            if first_chunk:\n",
    "                print(\"Performing feature selection...\")\n",
    "                selected_features, correlations = select_features_ml(sample_for_feature_selection)\n",
    "                feature_list = selected_features\n",
    "                print(f\"Selected {len(feature_list)} features\")\n",
    "                \n",
    "                # Keep only selected features\n",
    "                available_features = [col for col in feature_list if col in combined.columns]\n",
    "                combined = combined[available_features]\n",
    "                \n",
    "                # Set a flag to indicate we've done feature selection\n",
    "                first_chunk = False\n",
    "                \n",
    "                # Save with mode='w' (write) for first chunk\n",
    "                combined.to_csv(output_file, index=False)\n",
    "            else:\n",
    "                # Keep only selected features\n",
    "                available_features = [col for col in feature_list if col in combined.columns]\n",
    "                combined = combined[available_features]\n",
    "                \n",
    "                # Save with mode='a' (append) for subsequent chunks\n",
    "                combined.to_csv(output_file, mode='a', header=False, index=False)\n",
    "                \n",
    "            # Clear chunks list to free memory\n",
    "            chunks = []\n",
    "    \n",
    "    # Save any remaining chunks\n",
    "    if chunks:\n",
    "        combined = pd.concat(chunks)\n",
    "        \n",
    "        # Keep only selected features\n",
    "        if feature_list is not None:\n",
    "            available_features = [col for col in feature_list if col in combined.columns]\n",
    "            combined = combined[available_features]\n",
    "            \n",
    "        # Append to the output file\n",
    "        combined.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    print(f\"ML preprocessing complete! Output saved to {output_file}\")\n",
    "    \n",
    "    # Return for subsequent operations\n",
    "    return encoders, categorical_cols, feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b665d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML preprocessing of /Users/osx/flightDelayPIPELINE.2/data/processed/base_preprocessed_flights.csv...\n",
      "Processed chunk 1: 500,000 rows in 2.40 seconds\n",
      "Performing feature selection...\n",
      "Selected 21 features\n",
      "Processed chunk 2: 500,000 rows in 2.25 seconds\n",
      "Processed chunk 3: 500,000 rows in 2.21 seconds\n",
      "Processed chunk 4: 500,000 rows in 2.87 seconds\n",
      "Processed chunk 5: 500,000 rows in 2.60 seconds\n",
      "Processed chunk 6: 20,003 rows in 0.09 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execute the ML preprocessing pipeline\n",
    "encoders, categorical_cols, feature_list = prepare_ml_dataset(BASE_PROCESSED_PATH, ML_PROCESSED_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84afe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML processed data shape: (1000, 21)\n",
      "\n",
      "Columns in ML processed data:\n",
      "- DEP_DELAY_IS_OUTLIER: int64\n",
      "- ARR_DELAY: float64\n",
      "- DEP_TIME: float64\n",
      "- DEP_DIFF: float64\n",
      "- WHEELS_OFF: float64\n",
      "- DEP_HOUR_SIN: float64\n",
      "- DEP_HOUR: int64\n",
      "- CRS_DEP_TIME: int64\n",
      "- CRS_ARR_TIME: int64\n",
      "- TIME_OF_DAY_Morning: float64\n",
      "- TIME_OF_DAY_Evening: float64\n",
      "- DIVERTED: int64\n",
      "- DAY_OF_WEEK_COS: float64\n",
      "- TIME_OF_DAY_Afternoon: float64\n",
      "- TIME_OF_DAY_Night: float64\n",
      "- MONTH_COS: float64\n",
      "- TAXI_OUT: float64\n",
      "- DEP_HOUR_COS: float64\n",
      "- YEAR: int64\n",
      "- CANCELLATION_CODE_B: float64\n",
      "- DEP_DELAY: float64\n",
      "\n",
      "Sample of ML processed data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEP_DELAY_IS_OUTLIER</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DIFF</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>DEP_HOUR_SIN</th>\n",
       "      <th>DEP_HOUR</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>TIME_OF_DAY_Morning</th>\n",
       "      <th>TIME_OF_DAY_Evening</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>DAY_OF_WEEK_COS</th>\n",
       "      <th>TIME_OF_DAY_Afternoon</th>\n",
       "      <th>TIME_OF_DAY_Night</th>\n",
       "      <th>MONTH_COS</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>DEP_HOUR_COS</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>CANCELLATION_CODE_B</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>1151.0</td>\n",
       "      <td>436.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>11</td>\n",
       "      <td>715</td>\n",
       "      <td>901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2114.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>2123.0</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>21</td>\n",
       "      <td>1280</td>\n",
       "      <td>1395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>9</td>\n",
       "      <td>594</td>\n",
       "      <td>772</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1608.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>16</td>\n",
       "      <td>969</td>\n",
       "      <td>1109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>42.5</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>610</td>\n",
       "      <td>670</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEP_DELAY_IS_OUTLIER  ARR_DELAY  DEP_TIME  DEP_DIFF  WHEELS_OFF  \\\n",
       "0                     0      -14.0    1151.0     436.0      1210.0   \n",
       "1                     0       -5.0    2114.0     834.0      2123.0   \n",
       "2                     0        0.0    1000.0     406.0      1020.0   \n",
       "3                     0       24.0    1608.0     639.0      1635.0   \n",
       "4                     1       42.5    1237.0     627.0      1252.0   \n",
       "\n",
       "   DEP_HOUR_SIN  DEP_HOUR  CRS_DEP_TIME  CRS_ARR_TIME  TIME_OF_DAY_Morning  \\\n",
       "0      0.258819        11           715           901                  1.0   \n",
       "1     -0.707107        21          1280          1395                  0.0   \n",
       "2      0.707107         9           594           772                  1.0   \n",
       "3     -0.866025        16           969          1109                  0.0   \n",
       "4      0.500000        10           610           670                  1.0   \n",
       "\n",
       "   TIME_OF_DAY_Evening  DIVERTED  DAY_OF_WEEK_COS  TIME_OF_DAY_Afternoon  \\\n",
       "0                  0.0         0        -0.900969                    0.0   \n",
       "1                  1.0         0         0.623490                    0.0   \n",
       "2                  0.0         0        -0.222521                    0.0   \n",
       "3                  0.0         0         0.623490                    1.0   \n",
       "4                  0.0         0        -0.900969                    0.0   \n",
       "\n",
       "   TIME_OF_DAY_Night     MONTH_COS  TAXI_OUT  DEP_HOUR_COS  YEAR  \\\n",
       "0                0.0  8.660254e-01      19.0     -0.965926  2019   \n",
       "1                0.0  8.660254e-01       9.0      0.707107  2022   \n",
       "2                0.0 -8.660254e-01      20.0     -0.707107  2022   \n",
       "3                0.0  6.123234e-17      27.0     -0.500000  2023   \n",
       "4                0.0 -8.660254e-01      15.0     -0.866025  2019   \n",
       "\n",
       "   CANCELLATION_CODE_B  DEP_DELAY  \n",
       "0                  0.0       -4.0  \n",
       "1                  0.0       -6.0  \n",
       "2                  0.0        6.0  \n",
       "3                  0.0       -1.0  \n",
       "4                  0.0      147.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify the output file\n",
    "try:\n",
    "    # Read a sample of the processed data\n",
    "    ml_sample = pd.read_csv(ML_PROCESSED_PATH, nrows=1000)\n",
    "    print(f\"ML processed data shape: {ml_sample.shape}\")\n",
    "    print(\"\\nColumns in ML processed data:\")\n",
    "    for col in ml_sample.columns:\n",
    "        print(f\"- {col}: {ml_sample[col].dtype}\")\n",
    "    \n",
    "    print(\"\\nSample of ML processed data:\")\n",
    "    display(ml_sample.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading processed file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in time-based splitting demonstration: FL_DATE not found in dataframe\n"
     ]
    }
   ],
   "source": [
    "# Load a sample of the full ML-ready dataset for time-based splitting demonstration\n",
    "try:\n",
    "    # Read a larger sample for demonstration\n",
    "    ml_data_sample = pd.read_csv(ML_PROCESSED_PATH, nrows=100000)\n",
    "    \n",
    "    # Convert date column back to datetime if needed\n",
    "    if 'FL_DATE' in ml_data_sample.columns:\n",
    "        ml_data_sample['FL_DATE'] = pd.to_datetime(ml_data_sample['FL_DATE'])\n",
    "    \n",
    "    # Perform time-based train-test-validation split\n",
    "    train_data, val_data, test_data = time_based_train_test_split(ml_data_sample)\n",
    "    \n",
    "    # Visualize the distribution of the target variable in each split\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    train_data['DEP_DELAY'].hist(bins=50, alpha=0.7)\n",
    "    plt.title('Train Set DEP_DELAY Distribution')\n",
    "    plt.xlabel('Departure Delay (minutes)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    val_data['DEP_DELAY'].hist(bins=50, alpha=0.7)\n",
    "    plt.title('Validation Set DEP_DELAY Distribution')\n",
    "    plt.xlabel('Departure Delay (minutes)')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    test_data['DEP_DELAY'].hist(bins=50, alpha=0.7)\n",
    "    plt.title('Test Set DEP_DELAY Distribution')\n",
    "    plt.xlabel('Departure Delay (minutes)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in time-based splitting demonstration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a00a1c",
   "metadata": {},
   "source": [
    "## Summary of Machine Learning Preprocessing\n",
    "\n",
    "The ML preprocessing pipeline has:\n",
    "\n",
    "1. Added ML-specific engineered features\n",
    "2. Handled outliers using winsorization\n",
    "3. Encoded categorical variables using appropriate techniques\n",
    "4. Selected the most relevant features\n",
    "5. Prepared the data for time-based validation\n",
    "6. Created a dataset ready for ML model training\n",
    "\n",
    "This ML-ready dataset is optimized for traditional machine learning algorithms like XGBoost, Random Forest, etc. It includes properly encoded categorical variables, handles outliers and missing values, and provides a robust selection of features with strong predictive power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
