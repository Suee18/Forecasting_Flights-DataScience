{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a818ff03",
   "metadata": {},
   "source": [
    "# Flight Delay Prediction - Time Series Pipeline\n",
    "\n",
    "This notebook implements the time series pipeline for the flight delay prediction project. It includes specialized preprocessing operations for time series modeling approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bee818",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for imports\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# Import the BasePipeline from our base pipeline notebook\n",
    "%run \"base_pipeline.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067d4ba",
   "metadata": {},
   "source": [
    "## Time Series Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426532b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesPipeline(BasePipeline):\n",
    "    \"\"\"Time series preprocessing pipeline for flight delay prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"\n",
    "        Initialize the time series pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        config : dict, optional\n",
    "            Configuration parameters for the pipeline.\n",
    "            \n",
    "        Additional Parameters:\n",
    "        ---------------------\n",
    "        timestamp_col : str\n",
    "            Column to use as timestamp\n",
    "        resample_freq : str\n",
    "            Pandas frequency string for resampling (e.g. 'H', 'D')\n",
    "        n_lags : int\n",
    "            Number of lag features to create\n",
    "        rolling_windows : list\n",
    "            List of window sizes for rolling features\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Default config\n",
    "        default_config = {\n",
    "            'timestamp_col': 'FL_DATE',\n",
    "            'resample_freq': 'D',  # Daily resampling\n",
    "            'n_lags': 7,  # One week of lags\n",
    "            'rolling_windows': [3, 7, 14],  # Rolling features windows\n",
    "            'ts_test_size': 0.2,  # Time series test split ratio\n",
    "            'use_time_split': True  # Use time-based split instead of random\n",
    "        }\n",
    "        \n",
    "        # Update with user config\n",
    "        if config is not None:\n",
    "            default_config.update(config)\n",
    "        \n",
    "        self.config = default_config\n",
    "        \n",
    "    def create_datetime_features(self, df):\n",
    "        \"\"\"Extract datetime features from timestamp column.\"\"\"\n",
    "        print(\"Creating datetime features...\")\n",
    "        \n",
    "        ts_col = self.config['timestamp_col']\n",
    "        \n",
    "        if ts_col in df.columns:\n",
    "            # Ensure the column is datetime type\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df[ts_col]):\n",
    "                df[ts_col] = pd.to_datetime(df[ts_col])\n",
    "            \n",
    "            # Extract datetime components\n",
    "            df['year'] = df[ts_col].dt.year\n",
    "            df['month'] = df[ts_col].dt.month\n",
    "            df['day'] = df[ts_col].dt.day\n",
    "            df['dayofweek'] = df[ts_col].dt.dayofweek\n",
    "            df['quarter'] = df[ts_col].dt.quarter\n",
    "            df['dayofyear'] = df[ts_col].dt.dayofyear\n",
    "            df['weekofyear'] = df[ts_col].dt.isocalendar().week\n",
    "            \n",
    "            # Cyclical encoding of month, day of week (sine/cosine transformation)\n",
    "            # This preserves the cyclical nature of these features\n",
    "            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "            df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "            df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "            \n",
    "            # Is holiday/weekend feature\n",
    "            if pd.api.types.is_datetime64_any_dtype(df[ts_col]):\n",
    "                df['is_weekend'] = df[ts_col].dt.dayofweek >= 5\n",
    "            \n",
    "            # Handle hour of day if available\n",
    "            if 'dep_hour' in df.columns:\n",
    "                df['hour_sin'] = np.sin(2 * np.pi * df['dep_hour'] / 24)\n",
    "                df['hour_cos'] = np.cos(2 * np.pi * df['dep_hour'] / 24)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def resample_to_frequency(self, df, group_cols=None):\n",
    "        \"\"\"\n",
    "        Resample data to specified frequency.\n",
    "        \n",
    "        This aggregates the data to a coarser time granularity, which is often\n",
    "        necessary for time series forecasting.\n",
    "        \"\"\"\n",
    "        print(f\"Resampling to {self.config['resample_freq']} frequency...\")\n",
    "        \n",
    "        ts_col = self.config['timestamp_col']\n",
    "        \n",
    "        # If we need to group by additional columns (e.g. by airport)\n",
    "        if group_cols is not None:\n",
    "            # Group by time and specified columns\n",
    "            resampled_dfs = []\n",
    "            for name, group in df.groupby(group_cols):\n",
    "                # Convert to single group name if only one group column\n",
    "                if not isinstance(name, tuple):\n",
    "                    name = (name,)\n",
    "                \n",
    "                # Set timestamp as index for resampling\n",
    "                group = group.set_index(ts_col)\n",
    "                \n",
    "                # Resample and aggregate\n",
    "                resampled = group.resample(self.config['resample_freq']).agg({\n",
    "                    self.target_column: 'mean',\n",
    "                    'is_delayed': 'mean',  # Becomes the proportion of delayed flights\n",
    "                    'year': 'first',\n",
    "                    'month': 'first',\n",
    "                    'dayofweek': 'first',\n",
    "                    'DISTANCE': 'mean'\n",
    "                })\n",
    "                \n",
    "                # Add back the group columns\n",
    "                for i, col in enumerate(group_cols):\n",
    "                    resampled[col] = name[i]\n",
    "                \n",
    "                resampled = resampled.reset_index()\n",
    "                resampled_dfs.append(resampled)\n",
    "                \n",
    "            # Combine all resampled groups\n",
    "            if resampled_dfs:\n",
    "                df = pd.concat(resampled_dfs, axis=0)\n",
    "            \n",
    "        else:\n",
    "            # Simple resampling without additional grouping\n",
    "            df = df.set_index(ts_col)\n",
    "            df = df.resample(self.config['resample_freq']).agg({\n",
    "                self.target_column: 'mean',\n",
    "                'is_delayed': 'mean',\n",
    "                'year': 'first',\n",
    "                'month': 'first',\n",
    "                'dayofweek': 'first',\n",
    "                'DISTANCE': 'mean'\n",
    "            })\n",
    "            df = df.reset_index()\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def generate_lag_features(self, df, group_cols=None):\n",
    "        \"\"\"\n",
    "        Generate lagged features for time series forecasting.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Input dataframe\n",
    "            \n",
    "        group_cols : list, optional\n",
    "            Columns to group by before creating lags (e.g., airport)\n",
    "        \"\"\"\n",
    "        print(\"Generating lag features...\")\n",
    "        \n",
    "        target = self.target_column\n",
    "        n_lags = self.config['n_lags']\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        ts_col = self.config['timestamp_col']\n",
    "        df = df.sort_values(ts_col)\n",
    "        \n",
    "        # If we need to create lags within groups\n",
    "        if group_cols is not None:\n",
    "            for col in group_cols:\n",
    "                if col not in df.columns:\n",
    "                    raise ValueError(f\"Group column {col} not found in dataframe\")\n",
    "            \n",
    "            # Group and create lags\n",
    "            for lag in range(1, n_lags + 1):\n",
    "                lag_values = df.groupby(group_cols)[target].shift(lag)\n",
    "                df[f\"{target}_lag_{lag}\"] = lag_values\n",
    "                \n",
    "                # Also create lags of the is_delayed feature if it exists\n",
    "                if 'is_delayed' in df.columns:\n",
    "                    delay_lag_values = df.groupby(group_cols)['is_delayed'].shift(lag)\n",
    "                    df[f\"is_delayed_lag_{lag}\"] = delay_lag_values\n",
    "                    \n",
    "        else:\n",
    "            # Create lags without grouping\n",
    "            for lag in range(1, n_lags + 1):\n",
    "                df[f\"{target}_lag_{lag}\"] = df[target].shift(lag)\n",
    "                \n",
    "                if 'is_delayed' in df.columns:\n",
    "                    df[f\"is_delayed_lag_{lag}\"] = df['is_delayed'].shift(lag)\n",
    "                    \n",
    "        # Drop rows with NaN lag values (first n_lags rows)\n",
    "        df = df.dropna(subset=[f\"{target}_lag_{n_lags}\"])\n",
    "                    \n",
    "        return df\n",
    "    \n",
    "    def create_rolling_features(self, df, group_cols=None):\n",
    "        \"\"\"\n",
    "        Create rolling window features like moving averages.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Input dataframe\n",
    "            \n",
    "        group_cols : list, optional\n",
    "            Columns to group by before creating rolling features\n",
    "        \"\"\"\n",
    "        print(\"Creating rolling features...\")\n",
    "        \n",
    "        target = self.target_column\n",
    "        windows = self.config['rolling_windows']\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        ts_col = self.config['timestamp_col']\n",
    "        df = df.sort_values(ts_col)\n",
    "        \n",
    "        # If we need to create rolling features within groups\n",
    "        if group_cols is not None:\n",
    "            # Group and create rolling features\n",
    "            for window in windows:\n",
    "                # Rolling mean\n",
    "                roll_means = df.groupby(group_cols)[target].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).mean()\n",
    "                )\n",
    "                df[f\"{target}_roll_mean_{window}\"] = roll_means\n",
    "                \n",
    "                # Rolling standard deviation\n",
    "                roll_stds = df.groupby(group_cols)[target].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).std()\n",
    "                )\n",
    "                df[f\"{target}_roll_std_{window}\"] = roll_stds.fillna(0)\n",
    "                \n",
    "                # Rolling max\n",
    "                roll_maxs = df.groupby(group_cols)[target].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).max()\n",
    "                )\n",
    "                df[f\"{target}_roll_max_{window}\"] = roll_maxs\n",
    "                \n",
    "        else:\n",
    "            # Create rolling features without grouping\n",
    "            for window in windows:\n",
    "                df[f\"{target}_roll_mean_{window}\"] = df[target].rolling(window, min_periods=1).mean()\n",
    "                df[f\"{target}_roll_std_{window}\"] = df[target].rolling(window, min_periods=1).std().fillna(0)\n",
    "                df[f\"{target}_roll_max_{window}\"] = df[target].rolling(window, min_periods=1).max()\n",
    "                \n",
    "        return df\n",
    "    \n",
    "    def handle_seasonality(self, df):\n",
    "        \"\"\"\n",
    "        Handle seasonality using decomposition or seasonal features.\n",
    "        \"\"\"\n",
    "        print(\"Handling seasonality...\")\n",
    "        \n",
    "        # Add seasonal indicators\n",
    "        # For daily data: day of week indicators\n",
    "        if 'dayofweek' in df.columns:\n",
    "            for day in range(7):\n",
    "                df[f'is_day_{day}'] = (df['dayofweek'] == day).astype(int)\n",
    "                \n",
    "        # For monthly data: month indicators  \n",
    "        if 'month' in df.columns:\n",
    "            for month in range(1, 13):\n",
    "                df[f'is_month_{month}'] = (df['month'] == month).astype(int)\n",
    "                \n",
    "        # Add US holiday indicators if we have a library for it\n",
    "        # This would require additional libraries like holidays\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def apply_time_based_split(self, df, test_size=None):\n",
    "        \"\"\"\n",
    "        Split data based on time rather than randomly.\n",
    "        \n",
    "        For time series, we should always split chronologically to avoid\n",
    "        data leakage, with the test set being the most recent data.\n",
    "        \"\"\"\n",
    "        print(\"Applying time-based data split...\")\n",
    "        \n",
    "        ts_col = self.config['timestamp_col']\n",
    "        \n",
    "        if test_size is None:\n",
    "            test_size = self.config['ts_test_size']\n",
    "            \n",
    "        # Sort by time\n",
    "        df = df.sort_values(ts_col)\n",
    "        \n",
    "        # Calculate split point\n",
    "        split_idx = int(len(df) * (1 - test_size))\n",
    "        val_idx = int(len(df) * (1 - 2 * test_size))\n",
    "        \n",
    "        # Split the data\n",
    "        train = df.iloc[:val_idx].copy()\n",
    "        val = df.iloc[val_idx:split_idx].copy()\n",
    "        test = df.iloc[split_idx:].copy()\n",
    "        \n",
    "        print(f\"Train: {len(train)} samples, Validation: {len(val)} samples, Test: {len(test)} samples\")\n",
    "        print(f\"Train period: {train[ts_col].min()} to {train[ts_col].max()}\")\n",
    "        print(f\"Val period: {val[ts_col].min()} to {val[ts_col].max()}\")\n",
    "        print(f\"Test period: {test[ts_col].min()} to {test[ts_col].max()}\")\n",
    "        \n",
    "        return train, val, test\n",
    "    \n",
    "    def run(self, data_path, group_cols=None):\n",
    "        \"\"\"Run the complete time series pipeline.\"\"\"\n",
    "        # Run base pipeline steps first (except splitting)\n",
    "        df = self.load_data(data_path)\n",
    "        df = self.clean_data(df)\n",
    "        df = self.handle_missing_values(df)\n",
    "        df = self.encode_categorical_variables(df)\n",
    "        df = self.generate_basic_features(df)\n",
    "        \n",
    "        # Time series specific steps\n",
    "        df = self.create_datetime_features(df)\n",
    "        \n",
    "        # Optional resampling\n",
    "        if self.config.get('perform_resampling', False):\n",
    "            df = self.resample_to_frequency(df, group_cols)\n",
    "            \n",
    "        df = self.generate_lag_features(df, group_cols)\n",
    "        df = self.create_rolling_features(df, group_cols)\n",
    "        df = self.handle_seasonality(df)\n",
    "        \n",
    "        # Time-based split\n",
    "        if self.config.get('use_time_split', True):\n",
    "            train, val, test = self.apply_time_based_split(df)\n",
    "        else:\n",
    "            train, val, test = self.split_data(df)\n",
    "            \n",
    "        return {\n",
    "            'train': train,\n",
    "            'validation': val, \n",
    "            'test': test,\n",
    "            'full_data': df\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4247da8",
   "metadata": {},
   "source": [
    "## Test Time Series Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf7daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path to raw data\n",
    "file_path = os.path.join(PROJECT_ROOT, 'data', 'raw', 'flights_sample_3m.csv')\n",
    "\n",
    "# Configure the time series pipeline\n",
    "ts_config = {\n",
    "    'timestamp_col': 'FL_DATE',\n",
    "    'resample_freq': 'D',  # Daily resampling\n",
    "    'n_lags': 7,           # One week of lags\n",
    "    'rolling_windows': [3, 7, 14],  # Different rolling windows\n",
    "    'perform_resampling': False,  # Skip resampling for testing\n",
    "    'use_time_split': True  # Use time-based split\n",
    "}\n",
    "\n",
    "# Create time series pipeline instance\n",
    "ts_pipeline = TimeSeriesPipeline(config=ts_config)\n",
    "\n",
    "# Load a sample of the data to test\n",
    "sample_df = pd.read_csv(file_path, nrows=10000)\n",
    "print(f\"Sample data shape: {sample_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc68cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test time series specific preprocessing steps\n",
    "# Start with a cleaned dataframe with basic features\n",
    "prepared_df = ts_pipeline.clean_data(sample_df)\n",
    "prepared_df = ts_pipeline.handle_missing_values(prepared_df)\n",
    "prepared_df = ts_pipeline.generate_basic_features(prepared_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime features\n",
    "datetime_df = ts_pipeline.create_datetime_features(prepared_df)\n",
    "print(\"\\nDatetime features added:\")\n",
    "datetime_cols = [col for col in datetime_df.columns if col not in prepared_df.columns]\n",
    "print(datetime_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lag features\n",
    "# Group by origin airport\n",
    "group_cols = ['ORIGIN']\n",
    "lag_df = ts_pipeline.generate_lag_features(datetime_df, group_cols)\n",
    "print(\"\\nLag features added:\")\n",
    "lag_cols = [col for col in lag_df.columns if 'lag' in col]\n",
    "print(lag_cols[:5], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4baea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rolling features\n",
    "rolling_df = ts_pipeline.create_rolling_features(lag_df, group_cols)\n",
    "print(\"\\nRolling features added:\")\n",
    "roll_cols = [col for col in rolling_df.columns if 'roll' in col]\n",
    "print(roll_cols[:5], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1841c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle seasonality\n",
    "seasonal_df = ts_pipeline.handle_seasonality(rolling_df)\n",
    "print(\"\\nSeasonal features added:\")\n",
    "seasonal_cols = [col for col in seasonal_df.columns if 'is_day' in col or 'is_month' in col]\n",
    "print(seasonal_cols[:5], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fd4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply time-based split\n",
    "train, val, test = ts_pipeline.apply_time_based_split(seasonal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e6484",
   "metadata": {},
   "source": [
    "## Run Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency, run on a sample of the data\n",
    "sample_path = os.path.join(PROJECT_ROOT, 'data', 'raw', 'flights_sample_3m.csv')\n",
    "\n",
    "# For ATL airport specifically\n",
    "group_cols = ['ORIGIN']\n",
    "result = ts_pipeline.run(sample_path, group_cols=group_cols)\n",
    "\n",
    "print(\"\\nPipeline execution complete!\")\n",
    "for key, df in result.items():\n",
    "    if key != 'full_data':\n",
    "        print(f\"{key} shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd48c3",
   "metadata": {},
   "source": [
    "## Explore Generated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da7814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a sample of the processed data for one airport\n",
    "airport = 'ATL'\n",
    "airport_data = result['train'][result['train']['ORIGIN'] == airport].copy()\n",
    "\n",
    "# Sort by date\n",
    "airport_data = airport_data.sort_values('FL_DATE')\n",
    "\n",
    "print(f\"\\nSample of time series features for {airport} airport:\")\n",
    "cols_to_show = ['FL_DATE', 'DEP_DELAY', 'DEP_DELAY_lag_1', 'DEP_DELAY_roll_mean_7']\n",
    "print(airport_data[cols_to_show].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the time series and lag features\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(airport_data['FL_DATE'], airport_data['DEP_DELAY'], label='Actual Delay')\n",
    "plt.plot(airport_data['FL_DATE'], airport_data['DEP_DELAY_lag_1'], label='1-Day Lag')\n",
    "plt.plot(airport_data['FL_DATE'], airport_data['DEP_DELAY_roll_mean_7'], label='7-Day Rolling Mean')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Delay (minutes)')\n",
    "plt.title(f'Time Series Features for {airport} Airport')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39542176",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c00513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data to disk for future use\n",
    "ts_train_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'ts_train.csv')\n",
    "ts_val_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'ts_val.csv')\n",
    "ts_test_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'ts_test.csv')\n",
    "\n",
    "result['train'].to_csv(ts_train_path, index=False)\n",
    "result['validation'].to_csv(ts_val_path, index=False)\n",
    "result['test'].to_csv(ts_test_path, index=False)\n",
    "\n",
    "print(f\"Time series data saved to processed directory\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
