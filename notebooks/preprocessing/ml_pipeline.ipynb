{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3325a2e2",
   "metadata": {},
   "source": [
    "# Flight Delay Prediction - Machine Learning Pipeline\n",
    "\n",
    "This notebook implements the traditional machine learning pipeline for the flight delay prediction project. It includes specialized preprocessing operations for traditional ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f89ac",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce706d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Add src directory to path for imports\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# Import the BasePipeline from our base pipeline notebook\n",
    "%run \"base_pipeline.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8373121a",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPipeline(BasePipeline):\n",
    "    \"\"\"Machine learning preprocessing pipeline for flight delay prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"\n",
    "        Initialize the machine learning pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        config : dict, optional\n",
    "            Configuration parameters for the pipeline.\n",
    "            \n",
    "        Additional Parameters:\n",
    "        ---------------------\n",
    "        feature_selection : str\n",
    "            Method for feature selection ('none', 'kbest', 'rfe', 'pca')\n",
    "        k_features : int\n",
    "            Number of features to select\n",
    "        scaler_type : str\n",
    "            Type of scaling to apply ('standard', 'robust')\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Default config for ML-specific settings\n",
    "        default_config = {\n",
    "            'feature_selection': 'none',  # 'none', 'kbest', 'rfe', 'pca'\n",
    "            'k_features': 20,  # Number of features to select\n",
    "            'scaler_type': 'robust',  # 'standard' or 'robust'\n",
    "            'handle_outliers': True,  # Whether to remove outliers\n",
    "            'outlier_threshold': 3.0,  # Z-score threshold for outliers\n",
    "            'create_polynomial': False,  # Whether to create polynomial features\n",
    "            'poly_degree': 2  # Degree of polynomial features\n",
    "        }\n",
    "        \n",
    "        # Update with user config\n",
    "        if config is not None:\n",
    "            default_config.update(config)\n",
    "        \n",
    "        self.config = default_config\n",
    "        \n",
    "    def advanced_feature_engineering(self, df):\n",
    "        \"\"\"Create advanced features for traditional ML models.\"\"\"\n",
    "        print(\"Performing advanced feature engineering...\")\n",
    "        \n",
    "        # Create interaction features between important variables\n",
    "        \n",
    "        # Distance and time features interactions\n",
    "        if 'DISTANCE' in df.columns and 'CRS_ELAPSED_TIME' in df.columns:\n",
    "            df['SPEED'] = df['DISTANCE'] / df['CRS_ELAPSED_TIME']\n",
    "        \n",
    "        # Time of day and day of week interactions\n",
    "        if 'dep_hour' in df.columns and 'DAY_OF_WEEK' in df.columns:\n",
    "            # Morning rush hour on weekdays\n",
    "            df['MORNING_RUSH'] = ((df['dep_hour'] >= 7) & \n",
    "                                 (df['dep_hour'] <= 9) & \n",
    "                                 (df['DAY_OF_WEEK'] < 5)).astype(int)\n",
    "            \n",
    "            # Evening rush hour on weekdays\n",
    "            df['EVENING_RUSH'] = ((df['dep_hour'] >= 16) & \n",
    "                                 (df['dep_hour'] <= 18) & \n",
    "                                 (df['DAY_OF_WEEK'] < 5)).astype(int)\n",
    "        \n",
    "        # Weekend flag\n",
    "        if 'DAY_OF_WEEK' in df.columns:\n",
    "            df['IS_WEEKEND'] = (df['DAY_OF_WEEK'] >= 5).astype(int)\n",
    "        \n",
    "        # Airport busyness (if we have carrier flight number)\n",
    "        # This approximates how busy each origin airport is\n",
    "        if 'ORIGIN' in df.columns:\n",
    "            origin_counts = df['ORIGIN'].value_counts()\n",
    "            df['ORIGIN_FLIGHTS_COUNT'] = df['ORIGIN'].map(origin_counts)\n",
    "        \n",
    "        # Create polynomial features if configured\n",
    "        if self.config.get('create_polynomial', False):\n",
    "            from sklearn.preprocessing import PolynomialFeatures\n",
    "            degree = self.config.get('poly_degree', 2)\n",
    "            \n",
    "            # Select numerical columns for polynomial features\n",
    "            poly_cols = ['DISTANCE', 'CRS_ELAPSED_TIME']\n",
    "            present_cols = [col for col in poly_cols if col in df.columns]\n",
    "            \n",
    "            if present_cols:\n",
    "                poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "                poly_features = poly.fit_transform(df[present_cols])\n",
    "                \n",
    "                # Create column names for polynomial features\n",
    "                feature_names = poly.get_feature_names_out(present_cols)\n",
    "                \n",
    "                # Add polynomial features to dataframe, skipping the originals\n",
    "                for i, name in enumerate(feature_names):\n",
    "                    if name in present_cols:  # Skip original features\n",
    "                        continue\n",
    "                    df[f\"POLY_{name}\"] = poly_features[:, i]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def detect_outliers(self, df):\n",
    "        \"\"\"Detect and handle outliers in the data.\"\"\"\n",
    "        if not self.config.get('handle_outliers', True):\n",
    "            return df\n",
    "            \n",
    "        print(\"Detecting and handling outliers...\")\n",
    "        \n",
    "        target = self.target_column\n",
    "        threshold = self.config.get('outlier_threshold', 3.0)\n",
    "        \n",
    "        # Detect outliers in the target variable\n",
    "        if target in df.columns:\n",
    "            # Calculate z-scores\n",
    "            z_scores = np.abs((df[target] - df[target].mean()) / df[target].std())\n",
    "            \n",
    "            # Identify outliers\n",
    "            outliers = z_scores > threshold\n",
    "            print(f\"Detected {outliers.sum()} outliers in {target}\")\n",
    "            \n",
    "            # Option 1: Remove outliers\n",
    "            # df = df[~outliers].copy()\n",
    "            \n",
    "            # Option 2: Clip outliers to the threshold\n",
    "            upper_bound = df[target].mean() + threshold * df[target].std()\n",
    "            lower_bound = df[target].mean() - threshold * df[target].std()\n",
    "            df[target] = df[target].clip(lower=lower_bound, upper=upper_bound)\n",
    "            \n",
    "        # Detect outliers in numerical features\n",
    "        for col in self.numerical_columns:\n",
    "            if col in df.columns and df[col].dtype in [np.float64, np.int64]:\n",
    "                # Calculate z-scores\n",
    "                z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "                \n",
    "                # Identify outliers\n",
    "                outliers = z_scores > threshold\n",
    "                if outliers.sum() > 0:\n",
    "                    print(f\"Detected {outliers.sum()} outliers in {col}\")\n",
    "                    \n",
    "                    # Clip outliers to the threshold\n",
    "                    upper_bound = df[col].mean() + threshold * df[col].std()\n",
    "                    lower_bound = df[col].mean() - threshold * df[col].std()\n",
    "                    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scale_features(self, df, fit=True):\n",
    "        \"\"\"Scale numerical features for ML models.\"\"\"\n",
    "        print(\"Scaling features...\")\n",
    "        \n",
    "        # Get numerical columns (including engineered features)\n",
    "        numerical_cols = [col for col in df.columns \n",
    "                         if col != self.target_column \n",
    "                         and df[col].dtype in [np.float64, np.int64] \n",
    "                         and not col.startswith('is_') \n",
    "                         and not col.endswith('_idx')]\n",
    "        \n",
    "        # Create scaler if fitting\n",
    "        if fit:\n",
    "            if self.config['scaler_type'] == 'standard':\n",
    "                self.scaler = StandardScaler()\n",
    "            else:  # Use RobustScaler for outlier resilience\n",
    "                self.scaler = RobustScaler()\n",
    "                \n",
    "            # Fit scaler\n",
    "            self.scaler.fit(df[numerical_cols])\n",
    "        \n",
    "        # Transform data\n",
    "        if hasattr(self, 'scaler'):\n",
    "            scaled_data = self.scaler.transform(df[numerical_cols])\n",
    "            \n",
    "            # Replace original columns with scaled values\n",
    "            for i, col in enumerate(numerical_cols):\n",
    "                df[col] = scaled_data[:, i]\n",
    "        else:\n",
    "            print(\"Warning: No scaler found. Run with fit=True first.\")\n",
    "                \n",
    "        return df\n",
    "    \n",
    "    def select_features(self, X, y, fit=True):\n",
    "        \"\"\"Select most important features for the model.\"\"\"\n",
    "        feature_selection = self.config.get('feature_selection', 'none')\n",
    "        \n",
    "        if feature_selection == 'none':\n",
    "            return X\n",
    "        \n",
    "        print(f\"Selecting features using {feature_selection}...\")\n",
    "        \n",
    "        k = self.config.get('k_features', 20)\n",
    "        k = min(k, X.shape[1])  # Ensure k is not larger than number of features\n",
    "        \n",
    "        if fit:\n",
    "            if feature_selection == 'kbest':\n",
    "                # Select top k features based on correlation with target\n",
    "                self.selector = SelectKBest(f_regression, k=k)\n",
    "                self.selector.fit(X, y)\n",
    "                \n",
    "                # Store selected feature names\n",
    "                if isinstance(X, pd.DataFrame):\n",
    "                    self.selected_features = X.columns[self.selector.get_support()].tolist()\n",
    "                \n",
    "            elif feature_selection == 'rfe':\n",
    "                # Recursive feature elimination\n",
    "                from sklearn.ensemble import RandomForestRegressor\n",
    "                base_model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "                self.selector = RFECV(estimator=base_model, step=1, cv=3, min_features_to_select=k)\n",
    "                self.selector.fit(X, y)\n",
    "                \n",
    "                # Store selected feature names\n",
    "                if isinstance(X, pd.DataFrame):\n",
    "                    self.selected_features = X.columns[self.selector.get_support()].tolist()\n",
    "                \n",
    "            elif feature_selection == 'pca':\n",
    "                # Principal component analysis for dimensionality reduction\n",
    "                self.selector = PCA(n_components=k)\n",
    "                self.selector.fit(X)\n",
    "                \n",
    "                # For PCA, we don't have specific features, but components\n",
    "                if isinstance(X, pd.DataFrame):\n",
    "                    self.selected_features = [f\"PC{i+1}\" for i in range(k)]\n",
    "                    \n",
    "        # Transform data using selected features or components\n",
    "        if hasattr(self, 'selector'):\n",
    "            X_selected = self.selector.transform(X)\n",
    "            \n",
    "            # If using PCA, return transformed data\n",
    "            if feature_selection == 'pca':\n",
    "                if isinstance(X, pd.DataFrame):\n",
    "                    return pd.DataFrame(X_selected, index=X.index, columns=self.selected_features)\n",
    "                return X_selected\n",
    "            \n",
    "            # For other methods, select columns from original data\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                return X[self.selected_features]\n",
    "            else:\n",
    "                return X_selected\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def prepare_X_y(self, df):\n",
    "        \"\"\"Separate features (X) and target (y) from dataframe.\"\"\"\n",
    "        print(\"Preparing X and y matrices...\")\n",
    "        \n",
    "        target = self.target_column\n",
    "        \n",
    "        if target not in df.columns:\n",
    "            raise ValueError(f\"Target column '{target}' not found in dataframe\")\n",
    "            \n",
    "        # Get all columns except target\n",
    "        X = df.drop(columns=[target])\n",
    "        y = df[target]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def run(self, data_path):\n",
    "        \"\"\"Run the complete machine learning pipeline.\"\"\"\n",
    "        # Run base pipeline steps first\n",
    "        df = self.load_data(data_path)\n",
    "        df = self.clean_data(df)\n",
    "        df = self.handle_missing_values(df)\n",
    "        df = self.encode_categorical_variables(df)\n",
    "        df = self.generate_basic_features(df)\n",
    "        \n",
    "        # Advanced feature engineering for ML\n",
    "        df = self.advanced_feature_engineering(df)\n",
    "        \n",
    "        # Handle outliers\n",
    "        df = self.detect_outliers(df)\n",
    "        \n",
    "        # Split the data\n",
    "        train_df, val_df, test_df = self.split_data(df)\n",
    "        \n",
    "        # Scale features\n",
    "        train_df = self.scale_features(train_df, fit=True)\n",
    "        val_df = self.scale_features(val_df, fit=False)\n",
    "        test_df = self.scale_features(test_df, fit=False)\n",
    "        \n",
    "        # Prepare X and y for each set\n",
    "        X_train, y_train = self.prepare_X_y(train_df)\n",
    "        X_val, y_val = self.prepare_X_y(val_df)\n",
    "        X_test, y_test = self.prepare_X_y(test_df)\n",
    "        \n",
    "        # Select features\n",
    "        X_train = self.select_features(X_train, y_train, fit=True)\n",
    "        X_val = self.select_features(X_val, y_val, fit=False)\n",
    "        X_test = self.select_features(X_test, y_test, fit=False)\n",
    "        \n",
    "        return {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'train_df': train_df,\n",
    "            'val_df': val_df,\n",
    "            'test_df': test_df,\n",
    "            'full_data': df\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00f762",
   "metadata": {},
   "source": [
    "## Test Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path to raw data\n",
    "file_path = os.path.join(PROJECT_ROOT, 'data', 'raw', 'flights_sample_3m.csv')\n",
    "\n",
    "# Configure the machine learning pipeline\n",
    "ml_config = {\n",
    "    'feature_selection': 'none',  # Start without feature selection\n",
    "    'scaler_type': 'robust',      # Robust scaling to handle outliers\n",
    "    'handle_outliers': True,      # Handle outliers in the data\n",
    "    'create_polynomial': True,    # Create polynomial features\n",
    "    'poly_degree': 2              # Quadratic features\n",
    "}\n",
    "\n",
    "# Create machine learning pipeline instance\n",
    "ml_pipeline = MLPipeline(config=ml_config)\n",
    "\n",
    "# Load a sample of the data to test\n",
    "sample_df = pd.read_csv(file_path, nrows=10000)\n",
    "print(f\"Sample data shape: {sample_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test machine learning specific preprocessing steps\n",
    "# Start with a cleaned dataframe with basic features\n",
    "prepared_df = ml_pipeline.clean_data(sample_df)\n",
    "prepared_df = ml_pipeline.handle_missing_values(prepared_df)\n",
    "prepared_df = ml_pipeline.generate_basic_features(prepared_df)\n",
    "prepared_df = ml_pipeline.encode_categorical_variables(prepared_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced feature engineering\n",
    "featured_df = ml_pipeline.advanced_feature_engineering(prepared_df)\n",
    "print(\"\\nAdvanced features added:\")\n",
    "new_cols = [col for col in featured_df.columns if col not in prepared_df.columns]\n",
    "print(new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test outlier detection and handling\n",
    "outlier_df = ml_pipeline.detect_outliers(featured_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature scaling\n",
    "scaled_df = ml_pipeline.scale_features(outlier_df)\n",
    "\n",
    "# Check the scaling results\n",
    "numerical_cols = ['DISTANCE', 'CRS_ELAPSED_TIME']\n",
    "present_cols = [col for col in numerical_cols if col in scaled_df.columns]\n",
    "\n",
    "if present_cols:\n",
    "    print(\"\\nScaled numeric columns statistics:\")\n",
    "    print(scaled_df[present_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prepare X_y\n",
    "X, y = ml_pipeline.prepare_X_y(scaled_df)\n",
    "print(f\"\\nX shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c7cab",
   "metadata": {},
   "source": [
    "## Test Feature Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec27115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SelectKBest feature selection\n",
    "kbest_config = ml_config.copy()\n",
    "kbest_config['feature_selection'] = 'kbest'\n",
    "kbest_config['k_features'] = 10\n",
    "\n",
    "kbest_pipeline = MLPipeline(config=kbest_config)\n",
    "X_kbest = kbest_pipeline.select_features(X, y)\n",
    "\n",
    "print(f\"\\nOriginal features: {X.shape[1]}\")\n",
    "print(f\"Selected features with KBest: {X_kbest.shape[1]}\")\n",
    "if hasattr(kbest_pipeline, 'selected_features'):\n",
    "    print(f\"Selected feature names: {kbest_pipeline.selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fe64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PCA feature reduction\n",
    "pca_config = ml_config.copy()\n",
    "pca_config['feature_selection'] = 'pca'\n",
    "pca_config['k_features'] = 10\n",
    "\n",
    "pca_pipeline = MLPipeline(config=pca_config)\n",
    "X_pca = pca_pipeline.select_features(X, y)\n",
    "\n",
    "print(f\"\\nOriginal dimensions: {X.shape[1]}\")\n",
    "print(f\"Reduced dimensions with PCA: {X_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc6624",
   "metadata": {},
   "source": [
    "## Run Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff358cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency, run on a sample of the data\n",
    "sample_path = os.path.join(PROJECT_ROOT, 'data', 'raw', 'flights_sample_3m.csv')\n",
    "\n",
    "# Use the original pipeline without feature selection for now\n",
    "result = ml_pipeline.run(sample_path)\n",
    "\n",
    "print(\"\\nPipeline execution complete!\")\n",
    "print(f\"X_train shape: {result['X_train'].shape}, y_train shape: {result['y_train'].shape}\")\n",
    "print(f\"X_val shape: {result['X_val'].shape}, y_val shape: {result['y_val'].shape}\")\n",
    "print(f\"X_test shape: {result['X_test'].shape}, y_test shape: {result['y_test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33275102",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e79107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature correlations with target\n",
    "if isinstance(result['X_train'], pd.DataFrame):\n",
    "    correlation_df = pd.concat([result['X_train'], result['y_train']], axis=1)\n",
    "    correlations = correlation_df.corr()[ml_pipeline.target_column].sort_values(ascending=False)\n",
    "    \n",
    "    # Plot top 15 correlations\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    correlations.drop(ml_pipeline.target_column).head(15).plot(kind='barh')\n",
    "    plt.title('Top 15 Features Correlated with Departure Delay')\n",
    "    plt.xlabel('Correlation')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a quick model to evaluate feature importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train a simple Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "rf.fit(result['X_train'], result['y_train'])\n",
    "\n",
    "# Get feature importances\n",
    "if isinstance(result['X_train'], pd.DataFrame):\n",
    "    importances = pd.Series(rf.feature_importances_, index=result['X_train'].columns)\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "    \n",
    "    # Plot top 15 feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    importances.head(15).plot(kind='barh')\n",
    "    plt.title('Random Forest Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230379ab",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data to disk for future use\n",
    "ml_X_train_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'ml_X_train.csv')\n",
    "ml_y_train_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'ml_y_train.csv')\n",
    "ml_X_val_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'ml_X_val.csv')\n",
    "ml_y_val_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'ml_y_val.csv')\n",
    "ml_X_test_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'ml_X_test.csv')\n",
    "ml_y_test_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'ml_y_test.csv')\n",
    "\n",
    "# Save X and y separately\n",
    "if isinstance(result['X_train'], pd.DataFrame):\n",
    "    result['X_train'].to_csv(ml_X_train_path, index=False)\n",
    "    result['X_val'].to_csv(ml_X_val_path, index=False)\n",
    "    result['X_test'].to_csv(ml_X_test_path, index=False)\n",
    "else:\n",
    "    np.savetxt(ml_X_train_path, result['X_train'], delimiter=',')\n",
    "    np.savetxt(ml_X_val_path, result['X_val'], delimiter=',')\n",
    "    np.savetxt(ml_X_test_path, result['X_test'], delimiter=',')\n",
    "    \n",
    "if isinstance(result['y_train'], pd.Series):\n",
    "    result['y_train'].to_csv(ml_y_train_path, index=False, header=True)\n",
    "    result['y_val'].to_csv(ml_y_val_path, index=False, header=True)\n",
    "    result['y_test'].to_csv(ml_y_test_path, index=False, header=True)\n",
    "else:\n",
    "    np.savetxt(ml_y_train_path, result['y_train'], delimiter=',')\n",
    "    np.savetxt(ml_y_val_path, result['y_val'], delimiter=',')\n",
    "    np.savetxt(ml_y_test_path, result['y_test'], delimiter=',')\n",
    "\n",
    "print(f\"Machine learning data saved to processed directory\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
