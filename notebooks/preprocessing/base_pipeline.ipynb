{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d38c1d5",
   "metadata": {},
   "source": [
    "# Flight Delay Prediction - Base Pipeline\n",
    "\n",
    "This notebook implements the base pipeline for the flight delay prediction project. It contains common preprocessing operations shared across all modeling approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48760f43",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for imports\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.data import processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87fe103",
   "metadata": {},
   "source": [
    "## Base Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25308df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePipeline:\n",
    "    \"\"\"Base preprocessing pipeline for flight delay prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"\n",
    "        Initialize the base pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        config : dict, optional\n",
    "            Configuration parameters for the pipeline.\n",
    "        \"\"\"\n",
    "        self.config = config if config is not None else {}\n",
    "        self.categorical_columns = [\n",
    "            'OP_CARRIER', 'ORIGIN', 'DEST', 'OP_CARRIER_FL_NUM', \n",
    "            'ORIGIN_CITY', 'DEST_CITY'\n",
    "        ]\n",
    "        self.numerical_columns = [\n",
    "            'DISTANCE', 'CRS_ELAPSED_TIME', 'CRS_DEP_TIME', \n",
    "            'CRS_ARR_TIME', 'CARRIER_DELAY', 'WEATHER_DELAY',\n",
    "            'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY'\n",
    "        ]\n",
    "        self.datetime_columns = ['FL_DATE', 'CRS_DEP_DATETIME', 'CRS_ARR_DATETIME']\n",
    "        self.target_column = 'DEP_DELAY'\n",
    "        \n",
    "    def load_data(self, data_path):\n",
    "        \"\"\"Load flight data from CSV file.\"\"\"\n",
    "        print(f\"Loading data from {data_path}\")\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Convert date columns to datetime\n",
    "        for col in self.datetime_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "                \n",
    "        return df\n",
    "    \n",
    "    def clean_data(self, df):\n",
    "        \"\"\"Clean the dataframe by removing invalid entries.\"\"\"\n",
    "        print(\"Cleaning data...\")\n",
    "        orig_len = len(df)\n",
    "        \n",
    "        # Remove duplicate flights\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # Remove cancelled flights (these won't have delay information)\n",
    "        if 'CANCELLED' in df.columns:\n",
    "            df = df[df['CANCELLED'] == 0]\n",
    "            \n",
    "        # Handle negative delays (early departures) - clip or keep depending on goal\n",
    "        if self.target_column in df.columns:\n",
    "            if self.config.get('clip_negative_delays', True):\n",
    "                df[self.target_column] = df[self.target_column].clip(lower=0)\n",
    "        \n",
    "        print(f\"Removed {orig_len - len(df)} invalid records\")\n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"Handle missing values in the dataframe.\"\"\"\n",
    "        print(\"Handling missing values...\")\n",
    "        \n",
    "        # For target variable, we can only drop if missing\n",
    "        if self.target_column in df.columns:\n",
    "            df = df.dropna(subset=[self.target_column])\n",
    "            \n",
    "        # For numerical columns, impute with median\n",
    "        for col in self.numerical_columns:\n",
    "            if col in df.columns and df[col].isna().sum() > 0:\n",
    "                median_value = df[col].median()\n",
    "                df[col] = df[col].fillna(median_value)\n",
    "                \n",
    "        # For categorical columns, impute with mode\n",
    "        for col in self.categorical_columns:\n",
    "            if col in df.columns and df[col].isna().sum() > 0:\n",
    "                mode_value = df[col].mode()[0]\n",
    "                df[col] = df[col].fillna(mode_value)\n",
    "                \n",
    "        return df\n",
    "    \n",
    "    def encode_categorical_variables(self, df, fit=True):\n",
    "        \"\"\"\n",
    "        Encode categorical variables using appropriate methods.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Input dataframe\n",
    "            \n",
    "        fit : bool, default=True\n",
    "            Whether to fit new encoders or use existing ones\n",
    "        \"\"\"\n",
    "        print(\"Encoding categorical variables...\")\n",
    "        \n",
    "        # High-cardinality categorical columns - use ordinal encoding to save memory\n",
    "        high_card_cols = ['ORIGIN', 'DEST', 'ORIGIN_CITY', 'DEST_CITY']\n",
    "        low_card_cols = [col for col in self.categorical_columns \n",
    "                         if col in df.columns and col not in high_card_cols]\n",
    "        \n",
    "        # Encode high-cardinality columns with ordinal encoding\n",
    "        if fit:\n",
    "            self.high_card_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', \n",
    "                                                   unknown_value=-1)\n",
    "            high_card_cols_present = [col for col in high_card_cols if col in df.columns]\n",
    "            if high_card_cols_present:\n",
    "                self.high_card_encoder.fit(df[high_card_cols_present])\n",
    "        \n",
    "        high_card_cols_present = [col for col in high_card_cols if col in df.columns]\n",
    "        if high_card_cols_present:\n",
    "            encoded_vals = self.high_card_encoder.transform(df[high_card_cols_present])\n",
    "            for i, col in enumerate(high_card_cols_present):\n",
    "                df[f\"{col}_encoded\"] = encoded_vals[:, i]\n",
    "        \n",
    "        # One-hot encode low-cardinality columns\n",
    "        if fit:\n",
    "            self.low_card_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "            low_card_cols_present = [col for col in low_card_cols if col in df.columns]\n",
    "            if low_card_cols_present:\n",
    "                self.low_card_encoder.fit(df[low_card_cols_present])\n",
    "        \n",
    "        low_card_cols_present = [col for col in low_card_cols if col in df.columns]\n",
    "        if low_card_cols_present:\n",
    "            encoded_vals = self.low_card_encoder.transform(df[low_card_cols_present])\n",
    "            encoded_df = pd.DataFrame(\n",
    "                encoded_vals, \n",
    "                columns=self.low_card_encoder.get_feature_names_out(low_card_cols_present),\n",
    "                index=df.index\n",
    "            )\n",
    "            df = pd.concat([df, encoded_df], axis=1)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def generate_basic_features(self, df):\n",
    "        \"\"\"Generate basic features common to all models.\"\"\"\n",
    "        print(\"Generating basic features...\")\n",
    "        \n",
    "        # Extract date components\n",
    "        if 'FL_DATE' in df.columns:\n",
    "            df['MONTH'] = df['FL_DATE'].dt.month\n",
    "            df['DAY'] = df['FL_DATE'].dt.day\n",
    "            df['DAY_OF_WEEK'] = df['FL_DATE'].dt.dayofweek\n",
    "        \n",
    "        # Time of day features from scheduled departure time\n",
    "        if 'CRS_DEP_TIME' in df.columns:\n",
    "            # Convert HHMM format to hour of day\n",
    "            df['dep_hour'] = df['CRS_DEP_TIME'] // 100\n",
    "            df['dep_minute'] = df['CRS_DEP_TIME'] % 100\n",
    "            \n",
    "        # Early morning, morning, afternoon, evening, night\n",
    "        if 'dep_hour' in df.columns:\n",
    "            df['time_of_day'] = pd.cut(\n",
    "                df['dep_hour'], \n",
    "                bins=[-1, 5, 10, 15, 20, 24], \n",
    "                labels=['early_morning', 'morning', 'afternoon', 'evening', 'night']\n",
    "            )\n",
    "            \n",
    "        # Delay binary target (for classification)\n",
    "        if self.target_column in df.columns:\n",
    "            df['is_delayed'] = (df[self.target_column] > 15).astype(int)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def split_data(self, df, test_size=0.2, validation_size=0.2, random_state=42):\n",
    "        \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
    "        print(\"Splitting data into train, validation, and test sets...\")\n",
    "        \n",
    "        # First split off test data\n",
    "        train_val, test = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Then split train data to get validation set\n",
    "        val_split = validation_size / (1 - test_size)\n",
    "        train, val = train_test_split(train_val, test_size=val_split, random_state=random_state)\n",
    "        \n",
    "        print(f\"Train: {len(train)} samples, Validation: {len(val)} samples, Test: {len(test)} samples\")\n",
    "        return train, val, test\n",
    "    \n",
    "    def run(self, data_path):\n",
    "        \"\"\"Run the complete base pipeline.\"\"\"\n",
    "        df = self.load_data(data_path)\n",
    "        df = self.clean_data(df)\n",
    "        df = self.handle_missing_values(df)\n",
    "        df = self.generate_basic_features(df)\n",
    "        df = self.encode_categorical_variables(df)\n",
    "        train, val, test = self.split_data(df)\n",
    "        \n",
    "        return {\n",
    "            'train': train,\n",
    "            'validation': val,\n",
    "            'test': test,\n",
    "            'full_data': df\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdc0c1",
   "metadata": {},
   "source": [
    "## Test Base Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266bd61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path to raw data\n",
    "file_path = os.path.join(PROJECT_ROOT, 'data', 'raw', 'flights_sample_3m.csv')\n",
    "\n",
    "# Create base pipeline instance\n",
    "base_pipeline = BasePipeline()\n",
    "\n",
    "# Load a sample of the data to test\n",
    "sample_df = pd.read_csv(file_path, nrows=10000)\n",
    "print(f\"Sample data shape: {sample_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each pipeline step individually\n",
    "clean_df = base_pipeline.clean_data(sample_df)\n",
    "print(f\"\\nAfter cleaning: {clean_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2cbeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "processed_df = base_pipeline.handle_missing_values(clean_df)\n",
    "print(f\"\\nAfter handling missing values: {processed_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features\n",
    "featured_df = base_pipeline.generate_basic_features(processed_df)\n",
    "print(\"\\nNew features added:\")\n",
    "new_cols = [col for col in featured_df.columns if col not in processed_df.columns]\n",
    "print(new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "encoded_df = base_pipeline.encode_categorical_variables(featured_df)\n",
    "print(f\"\\nAfter encoding: {encoded_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4317f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, val, test = base_pipeline.split_data(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd75071b",
   "metadata": {},
   "source": [
    "## Run Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5128ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency, run on a sample of the data\n",
    "sample_path = os.path.join(PROJECT_ROOT, 'data', 'raw', 'flights_sample_3m.csv')\n",
    "result = base_pipeline.run(sample_path)\n",
    "\n",
    "print(\"\\nPipeline execution complete!\")\n",
    "for key, df in result.items():\n",
    "    if key != 'full_data':\n",
    "        print(f\"{key} shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data to disk for future use\n",
    "train_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'base_train.csv')\n",
    "val_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'base_val.csv')\n",
    "test_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'base_test.csv')\n",
    "\n",
    "result['train'].to_csv(train_path, index=False)\n",
    "result['validation'].to_csv(val_path, index=False)\n",
    "result['test'].to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Data saved to processed directory\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
