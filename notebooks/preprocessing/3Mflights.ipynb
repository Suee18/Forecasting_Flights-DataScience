{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662e300b",
   "metadata": {},
   "source": [
    "# 3M dataset - Data Preprocessing\n",
    "\n",
    " cleansing 3M flight records for delay prediction modeling.\n",
    "\n",
    "## Contents\n",
    "1. [Data Loading](#loading)\n",
    "2. [Handling cancelled flihgts](#cancelled)\n",
    "3. [Save Processed Dataset](#save)\n",
    "4. [Handling Missing Values](#missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c1bf8",
   "metadata": {},
   "source": [
    "## 1. Data Loading <a id='loading'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5dbe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.data import loader, processor\n",
    "from src.visualization import exploratory_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70e27303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data file: c:\\Users\\HP\\Desktop\\Forecasting_Flights-DataScience\\data\\raw\\flights_sample_3m.csv\n",
      "Processed data directory: c:\\Users\\HP\\Desktop\\Forecasting_Flights-DataScience\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "raw_file_path = os.path.join(PROJECT_ROOT, 'data', 'raw', 'flights_sample_3m.csv')\n",
    "processed_dir = os.path.join(PROJECT_ROOT, 'data', 'processed')\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Input data file: {raw_file_path}\")\n",
    "print(f\"Processed data directory: {processed_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9cd3ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>AIRLINE_DOT</th>\n",
       "      <th>AIRLINE_CODE</th>\n",
       "      <th>DOT_CODE</th>\n",
       "      <th>FL_NUMBER</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>ORIGIN_CITY</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEST_CITY</th>\n",
       "      <th>...</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>DELAY_DUE_CARRIER</th>\n",
       "      <th>DELAY_DUE_WEATHER</th>\n",
       "      <th>DELAY_DUE_NAS</th>\n",
       "      <th>DELAY_DUE_SECURITY</th>\n",
       "      <th>DELAY_DUE_LATE_AIRCRAFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "      <td>United Air Lines Inc.: UA</td>\n",
       "      <td>UA</td>\n",
       "      <td>19977</td>\n",
       "      <td>1562</td>\n",
       "      <td>FLL</td>\n",
       "      <td>Fort Lauderdale, FL</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark, NJ</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>Delta Air Lines Inc.</td>\n",
       "      <td>Delta Air Lines Inc.: DL</td>\n",
       "      <td>DL</td>\n",
       "      <td>19790</td>\n",
       "      <td>1149</td>\n",
       "      <td>MSP</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>SEA</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "      <td>United Air Lines Inc.: UA</td>\n",
       "      <td>UA</td>\n",
       "      <td>19977</td>\n",
       "      <td>459</td>\n",
       "      <td>DEN</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>MSP</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>Delta Air Lines Inc.</td>\n",
       "      <td>Delta Air Lines Inc.: DL</td>\n",
       "      <td>DL</td>\n",
       "      <td>19790</td>\n",
       "      <td>2295</td>\n",
       "      <td>MSP</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>Spirit Air Lines</td>\n",
       "      <td>Spirit Air Lines: NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>20416</td>\n",
       "      <td>407</td>\n",
       "      <td>MCO</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>985.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FL_DATE                AIRLINE                AIRLINE_DOT AIRLINE_CODE  \\\n",
       "0  2019-01-09  United Air Lines Inc.  United Air Lines Inc.: UA           UA   \n",
       "1  2022-11-19   Delta Air Lines Inc.   Delta Air Lines Inc.: DL           DL   \n",
       "2  2022-07-22  United Air Lines Inc.  United Air Lines Inc.: UA           UA   \n",
       "3  2023-03-06   Delta Air Lines Inc.   Delta Air Lines Inc.: DL           DL   \n",
       "4  2020-02-23       Spirit Air Lines       Spirit Air Lines: NK           NK   \n",
       "\n",
       "   DOT_CODE  FL_NUMBER ORIGIN          ORIGIN_CITY DEST  \\\n",
       "0     19977       1562    FLL  Fort Lauderdale, FL  EWR   \n",
       "1     19790       1149    MSP      Minneapolis, MN  SEA   \n",
       "2     19977        459    DEN           Denver, CO  MSP   \n",
       "3     19790       2295    MSP      Minneapolis, MN  SFO   \n",
       "4     20416        407    MCO          Orlando, FL  DFW   \n",
       "\n",
       "               DEST_CITY  ...  DIVERTED  CRS_ELAPSED_TIME  ELAPSED_TIME  \\\n",
       "0             Newark, NJ  ...       0.0             186.0         176.0   \n",
       "1            Seattle, WA  ...       0.0             235.0         236.0   \n",
       "2        Minneapolis, MN  ...       0.0             118.0         112.0   \n",
       "3      San Francisco, CA  ...       0.0             260.0         285.0   \n",
       "4  Dallas/Fort Worth, TX  ...       0.0             181.0         182.0   \n",
       "\n",
       "   AIR_TIME  DISTANCE  DELAY_DUE_CARRIER  DELAY_DUE_WEATHER  DELAY_DUE_NAS  \\\n",
       "0     153.0    1065.0                NaN                NaN            NaN   \n",
       "1     189.0    1399.0                NaN                NaN            NaN   \n",
       "2      87.0     680.0                NaN                NaN            NaN   \n",
       "3     249.0    1589.0                0.0                0.0           24.0   \n",
       "4     153.0     985.0                NaN                NaN            NaN   \n",
       "\n",
       "   DELAY_DUE_SECURITY  DELAY_DUE_LATE_AIRCRAFT  \n",
       "0                 NaN                      NaN  \n",
       "1                 NaN                      NaN  \n",
       "2                 NaN                      NaN  \n",
       "3                 0.0                      0.0  \n",
       "4                 NaN                      NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to process data in chunks to handle 3M rows efficiently\n",
    "def process_data_in_chunks(file_path, chunksize=100000):\n",
    "    \"\"\"\n",
    "    Generator function to process the data in chunks\n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        chunksize: Number of rows to process at a time\n",
    "    Yields:\n",
    "        Processed DataFrame chunks\n",
    "    \"\"\"\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        yield chunk\n",
    "\n",
    "first_chunk = next(process_data_in_chunks(raw_file_path, chunksize=5))\n",
    "first_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93585f78",
   "metadata": {},
   "source": [
    "## 2. Handling Cancelled Flights <a id='cancelled'></a>\n",
    "\n",
    "As we're focusing on predicting delays for flights that actually operated, we'll separate cancelled flights from our analysis dataset. Cancelled flights represent a different phenomenon and would confuse our delay prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b1dfdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1,000,000 rows...\n",
      "Processed 2,000,000 rows...\n",
      "Processed 3,000,000 rows...\n",
      "Total Flights: 3,000,000\n",
      "Cancelled Flights: 79,140 (2.64%)\n",
      "\n",
      "Cancellation Codes:\n",
      "  B: 28,772 flights\n",
      "  D: 24,417 flights\n",
      "  A: 19,476 flights\n",
      "  C: 6,475 flights\n",
      "\n",
      "Cancellation Code Meanings:\n",
      "  A: Carrier\n",
      "  B: Weather\n",
      "  C: National Air System\n",
      "  D: Security\n"
     ]
    }
   ],
   "source": [
    "# First, let's analyze how many flights were cancelled\n",
    "def analyze_cancellations(file_path, chunksize=100000):\n",
    "    cancelled_count = 0\n",
    "    total_count = 0\n",
    "    cancellation_codes = {}\n",
    "    \n",
    "    # Process in chunks to handle the large dataset\n",
    "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize)):\n",
    "        total_count += len(chunk)\n",
    "        \n",
    "        # Count cancelled flights\n",
    "        cancelled_in_chunk = chunk[chunk['CANCELLED'] == 1.0]\n",
    "        cancelled_count += len(cancelled_in_chunk)\n",
    "        \n",
    "        # Track cancellation reasons\n",
    "        code_counts = cancelled_in_chunk['CANCELLATION_CODE'].value_counts()\n",
    "        for code, count in code_counts.items():\n",
    "            if code in cancellation_codes:\n",
    "                cancellation_codes[code] += count\n",
    "            else:\n",
    "                cancellation_codes[code] = count\n",
    "                \n",
    "        # Print progress\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Processed {(i+1)*chunksize:,} rows...\")\n",
    "    \n",
    "    # Calculate percentage\n",
    "    cancelled_pct = (cancelled_count / total_count) * 100 if total_count > 0 else 0\n",
    "    \n",
    "    # Create a summary dictionary\n",
    "    summary = {\n",
    "        'total_flights': total_count,\n",
    "        'cancelled_flights': cancelled_count,\n",
    "        'cancelled_percentage': cancelled_pct,\n",
    "        'cancellation_codes': cancellation_codes\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Run the analysis\n",
    "cancellation_summary = analyze_cancellations(raw_file_path)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Total Flights: {cancellation_summary['total_flights']:,}\")\n",
    "print(f\"Cancelled Flights: {cancellation_summary['cancelled_flights']:,} ({cancellation_summary['cancelled_percentage']:.2f}%)\")\n",
    "print(\"\\nCancellation Codes:\")\n",
    "for code, count in cancellation_summary['cancellation_codes'].items():\n",
    "    print(f\"  {code}: {count:,} flights\")\n",
    "    \n",
    "# Explanation of cancellation codes\n",
    "code_meanings = {\n",
    "    'A': 'Carrier',\n",
    "    'B': 'Weather',\n",
    "    'C': 'National Air System',\n",
    "    'D': 'Security'\n",
    "}\n",
    "\n",
    "print(\"\\nCancellation Code Meanings:\")\n",
    "for code, meaning in code_meanings.items():\n",
    "    print(f\"  {code}: {meaning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f2d3495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out cancelled flights...\n",
      "Processed 500,000 rows...\n",
      "Processed 1,000,000 rows...\n",
      "Processed 1,500,000 rows...\n",
      "Processed 2,000,000 rows...\n",
      "Processed 2,500,000 rows...\n",
      "Processed 3,000,000 rows...\n",
      "\n",
      "Filtering complete!\n",
      "Total rows processed: 3,000,000\n",
      "Operational flights: 2,920,860\n",
      "Cancelled flights removed: 79,140 (2.64%)\n",
      "\n",
      "Operational flights saved to: c:\\Users\\HP\\Desktop\\Forecasting_Flights-DataScience\\data\\processed\\operational_flights.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to filter out cancelled flights and save operational flights\n",
    "def filter_operational_flights(input_path, output_path, chunksize=100000):\n",
    "    \"\"\"\n",
    "    Filter the dataset to include only operational flights (not cancelled)\n",
    "    and save it to a new CSV file.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to the input CSV file\n",
    "        output_path: Path to save the filtered CSV file\n",
    "        chunksize: Number of rows to process at a time\n",
    "    \"\"\"\n",
    "    # Track statistics\n",
    "    total_rows = 0\n",
    "    operational_rows = 0\n",
    "    \n",
    "    # Process the first chunk to get the header\n",
    "    first_chunk = True\n",
    "    \n",
    "    # Process in chunks\n",
    "    for i, chunk in enumerate(pd.read_csv(input_path, chunksize=chunksize)):\n",
    "        # Update counts\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "        # Filter out cancelled flights\n",
    "        operational_chunk = chunk[chunk['CANCELLED'] != 1.0]\n",
    "        operational_rows += len(operational_chunk)\n",
    "        \n",
    "        # Save the chunk (append mode after first chunk)\n",
    "        if first_chunk:\n",
    "            operational_chunk.to_csv(output_path, index=False)\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            operational_chunk.to_csv(output_path, mode='a', header=False, index=False)\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f\"Processed {(i+1)*chunksize:,} rows...\")\n",
    "    \n",
    "    # Return statistics\n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'operational_rows': operational_rows,\n",
    "        'filtered_rows': total_rows - operational_rows,\n",
    "        'filtered_percentage': ((total_rows - operational_rows) / total_rows) * 100 if total_rows > 0 else 0\n",
    "    }\n",
    "\n",
    "operational_flights_path = os.path.join(processed_dir, 'operational_flights.csv')\n",
    "\n",
    "# Run the filtering process\n",
    "print(\"Filtering out cancelled flights...\")\n",
    "filter_stats = filter_operational_flights(raw_file_path, operational_flights_path)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nFiltering complete!\")\n",
    "print(f\"Total rows processed: {filter_stats['total_rows']:,}\")\n",
    "print(f\"Operational flights: {filter_stats['operational_rows']:,}\")\n",
    "print(f\"Cancelled flights removed: {filter_stats['filtered_rows']:,} ({filter_stats['filtered_percentage']:.2f}%)\")\n",
    "print(f\"\\nOperational flights saved to: {operational_flights_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166e241",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values <a id='missing_values'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255cf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def handle_missing_values_in_chunks(input_path, output_path, chunksize=100000, high_thresh=0.8):\n",
    "    \"\"\"\n",
    "    Cleans a large CSV file in chunks by:\n",
    "    - Dropping columns with missing values above a threshold (based on the first chunk).\n",
    "    - Filling or dropping remaining missing values.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to the input CSV file\n",
    "        output_path: Path to save the cleaned CSV file\n",
    "        chunksize: Number of rows to process at a time\n",
    "        high_thresh: Threshold for high missing values (default 0.8)\n",
    "    \"\"\"\n",
    "    first_chunk = True\n",
    "    total_rows = 0\n",
    "    total_kept_rows = 0\n",
    "    columns_to_drop = []\n",
    "\n",
    "    for i, chunk in enumerate(pd.read_csv(input_path, chunksize=chunksize)):\n",
    "        total_rows += len(chunk)\n",
    "\n",
    "        if first_chunk:\n",
    "            # Determine columns with high missing ratio\n",
    "            missing_ratio = chunk.isnull().mean()\n",
    "            columns_to_drop = missing_ratio[missing_ratio > high_thresh].index.tolist()\n",
    "            print(f\"Dropping columns with >{int(high_thresh*100)}% missing values: {columns_to_drop}\")\n",
    "            first_chunk = False\n",
    "\n",
    "        # Drop columns with high missing values\n",
    "        chunk = chunk.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "        # Fill delay reason columns if they still exist\n",
    "        delay_cols = ['DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY',\n",
    "                      'DELAY_DUE_CARRIER', 'DELAY_DUE_LATE_AIRCRAFT']\n",
    "        for col in delay_cols:\n",
    "            if col in chunk.columns:\n",
    "                chunk[col] = chunk[col].fillna(0)\n",
    "\n",
    "        # Drop rows with missing values in essential timing fields\n",
    "        critical_cols = ['DEP_TIME', 'ARR_TIME', 'AIR_TIME']\n",
    "        existing_critical_cols = [col for col in critical_cols if col in chunk.columns]\n",
    "        chunk = chunk.dropna(subset=existing_critical_cols)\n",
    "\n",
    "        total_kept_rows += len(chunk)\n",
    "\n",
    "        # Write chunk to file\n",
    "        if i == 0:\n",
    "            chunk.to_csv(output_path, index=False)\n",
    "        else:\n",
    "            chunk.to_csv(output_path, mode='a', header=False, index=False)\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"Processed {(i + 1) * chunksize:,} rows...\")\n",
    "\n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'kept_rows': total_kept_rows,\n",
    "        'dropped_rows': total_rows - total_kept_rows,\n",
    "        'dropped_percentage': ((total_rows - total_kept_rows) / total_rows) * 100 if total_rows > 0 else 0,\n",
    "        'dropped_columns': columns_to_drop\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0d1a05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values in chunks...\n",
      "Dropping columns with >80% missing values: ['CANCELLATION_CODE', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT']\n",
      "Processed 500,000 rows...\n",
      "Processed 1,000,000 rows...\n",
      "Processed 1,500,000 rows...\n",
      "Processed 2,000,000 rows...\n",
      "Processed 2,500,000 rows...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m cleaned_file_path = os.path.join(processed_dir, \u001b[33m'\u001b[39m\u001b[33mcleaned_operational_flights.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mHandling missing values in chunks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m clean_stats = \u001b[43mhandle_missing_values_in_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperational_flights_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcleaned_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCleaning complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal rows processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_stats[\u001b[33m'\u001b[39m\u001b[33mtotal_rows\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mhandle_missing_values_in_chunks\u001b[39m\u001b[34m(input_path, output_path, chunksize, high_thresh)\u001b[39m\n\u001b[32m     50\u001b[39m     chunk.to_csv(output_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ma\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % \u001b[32m5\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(i\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m)\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39mchunksize\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:3967\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3956\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3958\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3959\u001b[39m     frame=df,\n\u001b[32m   3960\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3964\u001b[39m     decimal=decimal,\n\u001b[32m   3965\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3972\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3984\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.filepath_or_buffer,\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[39m, in \u001b[36mCSVFormatter._save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._need_to_save_header:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_header()\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[39m, in \u001b[36mCSVFormatter._save_body\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_i >= end_i:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\formats\\csvs.py:324\u001b[39m, in \u001b[36mCSVFormatter._save_chunk\u001b[39m\u001b[34m(self, start_i, end_i)\u001b[39m\n\u001b[32m    321\u001b[39m data = \u001b[38;5;28mlist\u001b[39m(res._iter_column_arrays())\n\u001b[32m    323\u001b[39m ix = \u001b[38;5;28mself\u001b[39m.data_index[slicer]._get_values_for_csv(**\u001b[38;5;28mself\u001b[39m._number_format)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[43mlibwriters\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mwriters.pyx:73\u001b[39m, in \u001b[36mpandas._libs.writers.write_csv_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "cleaned_file_path = os.path.join(processed_dir, 'cleaned_operational_flights.csv')\n",
    "\n",
    "print(\"Handling missing values in chunks...\")\n",
    "clean_stats = handle_missing_values_in_chunks(operational_flights_path, cleaned_file_path, chunksize=100000)\n",
    "\n",
    "print(\"\\nCleaning complete!\")\n",
    "print(f\"Total rows processed: {clean_stats['total_rows']:,}\")\n",
    "print(f\"Rows kept: {clean_stats['kept_rows']:,}\")\n",
    "print(f\"Rows dropped: {clean_stats['dropped_rows']:,} ({clean_stats['dropped_percentage']:.2f}%)\")\n",
    "print(f\"Dropped columns: {clean_stats['dropped_columns']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1d0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
