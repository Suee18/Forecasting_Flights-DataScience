{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07160b13",
   "metadata": {},
   "source": [
    "# Flight Delay Prediction - Deep Learning Pipeline\n",
    "\n",
    "This notebook implements the deep learning pipeline for the flight delay prediction project. It includes specialized preprocessing operations for neural network modeling approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fcb683",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for imports\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# Import the BasePipeline from our base pipeline notebook\n",
    "%run \"base_pipeline.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a01c9bd",
   "metadata": {},
   "source": [
    "## Deep Learning Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59580b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearningPipeline(BasePipeline):\n",
    "    \"\"\"Deep learning preprocessing pipeline for flight delay prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"\n",
    "        Initialize the deep learning pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        config : dict, optional\n",
    "            Configuration parameters for the pipeline.\n",
    "            \n",
    "        Additional Parameters:\n",
    "        ---------------------\n",
    "        batch_size : int\n",
    "            Batch size for mini-batch training\n",
    "        sequence_length : int\n",
    "            Length of sequences for recurrent models\n",
    "        embedding_dims : dict\n",
    "            Dictionary mapping categorical columns to embedding dimensions\n",
    "        scaler_type : str\n",
    "            Type of scaling to apply ('standard', 'minmax')\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Default config\n",
    "        default_config = {\n",
    "            'batch_size': 64,\n",
    "            'sequence_length': 10,  # For sequential models\n",
    "            'embedding_dims': {\n",
    "                'OP_CARRIER': 4,\n",
    "                'ORIGIN': 8,\n",
    "                'DEST': 8\n",
    "            },\n",
    "            'scaler_type': 'standard',  # 'standard' or 'minmax'\n",
    "            'create_sequences': False,  # Whether to create sequences for RNNs\n",
    "            'categorical_embed_method': 'embedding'  # 'embedding' or 'onehot'\n",
    "        }\n",
    "        \n",
    "        # Update with user config\n",
    "        if config is not None:\n",
    "            default_config.update(config)\n",
    "            \n",
    "        self.config = default_config\n",
    "        \n",
    "    def normalize_inputs(self, df, fit=True):\n",
    "        \"\"\"\n",
    "        Normalize numerical inputs for neural networks.\n",
    "        \n",
    "        Neural networks generally train better with normalized inputs.\n",
    "        \"\"\"\n",
    "        print(\"Normalizing inputs...\")\n",
    "        \n",
    "        # Get numeric columns\n",
    "        numeric_cols = [col for col in df.columns if \n",
    "                       col in self.numerical_columns or \n",
    "                       (col.startswith(self.target_column) and col != self.target_column) or\n",
    "                       col.startswith('roll_') or\n",
    "                       col.startswith('lag_')]\n",
    "        \n",
    "        # Add target to be normalized\n",
    "        if self.target_column in df.columns:\n",
    "            numeric_cols.append(self.target_column)\n",
    "        \n",
    "        # Create scaler if fitting\n",
    "        if fit:\n",
    "            if self.config['scaler_type'] == 'standard':\n",
    "                self.scaler = StandardScaler()\n",
    "            else:\n",
    "                self.scaler = MinMaxScaler()\n",
    "            \n",
    "            # Fit scaler\n",
    "            self.scaler.fit(df[numeric_cols])\n",
    "            \n",
    "        # Transform data\n",
    "        normalized_data = self.scaler.transform(df[numeric_cols])\n",
    "        \n",
    "        # Replace original columns with normalized values\n",
    "        for i, col in enumerate(numeric_cols):\n",
    "            df[col] = normalized_data[:, i]\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def create_embeddings(self, df):\n",
    "        \"\"\"\n",
    "        Prepare categorical variables for embedding layers.\n",
    "        \n",
    "        For deep learning, we need to convert categories to integer indices\n",
    "        which will be inputs to embedding layers.\n",
    "        \"\"\"\n",
    "        print(\"Preparing embeddings for categorical variables...\")\n",
    "        \n",
    "        # Get embedding configuration\n",
    "        embedding_dims = self.config.get('embedding_dims', {})\n",
    "        categorical_columns = list(embedding_dims.keys())\n",
    "        \n",
    "        # Only process columns that exist in the data\n",
    "        for col in categorical_columns:\n",
    "            if col not in df.columns:\n",
    "                print(f\"Warning: Embedding column {col} not found in data\")\n",
    "                continue\n",
    "                \n",
    "            # Create category mapping if not already created\n",
    "            if not hasattr(self, f'{col}_mapping'):\n",
    "                # Get unique categories and assign indices\n",
    "                categories = df[col].unique()\n",
    "                mapping = {cat: idx for idx, cat in enumerate(categories)}\n",
    "                setattr(self, f'{col}_mapping', mapping)\n",
    "                \n",
    "                # Store vocab size for embedding layer configuration\n",
    "                setattr(self, f'{col}_vocab_size', len(mapping) + 1)  # +1 for unknown\n",
    "                \n",
    "            # Apply mapping\n",
    "            mapping = getattr(self, f'{col}_mapping')\n",
    "            df[f'{col}_idx'] = df[col].map(mapping).fillna(len(mapping)).astype(int)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def sequence_preparation(self, df):\n",
    "        \"\"\"\n",
    "        Prepare sequential data for RNNs, LSTMs or Transformer models.\n",
    "        \n",
    "        Creates sequences of data points for each group (e.g. airport, route).\n",
    "        \"\"\"\n",
    "        if not self.config.get('create_sequences', False):\n",
    "            return df\n",
    "            \n",
    "        print(\"Preparing sequences for recurrent models...\")\n",
    "        \n",
    "        ts_col = self.config.get('timestamp_col', 'FL_DATE')\n",
    "        seq_len = self.config.get('sequence_length', 10)\n",
    "        \n",
    "        # Sort by time\n",
    "        df = df.sort_values(ts_col)\n",
    "        \n",
    "        # Define features to include in sequences\n",
    "        feature_cols = [col for col in df.columns if \n",
    "                       col not in [ts_col, self.target_column] and\n",
    "                       not col.endswith('_idx')]\n",
    "        \n",
    "        # Add embedding indices\n",
    "        feature_cols.extend([col for col in df.columns if col.endswith('_idx')])\n",
    "        \n",
    "        # Create sequences\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        \n",
    "        # Get group columns from config\n",
    "        group_cols = self.config.get('group_cols', [])\n",
    "        \n",
    "        if group_cols:\n",
    "            # Create sequences for each group\n",
    "            for _, group in df.groupby(group_cols):\n",
    "                # Skip groups with too few samples\n",
    "                if len(group) < seq_len + 1:\n",
    "                    continue\n",
    "                    \n",
    "                # Extract features and target\n",
    "                features = group[feature_cols].values\n",
    "                target = group[self.target_column].values\n",
    "                \n",
    "                # Create sequences\n",
    "                for i in range(len(group) - seq_len):\n",
    "                    sequences.append(features[i:i+seq_len])\n",
    "                    targets.append(target[i+seq_len])\n",
    "        else:\n",
    "            # Create sequences without grouping\n",
    "            features = df[feature_cols].values\n",
    "            target = df[self.target_column].values\n",
    "            \n",
    "            for i in range(len(df) - seq_len):\n",
    "                sequences.append(features[i:i+seq_len])\n",
    "                targets.append(target[i+seq_len])\n",
    "                \n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(sequences)\n",
    "        y = np.array(targets)\n",
    "        \n",
    "        print(f\"Created {len(X)} sequences with shape {X.shape}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def batch_preparation(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Prepare data batches for training or inference.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Input features or sequences\n",
    "            \n",
    "        y : array-like, optional\n",
    "            Target variable\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dataset : tf.data.Dataset or similar\n",
    "            Dataset ready for neural network training\n",
    "        \"\"\"\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            print(\"Warning: batch_preparation expects numpy arrays, skipping\")\n",
    "            return X, y\n",
    "            \n",
    "        print(\"Preparing batches for training...\")\n",
    "        \n",
    "        batch_size = self.config.get('batch_size', 64)\n",
    "        \n",
    "        # This implementation depends on the deep learning framework being used\n",
    "        # Here we'll just return the arrays with a note about batching\n",
    "        print(f\"Data ready for batching with batch_size={batch_size}\")\n",
    "        \n",
    "        # If using TensorFlow:\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "            dataset = dataset.batch(batch_size)\n",
    "            dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "            print(\"Created TensorFlow dataset\")\n",
    "            return dataset\n",
    "        except:\n",
    "            print(\"TensorFlow not available, returning numpy arrays\")\n",
    "        \n",
    "        # If using PyTorch:\n",
    "        try:\n",
    "            from torch.utils.data import DataLoader, TensorDataset\n",
    "            import torch\n",
    "            tensor_x = torch.Tensor(X)\n",
    "            tensor_y = torch.Tensor(y)\n",
    "            dataset = TensorDataset(tensor_x, tensor_y)\n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            print(\"Created PyTorch dataloader\")\n",
    "            return dataloader\n",
    "        except:\n",
    "            print(\"PyTorch not available, returning numpy arrays\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def run(self, data_path):\n",
    "        \"\"\"Run the complete deep learning pipeline.\"\"\"\n",
    "        # Run base pipeline steps first\n",
    "        df = self.load_data(data_path)\n",
    "        df = self.clean_data(df)\n",
    "        df = self.handle_missing_values(df)\n",
    "        df = self.generate_basic_features(df)\n",
    "        \n",
    "        # For deep learning, we might want to use embeddings instead of one-hot encoding\n",
    "        if self.config.get('categorical_embed_method', 'embedding') == 'embedding':\n",
    "            df = self.create_embeddings(df)\n",
    "        else:\n",
    "            df = self.encode_categorical_variables(df)\n",
    "            \n",
    "        # Normalize inputs\n",
    "        df = self.normalize_inputs(df)\n",
    "        \n",
    "        # Split the data\n",
    "        train_df, val_df, test_df = self.split_data(df)\n",
    "        \n",
    "        # Create sequences if using RNN/LSTM\n",
    "        if self.config.get('create_sequences', False):\n",
    "            X_train, y_train = self.sequence_preparation(train_df)\n",
    "            X_val, y_val = self.sequence_preparation(val_df)\n",
    "            X_test, y_test = self.sequence_preparation(test_df)\n",
    "            \n",
    "            # Prepare batches\n",
    "            train_batches = self.batch_preparation(X_train, y_train)\n",
    "            val_batches = self.batch_preparation(X_val, y_val)\n",
    "            test_batches = self.batch_preparation(X_test, y_test)\n",
    "            \n",
    "            return {\n",
    "                'train': train_batches,\n",
    "                'validation': val_batches,\n",
    "                'test': test_batches,\n",
    "                'train_df': train_df,\n",
    "                'val_df': val_df,\n",
    "                'test_df': test_df,\n",
    "                'full_data': df\n",
    "            }\n",
    "        else:\n",
    "            # For non-sequential models\n",
    "            return {\n",
    "                'train': train_df,\n",
    "                'validation': val_df,\n",
    "                'test': test_df, \n",
    "                'full_data': df\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c7b78",
   "metadata": {},
   "source": [
    "## Test Deep Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path to raw data\n",
    "file_path = os.path.join(PROJECT_ROOT, 'data', 'raw', 'flights_sample_3m.csv')\n",
    "\n",
    "# Configure the deep learning pipeline\n",
    "dl_config = {\n",
    "    'batch_size': 64,\n",
    "    'embedding_dims': {\n",
    "        'OP_CARRIER': 4,\n",
    "        'ORIGIN': 8,\n",
    "        'DEST': 8\n",
    "    },\n",
    "    'scaler_type': 'standard',\n",
    "    'create_sequences': False,  # Start with non-sequential data\n",
    "    'categorical_embed_method': 'embedding'\n",
    "}\n",
    "\n",
    "# Create deep learning pipeline instance\n",
    "dl_pipeline = DeepLearningPipeline(config=dl_config)\n",
    "\n",
    "# Load a sample of the data to test\n",
    "sample_df = pd.read_csv(file_path, nrows=10000)\n",
    "print(f\"Sample data shape: {sample_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deep learning specific preprocessing steps\n",
    "# Start with a cleaned dataframe with basic features\n",
    "prepared_df = dl_pipeline.clean_data(sample_df)\n",
    "prepared_df = dl_pipeline.handle_missing_values(prepared_df)\n",
    "prepared_df = dl_pipeline.generate_basic_features(prepared_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings \n",
    "embedded_df = dl_pipeline.create_embeddings(prepared_df)\n",
    "print(\"\\nEmbedding indices created:\")\n",
    "idx_cols = [col for col in embedded_df.columns if col.endswith('_idx')]\n",
    "print(idx_cols)\n",
    "\n",
    "# Check category mappings\n",
    "print(\"\\nCategory mappings (first 5 items):\")\n",
    "for col in dl_config['embedding_dims'].keys():\n",
    "    if hasattr(dl_pipeline, f'{col}_mapping'):\n",
    "        mapping = getattr(dl_pipeline, f'{col}_mapping')\n",
    "        print(f\"{col}: {list(mapping.items())[:5]}...\")\n",
    "        print(f\"Vocabulary size: {getattr(dl_pipeline, f'{col}_vocab_size')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fac7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize inputs\n",
    "normalized_df = dl_pipeline.normalize_inputs(embedded_df)\n",
    "\n",
    "# Check the normalization results\n",
    "numerical_cols = dl_pipeline.numerical_columns\n",
    "present_cols = [col for col in numerical_cols if col in normalized_df.columns]\n",
    "\n",
    "if present_cols:\n",
    "    print(\"\\nNormalized numeric columns statistics:\")\n",
    "    print(normalized_df[present_cols[:3]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sequence preparation (enable sequences in config)\n",
    "dl_config_seq = dl_config.copy()\n",
    "dl_config_seq['create_sequences'] = True\n",
    "dl_config_seq['sequence_length'] = 5\n",
    "dl_config_seq['group_cols'] = ['ORIGIN']\n",
    "dl_config_seq['timestamp_col'] = 'FL_DATE'\n",
    "\n",
    "dl_seq_pipeline = DeepLearningPipeline(config=dl_config_seq)\n",
    "\n",
    "# Apply previous transformations\n",
    "seq_prepared_df = dl_seq_pipeline.clean_data(sample_df)\n",
    "seq_prepared_df = dl_seq_pipeline.handle_missing_values(seq_prepared_df)\n",
    "seq_prepared_df = dl_seq_pipeline.generate_basic_features(seq_prepared_df)\n",
    "seq_prepared_df = dl_seq_pipeline.create_embeddings(seq_prepared_df)\n",
    "seq_prepared_df = dl_seq_pipeline.normalize_inputs(seq_prepared_df)\n",
    "\n",
    "# Create sequences\n",
    "X_seq, y_seq = dl_seq_pipeline.sequence_preparation(seq_prepared_df)\n",
    "print(f\"\\nSequence X shape: {X_seq.shape}, y shape: {y_seq.shape}\")\n",
    "print(f\"Each sequence contains {X_seq.shape[1]} time steps and {X_seq.shape[2]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6589c4d",
   "metadata": {},
   "source": [
    "## Run Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b3da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency, run on a sample of the data\n",
    "sample_path = os.path.join(PROJECT_ROOT, 'data', 'raw', 'flights_sample_3m.csv')\n",
    "\n",
    "# First test: Feed-forward network approach (no sequences)\n",
    "result = dl_pipeline.run(sample_path)\n",
    "\n",
    "print(\"\\nFeed-forward pipeline execution complete!\")\n",
    "for key, df in result.items():\n",
    "    if key != 'full_data' and isinstance(df, pd.DataFrame):\n",
    "        print(f\"{key} shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second test: Sequence-based approach (RNN/LSTM)\n",
    "seq_result = dl_seq_pipeline.run(sample_path)\n",
    "\n",
    "print(\"\\nSequence-based pipeline execution complete!\")\n",
    "for key, data in seq_result.items():\n",
    "    if key not in ['full_data', 'train_df', 'val_df', 'test_df']:\n",
    "        if isinstance(data, tuple) and len(data) == 2:\n",
    "            X, y = data\n",
    "            print(f\"{key} shapes: X={X.shape}, y={y.shape}\")\n",
    "        elif hasattr(data, 'shape'):\n",
    "            print(f\"{key} shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0954fd",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data to disk for future use\n",
    "dl_train_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'dl_train.csv')\n",
    "dl_val_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'dl_val.csv')\n",
    "dl_test_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'dl_test.csv')\n",
    "\n",
    "# Save the feed-forward approach data\n",
    "result['train'].to_csv(dl_train_path, index=False)\n",
    "result['validation'].to_csv(dl_val_path, index=False)\n",
    "result['test'].to_csv(dl_test_path, index=False)\n",
    "\n",
    "print(f\"Deep learning data saved to processed directory\")\n",
    "\n",
    "# For sequence data, we need to use numpy's save function\n",
    "if isinstance(seq_result['train'], tuple):\n",
    "    X_train, y_train = seq_result['train']\n",
    "    X_val, y_val = seq_result['validation']\n",
    "    X_test, y_test = seq_result['test']\n",
    "    \n",
    "    # Save as numpy arrays\n",
    "    np.save(os.path.join(PROJECT_ROOT, 'data', 'processed', 'dl_X_train_seq.npy'), X_train)\n",
    "    np.save(os.path.join(PROJECT_ROOT, 'data', 'processed', 'dl_y_train_seq.npy'), y_train)\n",
    "    np.save(os.path.join(PROJECT_ROOT, 'data', 'processed', 'dl_X_val_seq.npy'), X_val)\n",
    "    np.save(os.path.join(PROJECT_ROOT, 'data', 'processed', 'dl_y_val_seq.npy'), y_val)\n",
    "    np.save(os.path.join(PROJECT_ROOT, 'data', 'processed', 'dl_X_test_seq.npy'), X_test)\n",
    "    np.save(os.path.join(PROJECT_ROOT, 'data', 'processed', 'dl_y_test_seq.npy'), y_test)\n",
    "    \n",
    "    print(\"Sequence data saved as numpy arrays\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
