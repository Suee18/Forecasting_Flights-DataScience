{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0dd7c27",
   "metadata": {},
   "source": [
    "# Flight Delay Prediction - Preprocessing Strategy\n",
    "\n",
    "This notebook outlines the different preprocessing approaches needed for our three modeling tracks:\n",
    "1. Time Series Models\n",
    "2. Deep Learning Models\n",
    "3. Traditional Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2186ad",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Add src directory to path for importing custom modules\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.data import loader, processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411bdf9",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the flights data\n",
    "file_path = os.path.join(PROJECT_ROOT, 'data', 'raw', 'flights_sample_3m.csv')\n",
    "flights_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset dimensions: {flights_df.shape[0]} rows Ã— {flights_df.shape[1]} columns\")\n",
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f7c08",
   "metadata": {},
   "source": [
    "## Common Preprocessing Steps for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_preprocessing(df):\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps common to all modeling approaches\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle missing values - either drop or impute based on column\n",
    "    df = processor.handle_missing_values(df)\n",
    "    \n",
    "    # Convert date features\n",
    "    df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])\n",
    "    \n",
    "    # Feature extraction from date\n",
    "    df['MONTH'] = df['FL_DATE'].dt.month\n",
    "    df['DAY'] = df['FL_DATE'].dt.day\n",
    "    df['DAY_OF_WEEK'] = df['FL_DATE'].dt.dayofweek\n",
    "    df['HOUR'] = df['DEP_TIME'] // 100\n",
    "    df['MINUTE'] = df['DEP_TIME'] % 100\n",
    "    \n",
    "    # Target variable: clip negative delays to 0 (early departures considered on-time)\n",
    "    df['DEP_DELAY'] = df['DEP_DELAY'].clip(lower=0)\n",
    "    \n",
    "    # Create a binary target for classification: 1 if delay > 15 min else 0\n",
    "    df['DELAYED_FLAG'] = (df['DEP_DELAY'] > 15).astype(int)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    df = processor.encode_categorical_columns(df)\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    important_cols = ['FL_DATE', 'ORIGIN', 'DEST', 'CARRIER', 'DEP_TIME', 'ARR_TIME', \n",
    "                    'DISTANCE', 'WEATHER_DELAY', 'DEP_DELAY', 'DELAYED_FLAG', \n",
    "                    'MONTH', 'DAY', 'DAY_OF_WEEK', 'HOUR', 'MINUTE']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daaa85c",
   "metadata": {},
   "source": [
    "## Time Series Specific Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_preprocessing(df, airport=None, resample_freq='1H'):\n",
    "    \"\"\"\n",
    "    Preprocess data specifically for time series models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input data\n",
    "    airport : str, optional\n",
    "        If provided, filter data for a specific airport\n",
    "    resample_freq : str\n",
    "        Frequency to resample the time series data\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Apply common preprocessing\n",
    "    df = common_preprocessing(df)\n",
    "    \n",
    "    # Filter for specific airport if required\n",
    "    if airport:\n",
    "        df = df[df['ORIGIN'] == airport]\n",
    "    \n",
    "    # Create datetime index for time series analysis\n",
    "    df['DATETIME'] = pd.to_datetime(df['FL_DATE'].dt.date.astype(str) + ' ' + \n",
    "                                  df['HOUR'].astype(str).str.zfill(2) + ':' + \n",
    "                                  df['MINUTE'].astype(str).str.zfill(2))\n",
    "    df = df.set_index('DATETIME')\n",
    "    \n",
    "    # Aggregate data by time periods\n",
    "    ts_data = df.resample(resample_freq).agg({\n",
    "        'DEP_DELAY': 'mean',\n",
    "        'DELAYED_FLAG': 'mean',  # Percentage of delayed flights\n",
    "        'CARRIER': 'count'      # Number of flights\n",
    "    }).rename(columns={'CARRIER': 'FLIGHT_COUNT'})\n",
    "    \n",
    "    # Fill missing time periods with forward fill then backward fill\n",
    "    ts_data = ts_data.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Add time features\n",
    "    ts_data['HOUR'] = ts_data.index.hour\n",
    "    ts_data['DAY'] = ts_data.index.day\n",
    "    ts_data['MONTH'] = ts_data.index.month\n",
    "    ts_data['DAY_OF_WEEK'] = ts_data.index.dayofweek\n",
    "    \n",
    "    # Create lag features\n",
    "    for lag in [1, 3, 6, 12, 24]:  # Various lag periods\n",
    "        ts_data[f'DEP_DELAY_LAG_{lag}'] = ts_data['DEP_DELAY'].shift(lag)\n",
    "        \n",
    "    # Create rolling window features\n",
    "    for window in [3, 6, 12, 24]:\n",
    "        ts_data[f'DEP_DELAY_ROLLING_MEAN_{window}'] = ts_data['DEP_DELAY'].rolling(window=window).mean()\n",
    "        ts_data[f'DEP_DELAY_ROLLING_STD_{window}'] = ts_data['DEP_DELAY'].rolling(window=window).std()\n",
    "    \n",
    "    # Drop rows with NaN values from lag features\n",
    "    ts_data = ts_data.dropna()\n",
    "    \n",
    "    return ts_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76383a",
   "metadata": {},
   "source": [
    "## Deep Learning Specific Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82373f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_learning_preprocessing(df):\n",
    "    \"\"\"\n",
    "    Preprocess data specifically for deep learning models\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Apply common preprocessing\n",
    "    df = common_preprocessing(df)\n",
    "    \n",
    "    # Normalize numerical features for deep learning\n",
    "    num_cols = ['DISTANCE', 'DEP_TIME', 'ARR_TIME']\n",
    "    df_dl = processor.normalize_numerical_features(df, num_cols)\n",
    "    \n",
    "    # Create embeddings for high-cardinality categorical variables\n",
    "    # This would be handled during model creation, but we need to prepare the data\n",
    "    categorical_cols = ['ORIGIN', 'DEST', 'CARRIER']\n",
    "    \n",
    "    # Get the mapping dictionaries for each categorical variable\n",
    "    mappings = {}\n",
    "    for col in categorical_cols:\n",
    "        df_dl[f'{col}_ID'] = pd.factorize(df_dl[col])[0]\n",
    "        # Store mapping for later use in embeddings\n",
    "        unique_vals = df_dl[col].unique()\n",
    "        mappings[col] = {val: i for i, val in enumerate(unique_vals)}\n",
    "        \n",
    "    # Deep learning often works better with sequences\n",
    "    # Create time-ordered sequences per airport\n",
    "    # (Implementation would depend on the specific DL approach)\n",
    "    \n",
    "    return df_dl, mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dafa25a",
   "metadata": {},
   "source": [
    "## Traditional Machine Learning Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_preprocessing(df):\n",
    "    \"\"\"\n",
    "    Preprocess data specifically for traditional machine learning models\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Apply common preprocessing\n",
    "    df = common_preprocessing(df)\n",
    "    \n",
    "    # Handle categorical variables - one-hot encoding\n",
    "    cat_cols = ['ORIGIN', 'DEST', 'CARRIER', 'DAY_OF_WEEK']\n",
    "    df_ml = processor.one_hot_encode_columns(df, cat_cols)\n",
    "    \n",
    "    # Feature engineering specific to ML models\n",
    "    # Interactions between variables\n",
    "    df_ml['HOUR_X_DAY_OF_WEEK'] = df_ml['HOUR'] * df_ml['DAY_OF_WEEK']\n",
    "    \n",
    "    # Distance buckets might perform better than raw distance\n",
    "    df_ml['DISTANCE_BUCKET'] = pd.cut(\n",
    "        df_ml['DISTANCE'], \n",
    "        bins=[0, 500, 1000, 1500, 2000, 3000, float('inf')], \n",
    "        labels=[0, 1, 2, 3, 4, 5]\n",
    "    )\n",
    "    \n",
    "    # Drop high-cardinality features after encoding to avoid dimensionality explosion\n",
    "    df_ml = df_ml.drop(['FL_DATE'], axis=1)\n",
    "    \n",
    "    return df_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f024cc",
   "metadata": {},
   "source": [
    "## Example: Preparing Data for Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de95d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of preparing small subset of data for each approach\n",
    "sample_df = flights_df.head(10000)\n",
    "\n",
    "# Time Series Preprocessing for a specific airport (e.g., ATL)\n",
    "ts_data = time_series_preprocessing(sample_df, airport='ATL')\n",
    "print(\"\\nTime Series Data Sample:\")\n",
    "print(ts_data.head())\n",
    "\n",
    "# Deep Learning Preprocessing\n",
    "dl_data, dl_mappings = deep_learning_preprocessing(sample_df)\n",
    "print(\"\\nDeep Learning Data Sample:\")\n",
    "print(dl_data.head())\n",
    "print(f\"Mapping sample (ORIGIN): {list(dl_mappings['ORIGIN'].items())[:5]}\")\n",
    "\n",
    "# Machine Learning Preprocessing\n",
    "ml_data = ml_preprocessing(sample_df)\n",
    "print(\"\\nMachine Learning Data Sample:\")\n",
    "print(ml_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a300b91",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Implement the custom preprocessing functions in `src/data/processor.py`\n",
    "2. Create dedicated preprocessing notebooks for each modeling approach\n",
    "3. Generate and save preprocessed datasets for each modeling approach"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
