{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35379dd1",
   "metadata": {},
   "source": [
    "# SARIMA Time Series Model for Flight Delay Forecasting\n",
    "\n",
    "This notebook implements a Seasonal Autoregressive Integrated Moving Average (SARIMA) model for forecasting flight delays using the preprocessed time series data. The SARIMA model is particularly suitable for this task as flight delay data typically exhibits both trend and seasonality components.\n",
    "\n",
    "## Key Steps:\n",
    "1. Loading the preprocessed time series data\n",
    "2. Model parameter selection using ACF/PACF analysis and AIC/BIC\n",
    "3. Model fitting with SARIMA\n",
    "4. Model evaluation and diagnostics\n",
    "5. Forecasting future flight delays\n",
    "6. Visualizing and analyzing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055ac74e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msm\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtsa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstatespace\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msarimax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SARIMAX\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraphics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtsaplots\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_acf, plot_pacf\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/statsmodels/api.py:125\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenmod\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m genmod\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenmod\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    116\u001b[39m     GEE,\n\u001b[32m    117\u001b[39m     GLM,\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m     families,\n\u001b[32m    124\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraphics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m graphics\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraphics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgofplots\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProbPlot, qqline, qqplot, qqplot_2samples\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimputation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbayes_mi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MI, BayesGaussMI\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/statsmodels/graphics/api.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgofplots\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m qqplot\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplottools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rainbow\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregressionplots\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     abline_plot,\n\u001b[32m     11\u001b[39m     influence_plot,\n\u001b[32m     12\u001b[39m     plot_ccpr,\n\u001b[32m     13\u001b[39m     plot_ccpr_grid,\n\u001b[32m     14\u001b[39m     plot_fit,\n\u001b[32m     15\u001b[39m     plot_leverage_resid2,\n\u001b[32m     16\u001b[39m     plot_partregress,\n\u001b[32m     17\u001b[39m     plot_partregress_grid,\n\u001b[32m     18\u001b[39m     plot_regress_exog,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m __all__ = [\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mabline_plot\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbeanplot\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mviolinplot\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/statsmodels/graphics/regressionplots.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenmod\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneralized_linear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GLM\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraphics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnonparametric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msmoothers_lowess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lowess\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregression\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GLS, OLS, WLS\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msandbox\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregression\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpredstd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wls_prediction_std\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/statsmodels/nonparametric/smoothers_lowess.py:11\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"Lowess - wrapper for cythonized extension\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[33;03mAuthor : Chris Jordan-Squire\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_smoothers_lowess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lowess \u001b[38;5;28;01mas\u001b[39;00m _lowess\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlowess\u001b[39m(endog, exog, frac=\u001b[32m2.0\u001b[39m/\u001b[32m3.0\u001b[39m, it=\u001b[32m3\u001b[39m, delta=\u001b[32m0.0\u001b[39m, xvals=\u001b[38;5;28;01mNone\u001b[39;00m, is_sorted=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     14\u001b[39m            missing=\u001b[33m'\u001b[39m\u001b[33mdrop\u001b[39m\u001b[33m'\u001b[39m, return_sorted=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''LOWESS (Locally Weighted Scatterplot Smoothing)\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03m    A lowess function that outs smoothed estimates of endog\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \n\u001b[32m    136\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstatsmodels/nonparametric/_smoothers_lowess.pyx:1\u001b[39m, in \u001b[36minit statsmodels.nonparametric._smoothers_lowess\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure paths dynamically using relative paths\n",
    "import os.path as path\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "notebook_dir = path.dirname(path.abspath('__file__'))\n",
    "# Get project root (parent of notebooks directory)\n",
    "project_root = path.abspath(path.join(notebook_dir, '..', '..'))\n",
    "\n",
    "# Define paths relative to project root\n",
    "TS_PROCESSED_PATH = path.join(project_root, 'data', 'processed', 'ts_ready_flights')\n",
    "TS_MODEL_PATH = path.join(project_root, 'models', 'time_series')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(TS_MODEL_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Time Series processed data path: {TS_PROCESSED_PATH}\")\n",
    "print(f\"Time Series model path: {TS_MODEL_PATH}\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Libraries and paths configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed time series data\n",
    "def load_ts_data(path, format='csv'):\n",
    "    \"\"\"\n",
    "    Load time series data from CSV or pickle file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    path : str\n",
    "        Base path to the data file\n",
    "    format : str\n",
    "        File format ('csv' or 'pkl')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Time series data with DatetimeIndex\n",
    "    \"\"\"\n",
    "    file_path = f\"{path}.{format}\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    if format == 'csv':\n",
    "        # Load CSV with date index\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df = df.set_index('date')\n",
    "        elif 'index' in df.columns:\n",
    "            df['index'] = pd.to_datetime(df['index'])\n",
    "            df = df.set_index('index')\n",
    "    elif format == 'pkl':\n",
    "        # Load pickle file\n",
    "        df = pd.read_pickle(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {format}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Try to load the daily time series data\n",
    "try:\n",
    "    daily_ts_path = os.path.join(TS_PROCESSED_PATH, \"daily_delay_ts\")\n",
    "    ts_daily = load_ts_data(daily_ts_path, format='csv')\n",
    "    print(f\"Successfully loaded daily time series data: {ts_daily.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading daily time series data: {e}\")\n",
    "    print(\"Trying to load model-ready ARIMA data instead...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to load the ARIMA data directly\n",
    "        with open(os.path.join(TS_PROCESSED_PATH, 'model_ready', 'arima_data.pkl'), 'rb') as f:\n",
    "            arima_data = pickle.load(f)\n",
    "            \n",
    "        with open(os.path.join(TS_PROCESSED_PATH, 'model_ready', 'arima_exog.pkl'), 'rb') as f:\n",
    "            arima_exog = pickle.load(f)\n",
    "            \n",
    "        print(\"Successfully loaded ARIMA data from pickle files\")\n",
    "        ts_data_loaded = True\n",
    "    except Exception as e2:\n",
    "        print(f\"Error loading ARIMA data: {e2}\")\n",
    "        ts_data_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e11f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the time series data\n",
    "if 'ts_daily' in locals():\n",
    "    # Show basic info\n",
    "    print(f\"Time series data shape: {ts_daily.shape}\")\n",
    "    print(f\"Date range: {ts_daily.index.min()} to {ts_daily.index.max()}\")\n",
    "    print(f\"Total days: {len(ts_daily)}\")\n",
    "    \n",
    "    # Display column information\n",
    "    print(\"\\nColumns:\")\n",
    "    for col in ts_daily.columns:\n",
    "        print(f\"- {col}: {ts_daily[col].dtype}\")\n",
    "    \n",
    "    # Display sample of the data\n",
    "    print(\"\\nSample data:\")\n",
    "    display(ts_daily.head())\n",
    "    \n",
    "    # Plot the main time series of interest\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(ts_daily.index, ts_daily['avg_delay'], linewidth=1.5)\n",
    "    plt.title('Daily Average Departure Delay', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Average Delay (minutes)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif 'arima_data' in locals():\n",
    "    # Extract basic info from ARIMA data\n",
    "    print(f\"ARIMA data shape: {arima_data.shape}\")\n",
    "    print(f\"Date range: {arima_data.index.min()} to {arima_data.index.max()}\")\n",
    "    \n",
    "    # Plot the main time series from ARIMA data\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(arima_data.index, arima_data, linewidth=1.5)\n",
    "    plt.title('Daily Average Departure Delay', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Average Delay (minutes)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # If exogenous variables are available, show their info\n",
    "    if arima_exog is not None:\n",
    "        print(f\"\\nExogenous variables shape: {arima_exog.shape}\")\n",
    "        print(\"Exogenous variables columns:\")\n",
    "        for col in arima_exog.columns:\n",
    "            print(f\"- {col}\")\n",
    "        \n",
    "        # Display a sample\n",
    "        print(\"\\nSample of exogenous data:\")\n",
    "        display(arima_exog.head())\n",
    "else:\n",
    "    print(\"No time series data available. Please check data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1532eab",
   "metadata": {},
   "source": [
    "## Data Preparation for SARIMA Modeling\n",
    "\n",
    "Before fitting the SARIMA model, we need to:\n",
    "1. Check and ensure stationarity of the time series\n",
    "2. Identify appropriate seasonal differencing parameters\n",
    "3. Analyze ACF and PACF plots to select SARIMA parameters\n",
    "4. Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350529d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for SARIMA modeling\n",
    "def prepare_sarima_data(ts_data, target_col='avg_delay', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare time series data for SARIMA modeling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ts_data : pandas.DataFrame\n",
    "        Time series data with DatetimeIndex\n",
    "    target_col : str\n",
    "        Column to be forecasted\n",
    "    test_size : float\n",
    "        Proportion of data to be used for testing\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (train_data, test_data, exog_train, exog_test)\n",
    "    \"\"\"\n",
    "    # If input is a Series, convert to DataFrame\n",
    "    if isinstance(ts_data, pd.Series):\n",
    "        # This is the case with arima_data from pickle\n",
    "        ts_series = ts_data\n",
    "        target_col = ts_series.name if ts_series.name else 'avg_delay'\n",
    "        ts_data = pd.DataFrame({target_col: ts_series})\n",
    "        ts_data.index = ts_series.index\n",
    "        exog_data = arima_exog if 'arima_exog' in globals() else None\n",
    "    else:\n",
    "        # Case with full DataFrame\n",
    "        if target_col not in ts_data.columns:\n",
    "            # Default to avg_delay if the target column is not in the data\n",
    "            if 'avg_delay' in ts_data.columns:\n",
    "                target_col = 'avg_delay'\n",
    "            else:\n",
    "                raise ValueError(f\"Target column '{target_col}' not found in data\")\n",
    "        \n",
    "        # Extract the target column\n",
    "        ts_series = ts_data[target_col]\n",
    "        \n",
    "        # Extract exogenous variables (all columns except target)\n",
    "        exog_cols = [col for col in ts_data.columns if col != target_col and not col.startswith('avg_delay_lag')]\n",
    "        exog_data = ts_data[exog_cols] if exog_cols else None\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    split_idx = int(len(ts_series) * (1 - test_size))\n",
    "    train_data = ts_series[:split_idx]\n",
    "    test_data = ts_series[split_idx:]\n",
    "    \n",
    "    # Split exogenous data if available\n",
    "    exog_train = None\n",
    "    exog_test = None\n",
    "    if exog_data is not None:\n",
    "        exog_train = exog_data.iloc[:split_idx]\n",
    "        exog_test = exog_data.iloc[split_idx:]\n",
    "    \n",
    "    return train_data, test_data, exog_train, exog_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for stationarity\n",
    "def check_stationarity(ts_series):\n",
    "    \"\"\"\n",
    "    Check if a time series is stationary using the Augmented Dickey-Fuller test\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ts_series : pandas.Series\n",
    "        Time series data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if stationary, False otherwise\n",
    "    \"\"\"\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    \n",
    "    # Perform ADF test\n",
    "    result = adfuller(ts_series.dropna())\n",
    "    \n",
    "    # Plot rolling statistics\n",
    "    rolling_mean = ts_series.rolling(window=7).mean()\n",
    "    rolling_std = ts_series.rolling(window=7).std()\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(211)\n",
    "    plt.plot(ts_series.index, ts_series, label='Original')\n",
    "    plt.plot(rolling_mean.index, rolling_mean, label='Rolling Mean (7-day)')\n",
    "    plt.plot(rolling_std.index, rolling_std, label='Rolling Std (7-day)')\n",
    "    plt.legend()\n",
    "    plt.title('Rolling Mean & Standard Deviation', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.bar(['ADF Statistic', 'p-value', '1%', '5%', '10%'], \n",
    "            [result[0], result[1], result[4]['1%'], result[4]['5%'], result[4]['10%']], \n",
    "            color=['blue', 'red', 'green', 'green', 'green'])\n",
    "    plt.axhline(y=result[4]['5%'], color='green', linestyle='-', alpha=0.5)\n",
    "    plt.title('Augmented Dickey-Fuller Test Results', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print test results\n",
    "    print('Augmented Dickey-Fuller Test Results:')\n",
    "    print(f'ADF Statistic: {result[0]:.4f}')\n",
    "    print(f'p-value: {result[1]:.4f}')\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'   {key}: {value:.4f}')\n",
    "    \n",
    "    # Interpret results\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"\\nConclusion: Time series is stationary (reject null hypothesis)\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\nConclusion: Time series is non-stationary (fail to reject null hypothesis)\")\n",
    "        return False\n",
    "\n",
    "# Apply differencing if non-stationary\n",
    "def difference_series(ts_series, d=1, D=0, s=0):\n",
    "    \"\"\"\n",
    "    Apply differencing to make a time series stationary\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ts_series : pandas.Series\n",
    "        Time series data\n",
    "    d : int\n",
    "        Order of non-seasonal differencing\n",
    "    D : int\n",
    "        Order of seasonal differencing\n",
    "    s : int\n",
    "        Seasonal period length\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series\n",
    "        Differenced time series\n",
    "    \"\"\"\n",
    "    diff_series = ts_series.copy()\n",
    "    \n",
    "    # Apply seasonal differencing\n",
    "    if D > 0 and s > 0:\n",
    "        for _ in range(D):\n",
    "            diff_series = diff_series.diff(s).dropna()\n",
    "    \n",
    "    # Apply regular differencing\n",
    "    if d > 0:\n",
    "        for _ in range(d):\n",
    "            diff_series = diff_series.diff().dropna()\n",
    "    \n",
    "    return diff_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb3c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for SARIMA modeling\n",
    "if 'ts_daily' in locals():\n",
    "    # Use the full dataframe\n",
    "    train_data, test_data, exog_train, exog_test = prepare_sarima_data(ts_daily)\n",
    "elif 'arima_data' in locals():\n",
    "    # Use the loaded ARIMA data\n",
    "    train_data, test_data, exog_train, exog_test = prepare_sarima_data(arima_data)\n",
    "else:\n",
    "    print(\"No time series data available for modeling.\")\n",
    "\n",
    "# Display training and testing data\n",
    "if 'train_data' in locals():\n",
    "    print(f\"Training data shape: {train_data.shape}\")\n",
    "    print(f\"Training data date range: {train_data.index.min()} to {train_data.index.max()}\")\n",
    "    \n",
    "    print(f\"\\nTesting data shape: {test_data.shape}\")\n",
    "    print(f\"Testing data date range: {test_data.index.min()} to {test_data.index.max()}\")\n",
    "    \n",
    "    # Plot train-test split\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(train_data.index, train_data, label='Training Data')\n",
    "    plt.plot(test_data.index, test_data, label='Testing Data', color='red')\n",
    "    plt.title('Train-Test Split for SARIMA Modeling', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Average Delay (minutes)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check stationarity\n",
    "    print(\"\\nChecking stationarity of the training data:\")\n",
    "    is_stationary = check_stationarity(train_data)\n",
    "    \n",
    "    # If not stationary, apply differencing\n",
    "    if not is_stationary:\n",
    "        print(\"\\nApplying first-order differencing:\")\n",
    "        diff1 = difference_series(train_data, d=1)\n",
    "        is_diff1_stationary = check_stationarity(diff1)\n",
    "        \n",
    "        # Try seasonal differencing if still not stationary\n",
    "        if not is_diff1_stationary:\n",
    "            # Try with weekly seasonality (s=7)\n",
    "            print(\"\\nApplying seasonal differencing (weekly):\")\n",
    "            diff_seasonal = difference_series(train_data, d=0, D=1, s=7)\n",
    "            is_seasonal_stationary = check_stationarity(diff_seasonal)\n",
    "            \n",
    "            # Try with both regular and seasonal differencing\n",
    "            if not is_seasonal_stationary:\n",
    "                print(\"\\nApplying both regular and seasonal differencing:\")\n",
    "                diff_both = difference_series(train_data, d=1, D=1, s=7)\n",
    "                is_both_stationary = check_stationarity(diff_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08659d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ACF and PACF plots for model parameter selection\n",
    "def analyze_acf_pacf(ts_series, lags=40, diff_order=(0,0,0), seasonal=False):\n",
    "    \"\"\"\n",
    "    Analyze ACF and PACF plots to determine SARIMA parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ts_series : pandas.Series\n",
    "        Time series data\n",
    "    lags : int\n",
    "        Number of lags to plot\n",
    "    diff_order : tuple\n",
    "        (d, D, s) for regular differencing, seasonal differencing, and seasonal period\n",
    "    seasonal : bool\n",
    "        Whether to analyze for seasonal patterns\n",
    "    \"\"\"\n",
    "    d, D, s = diff_order\n",
    "    \n",
    "    # Apply differencing if necessary\n",
    "    data = ts_series.copy()\n",
    "    if D > 0 and s > 0:\n",
    "        for _ in range(D):\n",
    "            data = data.diff(s).dropna()\n",
    "    \n",
    "    if d > 0:\n",
    "        for _ in range(d):\n",
    "            data = data.diff().dropna()\n",
    "    \n",
    "    # Create the plots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "    \n",
    "    # Plot ACF\n",
    "    sm.graphics.tsa.plot_acf(data.dropna(), lags=lags, alpha=0.05, \n",
    "                            title=f'Autocorrelation Function (ACF) - d={d}, D={D}, s={s}',\n",
    "                            ax=axes[0])\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot PACF\n",
    "    sm.graphics.tsa.plot_pacf(data.dropna(), lags=lags, alpha=0.05, method='ols',\n",
    "                             title=f'Partial Autocorrelation Function (PACF) - d={d}, D={D}, s={s}',\n",
    "                             ax=axes[1])\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze seasonal decomposition\n",
    "def analyze_seasonal_decomposition(ts_series, period):\n",
    "    \"\"\"\n",
    "    Decompose time series into trend, seasonal, and residual components\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ts_series : pandas.Series\n",
    "        Time series data\n",
    "    period : int\n",
    "        Seasonality period\n",
    "    \"\"\"\n",
    "    # Fill any missing values for decomposition\n",
    "    ts_filled = ts_series.fillna(method='ffill')\n",
    "    \n",
    "    # Perform decomposition\n",
    "    decomposition = seasonal_decompose(ts_filled, model='additive', period=period)\n",
    "    \n",
    "    # Plot decomposition\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    plt.subplot(411)\n",
    "    plt.plot(decomposition.observed)\n",
    "    plt.title('Observed', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(412)\n",
    "    plt.plot(decomposition.trend)\n",
    "    plt.title('Trend', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(413)\n",
    "    plt.plot(decomposition.seasonal)\n",
    "    plt.title(f'Seasonality (Period = {period})', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(414)\n",
    "    plt.plot(decomposition.resid)\n",
    "    plt.title('Residuals', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return the strength of seasonality (variance of the seasonal component relative to total)\n",
    "    seasonal_strength = 1 - decomposition.resid.var() / (decomposition.seasonal + decomposition.resid).var()\n",
    "    print(f\"Strength of seasonality (Period = {period}): {seasonal_strength:.4f}\")\n",
    "    return seasonal_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240daf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ACF and PACF for parameter selection\n",
    "if 'train_data' in locals():\n",
    "    # Original series\n",
    "    print(\"ACF and PACF of original series:\")\n",
    "    analyze_acf_pacf(train_data, diff_order=(0,0,0))\n",
    "    \n",
    "    # First differenced series\n",
    "    print(\"ACF and PACF after first-order differencing:\")\n",
    "    analyze_acf_pacf(train_data, diff_order=(1,0,0))\n",
    "    \n",
    "    # Check weekly seasonality (s=7)\n",
    "    print(\"ACF and PACF with weekly seasonal differencing:\")\n",
    "    analyze_acf_pacf(train_data, diff_order=(0,1,7))\n",
    "    \n",
    "    # Check both regular and seasonal differencing\n",
    "    print(\"ACF and PACF with both regular and weekly seasonal differencing:\")\n",
    "    analyze_acf_pacf(train_data, diff_order=(1,1,7))\n",
    "    \n",
    "    # Analyze seasonal components\n",
    "    print(\"\\nAnalyzing seasonal components:\")\n",
    "    \n",
    "    # Weekly seasonality\n",
    "    weekly_strength = analyze_seasonal_decomposition(train_data, period=7)\n",
    "    \n",
    "    # Monthly seasonality (approx 30 days)\n",
    "    monthly_strength = analyze_seasonal_decomposition(train_data, period=30)\n",
    "    \n",
    "    # Quarterly seasonality (approx 90 days)\n",
    "    quarterly_strength = analyze_seasonal_decomposition(train_data, period=90)\n",
    "    \n",
    "    # Determine the strongest seasonality\n",
    "    seasonalities = {\n",
    "        'Weekly (7 days)': weekly_strength,\n",
    "        'Monthly (30 days)': monthly_strength,\n",
    "        'Quarterly (90 days)': quarterly_strength\n",
    "    }\n",
    "    \n",
    "    strongest = max(seasonalities, key=seasonalities.get)\n",
    "    print(f\"\\nStrongest seasonal pattern detected: {strongest} with strength {seasonalities[strongest]:.4f}\")\n",
    "    \n",
    "    # Set seasonal period for SARIMA\n",
    "    if strongest == 'Weekly (7 days)':\n",
    "        seasonal_period = 7\n",
    "    elif strongest == 'Monthly (30 days)':\n",
    "        seasonal_period = 30\n",
    "    else:\n",
    "        seasonal_period = 90\n",
    "    \n",
    "    print(f\"Using seasonal period = {seasonal_period} for SARIMA modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe97e5f",
   "metadata": {},
   "source": [
    "## SARIMA Model Parameter Selection\n",
    "\n",
    "Based on the ACF, PACF, and seasonal decomposition analysis, we can now select appropriate parameters for our SARIMA model. A SARIMA model is defined by the parameters:\n",
    "\n",
    "SARIMA(p, d, q)(P, D, Q)s where:\n",
    "- p: Order of the AR term (autoregressive)\n",
    "- d: Order of differencing\n",
    "- q: Order of the MA term (moving average)\n",
    "- P: Seasonal order of the AR term\n",
    "- D: Seasonal order of differencing\n",
    "- Q: Seasonal order of the MA term\n",
    "- s: Seasonal period\n",
    "\n",
    "We'll use a grid search approach to find the optimal parameters based on AIC (Akaike Information Criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for optimal SARIMA parameters\n",
    "def sarima_grid_search(train_data, exog_train=None, seasonal_period=7,\n",
    "                     p_range=range(0, 3), d_range=range(0, 2),\n",
    "                     q_range=range(0, 3), P_range=range(0, 2),\n",
    "                     D_range=range(0, 2), Q_range=range(0, 2)):\n",
    "    \"\"\"\n",
    "    Grid search for optimal SARIMA parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : pandas.Series\n",
    "        Training time series data\n",
    "    exog_train : pandas.DataFrame, optional\n",
    "        Exogenous variables for training\n",
    "    seasonal_period : int\n",
    "        Seasonality period\n",
    "    p_range, d_range, q_range : range\n",
    "        Ranges for non-seasonal parameters p, d, q\n",
    "    P_range, D_range, Q_range : range\n",
    "        Ranges for seasonal parameters P, D, Q\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with best parameters and AIC value\n",
    "    \"\"\"\n",
    "    best_aic = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    total_combinations = len(p_range) * len(d_range) * len(q_range) * \\\n",
    "                        len(P_range) * len(D_range) * len(Q_range)\n",
    "    \n",
    "    print(f\"Grid search over {total_combinations} parameter combinations...\")\n",
    "    \n",
    "    # Counter for progress tracking\n",
    "    counter = 0\n",
    "    \n",
    "    # Grid search\n",
    "    for p in p_range:\n",
    "        for d in d_range:\n",
    "            for q in q_range:\n",
    "                for P in P_range:\n",
    "                    for D in D_range:\n",
    "                        for Q in Q_range:\n",
    "                            # Update progress\n",
    "                            counter += 1\n",
    "                            if counter % 10 == 0 or counter == total_combinations:\n",
    "                                print(f\"Progress: {counter}/{total_combinations} combinations tested\")\n",
    "                            \n",
    "                            # Skip invalid models\n",
    "                            if p == 0 and q == 0 and P == 0 and Q == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            try:\n",
    "                                # Fit SARIMA model\n",
    "                                model = SARIMAX(train_data,\n",
    "                                              exog=exog_train,\n",
    "                                              order=(p, d, q),\n",
    "                                              seasonal_order=(P, D, Q, seasonal_period),\n",
    "                                              enforce_stationarity=False,\n",
    "                                              enforce_invertibility=False)\n",
    "                                \n",
    "                                results = model.fit(disp=False)\n",
    "                                \n",
    "                                # Calculate AIC\n",
    "                                aic = results.aic\n",
    "                                \n",
    "                                # If better than current best, update\n",
    "                                if aic < best_aic:\n",
    "                                    best_aic = aic\n",
    "                                    best_params = (p, d, q, P, D, Q, seasonal_period)\n",
    "                                    best_model = results\n",
    "                                    \n",
    "                                    print(f\"New best model: SARIMA{best_params} with AIC={best_aic:.4f}\")\n",
    "                            \n",
    "                            except Exception as e:\n",
    "                                # Some combinations may not converge or have other issues\n",
    "                                continue\n",
    "    \n",
    "    if best_params is None:\n",
    "        print(\"No valid model found. Try different parameter ranges.\")\n",
    "        return None\n",
    "    \n",
    "    # Return best parameters and model\n",
    "    return {\n",
    "        'order': (best_params[0], best_params[1], best_params[2]),\n",
    "        'seasonal_order': (best_params[3], best_params[4], best_params[5], best_params[6]),\n",
    "        'aic': best_aic,\n",
    "        'model': best_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search for optimal parameters\n",
    "# Note: This can be time-consuming, so we'll use restricted parameter ranges\n",
    "if 'train_data' in locals():\n",
    "    # Get the seasonal period determined earlier\n",
    "    if 'seasonal_period' not in locals():\n",
    "        seasonal_period = 7  # Default to weekly seasonality if not determined\n",
    "    \n",
    "    # Run grid search for optimal parameters\n",
    "    # Reduced parameter space for demonstration purposes\n",
    "    best_model_params = sarima_grid_search(\n",
    "        train_data, \n",
    "        exog_train=exog_train, \n",
    "        seasonal_period=seasonal_period,\n",
    "        p_range=range(0, 3),  # 0, 1, 2\n",
    "        d_range=range(0, 2),  # 0, 1 (from stationarity test)\n",
    "        q_range=range(0, 3),  # 0, 1, 2\n",
    "        P_range=range(0, 2),  # 0, 1\n",
    "        D_range=range(0, 2),  # 0, 1 (from seasonal differencing test)\n",
    "        Q_range=range(0, 2)   # 0, 1\n",
    "    )\n",
    "    \n",
    "    # Display the results\n",
    "    if best_model_params:\n",
    "        print(\"\\nBest SARIMA Parameters:\")\n",
    "        print(f\"Non-seasonal order (p, d, q): {best_model_params['order']}\")\n",
    "        print(f\"Seasonal order (P, D, Q, s): {best_model_params['seasonal_order']}\")\n",
    "        print(f\"AIC: {best_model_params['aic']:.4f}\")\n",
    "        \n",
    "        # Store the best model\n",
    "        best_sarima_model = best_model_params['model']\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\nModel Summary:\")\n",
    "        display(best_sarima_model.summary())\n",
    "    else:\n",
    "        print(\"No valid model found. Will proceed with manual parameter selection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e47c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual parameter selection if grid search fails or is too time-consuming\n",
    "def fit_sarima_model(train_data, exog_train=None, order=(1,1,1), seasonal_order=(1,1,1,7)):\n",
    "    \"\"\"\n",
    "    Fit a SARIMA model with specified parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : pandas.Series\n",
    "        Training time series data\n",
    "    exog_train : pandas.DataFrame, optional\n",
    "        Exogenous variables for training\n",
    "    order : tuple\n",
    "        (p, d, q) order of the non-seasonal part\n",
    "    seasonal_order : tuple\n",
    "        (P, D, Q, s) order of the seasonal part\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    statsmodels.tsa.statespace.sarimax.SARIMAXResults\n",
    "        Fitted SARIMA model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fit SARIMA model\n",
    "        model = SARIMAX(train_data,\n",
    "                      exog=exog_train,\n",
    "                      order=order,\n",
    "                      seasonal_order=seasonal_order,\n",
    "                      enforce_stationarity=False,\n",
    "                      enforce_invertibility=False)\n",
    "        \n",
    "        results = model.fit(disp=False)\n",
    "        \n",
    "        print(f\"SARIMA{order}{seasonal_order} AIC: {results.aic:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting SARIMA{order}{seasonal_order}: {e}\")\n",
    "        return None\n",
    "\n",
    "# If grid search failed or user wants to try specific parameters\n",
    "if 'best_sarima_model' not in locals():\n",
    "    print(\"Trying some common SARIMA specifications:\")\n",
    "    \n",
    "    # Weekly seasonality models\n",
    "    models = []\n",
    "    \n",
    "    # Try SARIMA(1,1,1)(1,1,1,7) - common for weekly patterns\n",
    "    model1 = fit_sarima_model(train_data, exog_train, \n",
    "                             order=(1,1,1), \n",
    "                             seasonal_order=(1,1,1,7))\n",
    "    if model1:\n",
    "        models.append(('SARIMA(1,1,1)(1,1,1,7)', model1))\n",
    "    \n",
    "    # Try SARIMA(2,1,1)(1,1,1,7)\n",
    "    model2 = fit_sarima_model(train_data, exog_train, \n",
    "                             order=(2,1,1), \n",
    "                             seasonal_order=(1,1,1,7))\n",
    "    if model2:\n",
    "        models.append(('SARIMA(2,1,1)(1,1,1,7)', model2))\n",
    "    \n",
    "    # Try SARIMA(1,1,2)(1,1,1,7)\n",
    "    model3 = fit_sarima_model(train_data, exog_train, \n",
    "                             order=(1,1,2), \n",
    "                             seasonal_order=(1,1,1,7))\n",
    "    if model3:\n",
    "        models.append(('SARIMA(1,1,2)(1,1,1,7)', model3))\n",
    "    \n",
    "    # Find the best model based on AIC\n",
    "    if models:\n",
    "        best_model_name, best_sarima_model = min(models, key=lambda x: x[1].aic)\n",
    "        print(f\"\\nBest manually selected model: {best_model_name} with AIC: {best_sarima_model.aic:.4f}\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\nModel Summary:\")\n",
    "        display(best_sarima_model.summary())\n",
    "    else:\n",
    "        print(\"All manual models failed to fit. Consider trying different parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927cc6a",
   "metadata": {},
   "source": [
    "## Model Diagnostics and Validation\n",
    "\n",
    "Now that we have fitted our SARIMA model, we need to validate it by:\n",
    "1. Checking the model diagnostics (residuals)\n",
    "2. Forecasting on the test set\n",
    "3. Calculating error metrics (RMSE, MAE, MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic checking\n",
    "def check_model_diagnostics(model_results):\n",
    "    \"\"\"\n",
    "    Check the diagnostics of a fitted SARIMA model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_results : statsmodels.tsa.statespace.sarimax.SARIMAXResults\n",
    "        Fitted SARIMA model\n",
    "    \"\"\"\n",
    "    # Residual diagnostics\n",
    "    residuals = model_results.resid\n",
    "    \n",
    "    # Plot residuals\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Residuals over time\n",
    "    plt.subplot(311)\n",
    "    plt.plot(residuals.index, residuals)\n",
    "    plt.title('Residuals Over Time', fontsize=16)\n",
    "    plt.ylabel('Residual', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram plus estimated density\n",
    "    plt.subplot(312)\n",
    "    sns.histplot(residuals, kde=True)\n",
    "    plt.title('Histogram of Residuals', fontsize=16)\n",
    "    plt.xlabel('Residual', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # QQ Plot\n",
    "    plt.subplot(313)\n",
    "    sm.qqplot(residuals, line='45', fit=True)\n",
    "    plt.title('QQ Plot of Residuals', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for autocorrelation in residuals\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    plot_acf(residuals.dropna(), lags=40, alpha=0.05, title='ACF of Residuals', ax=plt.gca())\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plot_pacf(residuals.dropna(), lags=40, alpha=0.05, method='ols', title='PACF of Residuals', ax=plt.gca())\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Ljung-Box test for autocorrelation\n",
    "    from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "    lb_test = acorr_ljungbox(residuals.dropna(), lags=[10, 20, 30, 40])\n",
    "    \n",
    "    print(\"Ljung-Box Test for Autocorrelation in Residuals:\")\n",
    "    for i, lag in enumerate([10, 20, 30, 40]):\n",
    "        print(f\"Lag {lag}: p-value = {lb_test[1][i]:.4f}\")\n",
    "        if lb_test[1][i] > 0.05:\n",
    "            print(f\"  Fail to reject null hypothesis: No significant autocorrelation at lag {lag}\")\n",
    "        else:\n",
    "            print(f\"  Reject null hypothesis: Significant autocorrelation at lag {lag}\")\n",
    "\n",
    "# If we have a fitted model, check diagnostics\n",
    "if 'best_sarima_model' in locals():\n",
    "    check_model_diagnostics(best_sarima_model)\n",
    "else:\n",
    "    print(\"No fitted model available for diagnostics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b70603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forecasts and evaluate performance\n",
    "def forecast_and_evaluate(model, train_data, test_data, exog_train=None, exog_test=None):\n",
    "    \"\"\"\n",
    "    Create forecasts and evaluate model performance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : statsmodels.tsa.statespace.sarimax.SARIMAXResults\n",
    "        Fitted SARIMA model\n",
    "    train_data : pandas.Series\n",
    "        Training time series data\n",
    "    test_data : pandas.Series\n",
    "        Testing time series data\n",
    "    exog_train : pandas.DataFrame, optional\n",
    "        Exogenous variables for training\n",
    "    exog_test : pandas.DataFrame, optional\n",
    "        Exogenous variables for testing\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with forecast results and evaluation metrics\n",
    "    \"\"\"\n",
    "    # Get forecast for test period\n",
    "    forecast = model.get_forecast(steps=len(test_data), exog=exog_test)\n",
    "    forecast_mean = forecast.predicted_mean\n",
    "    forecast_ci = forecast.conf_int()\n",
    "    \n",
    "    # Plot forecast vs. actual\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(train_data.index, train_data, label='Training Data')\n",
    "    plt.plot(test_data.index, test_data, label='Actual Test Data')\n",
    "    plt.plot(forecast_mean.index, forecast_mean, color='red', label='Forecast')\n",
    "    plt.fill_between(forecast_ci.index, \n",
    "                    forecast_ci.iloc[:, 0], \n",
    "                    forecast_ci.iloc[:, 1], \n",
    "                    color='pink', alpha=0.3,\n",
    "                    label='95% Confidence Interval')\n",
    "    plt.title('SARIMA Forecast vs. Actual', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Average Delay (minutes)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    # Make sure both arrays are aligned properly\n",
    "    combined = pd.DataFrame({\n",
    "        'actual': test_data,\n",
    "        'forecast': forecast_mean\n",
    "    })\n",
    "    \n",
    "    # Drop any rows with missing values\n",
    "    combined = combined.dropna()\n",
    "    \n",
    "    if len(combined) == 0:\n",
    "        print(\"Error: No overlapping data between forecast and test data\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(combined['actual'], combined['forecast'])\n",
    "    rmse = np.sqrt(mean_squared_error(combined['actual'], combined['forecast']))\n",
    "    r2 = r2_score(combined['actual'], combined['forecast'])\n",
    "    \n",
    "    # Calculate MAPE, avoid division by zero\n",
    "    mape = np.mean(np.abs((combined['actual'] - combined['forecast']) / combined['actual'])) * 100\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"Forecast Evaluation Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}%\")\n",
    "    print(f\"R-squared (R): {r2:.4f}\")\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'forecast': forecast_mean,\n",
    "        'conf_int': forecast_ci,\n",
    "        'metrics': {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape,\n",
    "            'r2': r2\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6249c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forecasts and evaluate performance\n",
    "if 'best_sarima_model' in locals() and 'train_data' in locals() and 'test_data' in locals():\n",
    "    forecast_results = forecast_and_evaluate(\n",
    "        best_sarima_model, \n",
    "        train_data, \n",
    "        test_data, \n",
    "        exog_train=exog_train, \n",
    "        exog_test=exog_test\n",
    "    )\n",
    "else:\n",
    "    print(\"Missing required data for forecasting and evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469bcd3",
   "metadata": {},
   "source": [
    "## Future Forecasting\n",
    "\n",
    "Now that we have validated our model, we can use it to forecast future flight delays. This is the main purpose of our time series model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast future values\n",
    "def forecast_future(model, data, exog_data=None, steps=30):\n",
    "    \"\"\"\n",
    "    Forecast future values using the fitted model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : statsmodels.tsa.statespace.sarimax.SARIMAXResults\n",
    "        Fitted SARIMA model\n",
    "    data : pandas.Series\n",
    "        Complete time series data\n",
    "    exog_data : pandas.DataFrame, optional\n",
    "        Complete exogenous data\n",
    "    steps : int\n",
    "        Number of steps to forecast\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with forecasted values and confidence intervals\n",
    "    \"\"\"\n",
    "    # Create future dates\n",
    "    last_date = data.index[-1]\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=steps, freq='D')\n",
    "    \n",
    "    # If we have exogenous variables, we need to forecast or assume values for them\n",
    "    future_exog = None\n",
    "    if exog_data is not None:\n",
    "        # This is a placeholder - in a real scenario, you'd need to forecast exog variables\n",
    "        # or use known future values (like holidays, seasons, etc.)\n",
    "        print(\"Note: For accurate future forecasting, you would need future exogenous variables.\")\n",
    "        print(\"For demonstration, we'll use the last values of exogenous variables.\")\n",
    "        \n",
    "        # Use the last values as a simple approach\n",
    "        last_exog_values = exog_data.iloc[-1]\n",
    "        future_exog = pd.DataFrame([last_exog_values] * steps, index=future_dates)\n",
    "    \n",
    "    # Get forecast\n",
    "    forecast = model.get_forecast(steps=steps, exog=future_exog)\n",
    "    forecast_mean = forecast.predicted_mean\n",
    "    forecast_ci = forecast.conf_int()\n",
    "    \n",
    "    # Combine into a DataFrame\n",
    "    forecast_df = pd.DataFrame({\n",
    "        'forecast': forecast_mean,\n",
    "        'lower_ci': forecast_ci.iloc[:, 0],\n",
    "        'upper_ci': forecast_ci.iloc[:, 1]\n",
    "    }, index=future_dates)\n",
    "    \n",
    "    # Plot forecast\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(data.index, data, label='Historical Data')\n",
    "    plt.plot(forecast_df.index, forecast_df['forecast'], color='red', label='Forecast')\n",
    "    plt.fill_between(forecast_df.index, \n",
    "                    forecast_df['lower_ci'], \n",
    "                    forecast_df['upper_ci'], \n",
    "                    color='pink', alpha=0.3,\n",
    "                    label='95% Confidence Interval')\n",
    "    plt.title(f'{steps}-Day Future Forecast of Flight Delays', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Average Delay (minutes)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf84a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast 30 days into the future\n",
    "if 'best_sarima_model' in locals():\n",
    "    # Combine train and test data for full history\n",
    "    if isinstance(train_data, pd.Series) and isinstance(test_data, pd.Series):\n",
    "        full_data = pd.concat([train_data, test_data])\n",
    "        \n",
    "        # Combine exogenous data if available\n",
    "        full_exog = None\n",
    "        if exog_train is not None and exog_test is not None:\n",
    "            full_exog = pd.concat([exog_train, exog_test])\n",
    "        \n",
    "        # Forecast 30 days into the future\n",
    "        future_forecast = forecast_future(\n",
    "            best_sarima_model, \n",
    "            full_data, \n",
    "            exog_data=full_exog, \n",
    "            steps=30\n",
    "        )\n",
    "        \n",
    "        # Display forecast results\n",
    "        print(\"\\nForecast for the next 30 days:\")\n",
    "        display(future_forecast.head(10))\n",
    "        \n",
    "        # Save forecast to CSV\n",
    "        forecast_path = os.path.join(TS_MODEL_PATH, 'sarima_forecast_30days.csv')\n",
    "        future_forecast.to_csv(forecast_path)\n",
    "        print(f\"\\nSaved forecast to {forecast_path}\")\n",
    "    else:\n",
    "        print(\"Train and test data must be pandas Series for future forecasting.\")\n",
    "else:\n",
    "    print(\"No fitted model available for future forecasting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd078816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model for future use\n",
    "if 'best_sarima_model' in locals():\n",
    "    # Save the model\n",
    "    model_path = os.path.join(TS_MODEL_PATH, 'sarima_flight_delay_model.pkl')\n",
    "    \n",
    "    # Create a dictionary with model and parameters\n",
    "    model_dict = {\n",
    "        'model': best_sarima_model,\n",
    "        'order': best_sarima_model.model.order,\n",
    "        'seasonal_order': best_sarima_model.model.seasonal_order,\n",
    "        'aic': best_sarima_model.aic,\n",
    "        'bic': best_sarima_model.bic,\n",
    "        'training_end_date': train_data.index[-1],\n",
    "        'forecast_start_date': test_data.index[0]\n",
    "    }\n",
    "    \n",
    "    # Save to pickle\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model_dict, f)\n",
    "    \n",
    "    print(f\"Saved model to {model_path}\")\n",
    "    \n",
    "    # Save model parameters to text file for reference\n",
    "    param_path = os.path.join(TS_MODEL_PATH, 'sarima_model_params.txt')\n",
    "    \n",
    "    with open(param_path, 'w') as f:\n",
    "        f.write(\"SARIMA Flight Delay Model Parameters\\n\")\n",
    "        f.write(\"===================================\\n\\n\")\n",
    "        f.write(f\"Non-seasonal order (p,d,q): {best_sarima_model.model.order}\\n\")\n",
    "        f.write(f\"Seasonal order (P,D,Q,s): {best_sarima_model.model.seasonal_order}\\n\\n\")\n",
    "        f.write(f\"AIC: {best_sarima_model.aic:.4f}\\n\")\n",
    "        f.write(f\"BIC: {best_sarima_model.bic:.4f}\\n\\n\")\n",
    "        f.write(\"Model trained on data from \")\n",
    "        f.write(f\"{train_data.index[0]} to {train_data.index[-1]}\\n\\n\")\n",
    "        \n",
    "        if 'forecast_results' in locals() and forecast_results:\n",
    "            metrics = forecast_results['metrics']\n",
    "            f.write(\"Evaluation Metrics:\\n\")\n",
    "            f.write(f\"MAE: {metrics['mae']:.4f}\\n\")\n",
    "            f.write(f\"RMSE: {metrics['rmse']:.4f}\\n\")\n",
    "            f.write(f\"MAPE: {metrics['mape']:.4f}%\\n\")\n",
    "            f.write(f\"R: {metrics['r2']:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Saved model parameters to {param_path}\")\n",
    "else:\n",
    "    print(\"No fitted model available to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d958e",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Loaded and prepared time series data from the preprocessing pipeline\n",
    "2. Analyzed stationarity and seasonal patterns in the flight delay data\n",
    "3. Selected appropriate SARIMA parameters through grid search and diagnostic analysis\n",
    "4. Fitted a SARIMA model to capture both trend and seasonal patterns in flight delays\n",
    "5. Validated the model using test data and calculated performance metrics\n",
    "6. Created future forecasts of flight delays\n",
    "\n",
    "The SARIMA model is particularly well-suited for this task because flight delay data exhibits:\n",
    "- Clear seasonal patterns (weekly, monthly, quarterly)\n",
    "- Temporal dependence (today's delays affect tomorrow's operations)\n",
    "- Need for differencing to achieve stationarity\n",
    "\n",
    "For further improvement, we could:\n",
    "- Include more external regressors (weather data, airport congestion metrics)\n",
    "- Experiment with alternative models (Prophet, LSTM, hybrid approaches)\n",
    "- Create separate models for different airports or carriers\n",
    "- Implement ensemble methods combining multiple time series models\n",
    "\n",
    "This model can be used by airlines and airports to anticipate periods of higher delays and allocate resources accordingly, potentially improving operational efficiency and passenger experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
